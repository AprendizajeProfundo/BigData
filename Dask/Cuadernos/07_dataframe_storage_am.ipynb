{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Almacenamiento de dataframes<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask_horizontal.svg\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Introducción](#Introducción)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Fuente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una traducción libre del tutorial disponible en [dask-tutorial](https://github.com/dask/dask-tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/hdd.jpg\" width=\"20%\" align=\"right\">\n",
    "\n",
    "\n",
    "El almacenamiento eficiente puede mejorar drásticamente el rendimiento, especialmente cuando se opera repetidamente desde el disco.\n",
    "\n",
    "Descomprimir texto y analizar archivos CSV es caro. Una de las estrategias más efectivas  es utilizar un formato de almacenamiento binario como HDF5. A menudo, las ganancias de rendimiento al hacer esto son suficientes para que pueda volver a usar Pandas nuevamente en lugar de usar `dask.dataframe`.\n",
    "\n",
    "En esta sección, aprenderemos cómo organizar y almacenar de manera eficiente sus conjuntos de datos en formatos binarios en disco. Usaremos lo siguiente:\n",
    "\n",
    "\n",
    "1.  Formato [Pandas `HDFStore`](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5) sobre `HDF5`\n",
    "2.  `Categoricals` para almacenar datos de texto numéricamente\n",
    "\n",
    "**Principales conclusiones**\n",
    "\n",
    "1. Los formatos de almacenamiento afectan el rendimiento en un orden de magnitud\n",
    "2. Los datos de texto mantendrán lento incluso un formato rápido como HDF5\n",
    "3. Una combinación de formatos binarios, almacenamiento de columnas y datos particionados convierte tiempos de espera de un segundo en tiempos de espera de 80 ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Lectura CSV</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leemos nuestros datos *csv* como antes.\n",
    "\n",
    "CSV y otros formatos de archivo basados en texto son el almacenamiento más común para datos de muchas fuentes, ya que requieren un preprocesamiento mínimo, se pueden escribir línea por línea y son legibles por humanos. Dado que Pandas '`read_csv` está bien optimizado, los CSV son una entrada razonable, pero están lejos de estar optimizados, ya que la lectura requiere un análisis de texto extenso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df_csv = dd.read_csv(filename)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura a  HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 y netCDF son formatos de arreglos binarios muy utilizados en el ámbito científico.\n",
    "\n",
    "Pandas contiene un formato HDF5 especializado, `HDFStore`. El método ``dd.DataFrame.to_hdf'' funciona exactamente como el método ``pd.DataFrame.to_hdf''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = os.path.join('data', 'accounts.h5')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary format, takes some time up-front\n",
    "%time df_csv.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data as before\n",
    "df_hdf = dd.read_hdf(target, '/data')\n",
    "df_hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de las velocidades CSV y HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un cálculo simple que requiere leer una columna de nuestro conjunto de datos y comparar el rendimiento entre los archivos CSV y nuestro archivo HDF5 recién creado. ¿Cuál esperas que sea más rápido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_csv.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_hdf.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente, son casi iguales, o quizás incluso más lentos.\n",
    "\n",
    "El culpable aquí es la columna `names`, que es de tipo`object` y, por lo tanto, es difícil de almacenar de manera eficiente. Hay dos problemas aquí:\n",
    "\n",
    "1. ¿Cómo almacenamos datos de texto como *nombres* de manera eficiente en el disco?\n",
    "2. ¿Por qué tuvimos que leer la columna *nombres* cuando todo lo que queríamos era *cantidad*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Almacenamiento eficiente de carateres con categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar categorías de Pandas para reemplazar nuestros tipos de objetos con una representación numérica. Esto lleva un poco más de tiempo desde el principio, pero da como resultado un mejor rendimiento.\n",
    "\n",
    "Más sobre categóricos en el [pandas docs](http://pandas.pydata.org/pandas-docs/stable/categorical.html) y en [este blogpost](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize data, then store in HDFStore\n",
    "%time df_hdf.categorize(columns=['names']).to_hdf(target, '/data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks the same\n",
    "df_hdf = dd.read_hdf(target, '/data2')\n",
    "df_hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But loads more quickly\n",
    "%time df_hdf.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now definitely faster than before.  This tells us that it's not only the file type that we use but also how we represent our variables that influences storage performance. \n",
    "\n",
    "How does the performance of reading depend on the scheduler we use? You can try this with threaded, processes and distributed.\n",
    "\n",
    "However this can still be better.  We had to read all of the columns (`names` and `amount`) in order to compute the sum of one (`amount`).  We'll improve further on this with `parquet`, an on-disk column-store.  First though we learn about how to set an index in a dask.dataframe.\n",
    "\n",
    "\n",
    "Esto ahora es definitivamente más rápido que antes. Esto nos dice que no es solo el tipo de archivo que usamos, sino también cómo representamos nuestras variables lo que influye en el rendimiento del almacenamiento.\n",
    "\n",
    "¿Cómo depende el rendimiento de la lectura del programador que utilizamos? Puede probar esto con subprocesos, procesos y distribuidos.\n",
    "\n",
    "Sin embargo, esto aún puede ser mejor. Tuvimos que leer todas las columnas (`nombres` y` monto`) para calcular la suma de uno (`monto`). Mejoraremos aún más esto con `parquet`, un almacén de columnas en disco. Sin embargo, primero aprendemos cómo establecer un índice en un dask.dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastparquet` es una librería para interactuar con archivos en formato parquet, que son un formato muy común en el ecosistema de Big Data, y utilizado por herramientas como Hadoop, Spark e Impala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = os.path.join('data', 'accounts.parquet')\n",
    "df_csv.categorize(columns=['names']).to_parquet(target, storage_options={\"has_nulls\": True}, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigue la estructura de archivos en el nuevo directorio resultante: ¿para qué supone que son esos archivos?\n",
    "\n",
    "`to_parquet` viene con muchas opciones, como compresión, si escribir explícitamente información NULL (no es necesario en este caso) y cómo codificar cadenas. Puede experimentar con ellos para ver qué efecto tienen en el tamaño del archivo y los tiempos de procesamiento, a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l data/accounts.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = dd.read_parquet(target)\n",
    "# note that column names shows the type of the values - we could\n",
    "# choose to load as a categorical column or not.\n",
    "df_p.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vuelva a ejecutar el cálculo de la suma anterior para esta versión de los datos y el tiempo que tarda. Es posible que desee probar esto más de una vez; es común que muchas bibliotecas realicen varios trabajos de configuración cuando se llaman por primera vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_p.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al archivar datos, es común ordenar y dividir por una columna con identificadores únicos, para facilitar búsquedas rápidas más adelante. Para estos datos, esa columna es \"id\". Mide cuánto tiempo lleva recuperar las filas correspondientes a `id == 100` del CSV sin procesar, de HDF5 y versiones de parquet, y finalmente de una nueva versión de parquet escrita después de aplicar `set_index('id')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_p.set_index('id').to_parquet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Archivos remotos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask puede acceder a varios servicios de almacenamiento de datos orientados a la nube y al clúster, como Amazon S3 o HDFS\n",
    "\n",
    "Ventajas:\n",
    "* almacenamiento seguro y escalable\n",
    "\n",
    "Desventajas:\n",
    "* la velocidad de la red se convierte en un cuello de botella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de configurar marcos de datos (y otras colecciones) sigue siendo muy similar a antes. Tenga en cuenta que los datos aquí están disponibles de forma anónima, pero en general se puede pasar un parámetro adicional `storage_options =` con más detalles sobre cómo interactuar con el almacenamiento remoto.\n",
    "\n",
    "```python\n",
    "taxi = dd.read_csv('s3://nyc-tlc/trip data/yellow_tripdata_2015-*.csv',\n",
    "                   storage_options={'anon': True})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advertencia**: las operaciones a través de Internet pueden tardar mucho en ejecutarse. Estas operaciones funcionan muy bien en una configuración en clúster en la nube, por ejemplo, máquinas de Amazon EC2 que leen desde S3 o máquinas de cómputo de Google que leen desde GCS."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
