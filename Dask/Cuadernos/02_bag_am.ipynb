{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Bag: Listas paralelas para datos semiestructurados<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Creación de datos](#Creación-de-datos)\n",
    "* [Creación de una colección bag](#Creación-de-una-colección-bag)\n",
    "* [Manipulación de colecciones bag](#Manipulación-de-colecciones-bag)\n",
    "* [Ejemplo: datos Accounts.JSON](#Ejemplo:-datos-Accounts.JSONe)\n",
    "* [Groupby y Foldby](#Groupby-y-Foldby)\n",
    "* [DataFrames](#DataFrames)\n",
    "* [Limitaciones](#Limitaciones)\n",
    "* [Aprender más](#Aprender-más)\n",
    "* [Shutdown](#Shutdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Fuente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una traducción libre del tutorial disponible en [dask-tutorial](https://github.com/dask/dask-tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dask-bag` sobresale en el procesamiento de datos que se pueden representar como una secuencia de entradas arbitrarias. Nos referiremos a esto como datos \"desordenados\" (`messy data`), porque pueden contener estructuras anidadas complejas, campos faltantes, mezclas de tipos de datos, etc. El estilo de programación * funcional * encaja muy bien con la iteración estándar de Python, como se puede encontrar en el módulo `itertools`.\n",
    "\n",
    "Los datos desordenados a menudo se encuentran al comienzo de las canalizaciones de procesamiento de datos cuando se consumen por primera vez grandes volúmenes de datos sin procesar. El conjunto inicial de datos puede ser JSON, CSV, XML o cualquier otro formato que no aplique una estructura y tipos de datos estrictos.\n",
    "Por esta razón, el procesamiento y el procesamiento de datos iniciales a menudo se realiza con `list`s,` dict`s y `set`s de Python.\n",
    "\n",
    "Estas estructuras de datos centrales están optimizadas para el almacenamiento y procesamiento de uso general. Agregar cálculo de transmisión con iteradores / expresiones generadoras o bibliotecas como `itertools` o [` toolz`] (https://toolz.readthedocs.io/en/latest/) nos permite procesar grandes volúmenes en un espacio pequeño. Si combinamos esto con el procesamiento en paralelo, podemos generar una cantidad considerable de datos.\n",
    "\n",
    "Dask.bag es una colección de Dask de alto nivel para automatizar cargas de trabajo comunes de esta forma. En una palabra\n",
    "\n",
    "    dask.bag = mapa, filtro, toolz + ejecución paralela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Documentación relacionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Bag documentation](https://docs.dask.org/en/latest/bag.html)\n",
    "* [Bag screencast](https://youtu.be/-qIiJ1XtSv0)\n",
    "* [Bag API](https://docs.dask.org/en/latest/bag-api.html)\n",
    "* [Bag examples](https://examples.dask.org/bag.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Creación de datos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Configuración</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, usaremos el programador distribuido. Los programadores se explicarán en profundidad [más adelante] (05_distributed.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Creación de una colección bag</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede crear un Bag a partir de una secuencia de Python, de archivos, de datos en S3, etc. Demostramos el uso de `.take()` para mostrar elementos de los datos. (Hacer .take(1) da como resultado una tupla con un elemento)\n",
    "\n",
    "Tenga en cuenta que los datos se dividen en bloques y hay muchos elementos por bloque. En el primer ejemplo, las dos particiones contienen cinco elementos cada una, y en los dos siguientes, cada archivo está dividido en uno o más bloques de bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ lista [1,2,3,4,3]\n",
    "+ tupla (3,5,6,5)\n",
    "+ dict {key1:4, key2:4, }\n",
    "+ set {3,4,5}\n",
    "\n",
    "\n",
    "+ bag ()\n",
    "\n",
    "map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each element is an integer\n",
    "import dask.bag as db\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=2)\n",
    "b.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 "
     ]
    }
   ],
   "source": [
    "for item in b:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each element is a text file, where each line is a JSON object\n",
    "# note that the compression is handled automatically\n",
    "import os\n",
    "b = db.read_text(os.path.join('../data', 'accounts.*.json.gz'))\n",
    "b.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/accounts.*.json.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.path.join('../data', 'accounts.*.json.gz')\n",
    "b = db.read_text(files)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edit sources.py to configure source locations\n",
    "import sys\n",
    "sys.path.append('../') # para acceder al archivo source en la carpeta base de esta carpeta\n",
    "\n",
    "import sources\n",
    "sources.bag_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3 = Amazon Simple Storage Service (Amazon S3) es un servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. s3 es exabytes y permite la creación de grandes lagos de datos.\n",
    "\n",
    "+  define un protocolo de intercambio de información, en este caso s3. Otros protolos son\n",
    "\n",
    "+ s3://\n",
    "+ hdf5:// para archivos hdf5\n",
    "+ gcs:// para archivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,RateCodeID,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\\n',\n",
       " '2,2015-01-15 19:05:39,2015-01-15 19:23:42,1,1.59,-73.993896484375,40.750110626220703,1,N,-73.974784851074219,40.750617980957031,1,12,1,0.5,3.25,0,0.3,17.05\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires `s3fs` library\n",
    "# each partition is a remote CSV text file\n",
    "b = db.read_text(sources.bag_url,\n",
    "                 storage_options={'anon': True})\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_text in module dask.bag.text:\n",
      "\n",
      "read_text(urlpath, blocksize=None, compression='infer', encoding='utf-8', errors='strict', linedelimiter='\\n', collection=True, storage_options=None, files_per_partition=None, include_path=False)\n",
      "    Read lines from text files\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    urlpath : string or list\n",
      "        Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n",
      "        to read from alternative filesystems. To read from multiple files you\n",
      "        can pass a globstring or a list of paths, with the caveat that they\n",
      "        must all have the same protocol.\n",
      "    blocksize: None, int, or str\n",
      "        Size (in bytes) to cut up larger files.  Streams by default.\n",
      "        Can be ``None`` for streaming, an integer number of bytes, or a string\n",
      "        like \"128MiB\"\n",
      "    compression: string\n",
      "        Compression format like 'gzip' or 'xz'.  Defaults to 'infer'\n",
      "    encoding: string\n",
      "    errors: string\n",
      "    linedelimiter: string\n",
      "    collection: bool, optional\n",
      "        Return dask.bag if True, or list of delayed values if false\n",
      "    storage_options: dict\n",
      "        Extra options that make sense to a particular storage connection, e.g.\n",
      "        host, port, username, password, etc.\n",
      "    files_per_partition: None or int\n",
      "        If set, group input files into partitions of the requested size,\n",
      "        instead of one partition per file. Mutually exclusive with blocksize.\n",
      "    include_path: bool\n",
      "        Whether or not to include the path in the bag.\n",
      "        If true, elements are tuples of (line, path).\n",
      "        Default is False.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> b = read_text('myfiles.1.txt')  # doctest: +SKIP\n",
      "    >>> b = read_text('myfiles.*.txt')  # doctest: +SKIP\n",
      "    >>> b = read_text('myfiles.*.txt.gz')  # doctest: +SKIP\n",
      "    >>> b = read_text('s3://bucket/myfiles.*.txt')  # doctest: +SKIP\n",
      "    >>> b = read_text('s3://key:secret@bucket/myfiles.*.txt')  # doctest: +SKIP\n",
      "    >>> b = read_text('hdfs://namenode.example.com/myfiles.*.txt')  # doctest: +SKIP\n",
      "    \n",
      "    Parallelize a large file by providing the number of uncompressed bytes to\n",
      "    load into each partition.\n",
      "    \n",
      "    >>> b = read_text('largefile.txt', blocksize='10MB')  # doctest: +SKIP\n",
      "    \n",
      "    Get file paths of the bag by setting include_path=True\n",
      "    \n",
      "    >>> b = read_text('myfiles.*.txt', include_path=True) # doctest: +SKIP\n",
      "    >>> b.take(1) # doctest: +SKIP\n",
      "    (('first line of the first file', '/home/dask/myfiles.0.txt'),)\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dask.bag.Bag or list\n",
      "        dask.bag.Bag if collection is True or list of Delayed lists otherwise.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    from_sequence: Build bag from Python sequence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(db.read_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Manipulación de colecciones bag</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos  `Bag` contienen la API funcional estándar que se encuentra en proyectos como la biblioteca estándar de Python, `toolz`, o `pyspark`, incluyendo `map`, `filter`, `groupby`, etc..\n",
    "\n",
    "Operaciones sobre objetos `Bag` crea nuevos objetos bags. Llame el método `.compute()` para lanzar la ejecución, como hicimos para objetos  `Delayed`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36, 64, 100]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(n):\n",
    "    return  n%2 == 0\n",
    "\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "c = b.filter(is_even).map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36, 64, 100]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blocking form: wait for completion (which is very fast in this case)\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo: datos Accounts.JSON</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado un conjunto simulado de datos JSON comprimidos con gzip en el directorio de datos. Esto es como el ejemplo usado en el ejemplo de `DataFrame` que veremos más adelante, excepto que ha agrupado todos los entradas para cada `id` individual en un solo registro. Esto es similar a los datos que puede recopilar de una base de datos de almacén de documentos o una API web.\n",
    "\n",
    "Cada línea es un diccionario codificado JSON con las siguientes claves\n",
    "\n",
    "- id: Identificador único del cliente\n",
    "- name: Nombre del cliente\n",
    "- transactions: Lista de pares  (transaction-id, amount) , uno para cada transacción para el cliente en ese archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('../data', 'accounts.*.json.gz')\n",
    "lines = db.read_text(filename)\n",
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/accounts.*.json.gz'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestros datos salen del archivo como líneas de texto. Tenga en cuenta que la descompresión de archivos se produjo automáticamente. Podemos hacer que estos datos parezcan más razonables mapeando la función `json.loads` en nuestro `bag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "js = lines.map(json.loads)\n",
    "# take: inspect first few elements\n",
    "js.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que traducimos (parse) nuestros datos JSON en objetos Python adecuados (`dict`s,` list`s, etc.) podemos realizar consultas más interesantes creando pequeñas funciones de Python para ejecutar en nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter: keep only some elements of the sequence\n",
    "js.filter(lambda record: record['name'] == 'Alice').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'Alice', 'count': 18},\n",
       " {'name': 'Alice', 'count': 30},\n",
       " {'name': 'Alice', 'count': 209},\n",
       " {'name': 'Alice', 'count': 189},\n",
       " {'name': 'Alice', 'count': 265})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_transactions(d):\n",
    "    return {'name': d['name'], 'count': len(d['transactions'])}\n",
    "\n",
    "# map: apply a function to each element\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 30, 209, 189, 265)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pluck: select a field, as from a dictionary, element[field]\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .pluck('count')\n",
    "   .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167.1822222222222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average number of transactions for all of the Alice entries\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .pluck('count')\n",
    "   .mean()\n",
    "   .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de  `flatten` des-anidar los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejmeplo usamos `.flatten()` para aplanar los resultados.  Vaos a calcular el monto promedio de todas las transacciones para todas las Alices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'transaction-id': 330, 'amount': 402},\n",
       " {'transaction-id': 1804, 'amount': 372},\n",
       " {'transaction-id': 3347, 'amount': 377})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'transaction-id': 330, 'amount': 402},\n",
       " {'transaction-id': 1804, 'amount': 372},\n",
       " {'transaction-id': 3347, 'amount': 377})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(js.filter(lambda record: record['name']=='Alice')\n",
    " .pluck('transactions')\n",
    " .flatten()\n",
    " .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 372, 377)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .pluck('amount')\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907.8130848575074"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .pluck('amount')\n",
    "   .mean()\n",
    "   .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Groupby y Foldby</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con frecuencia queremos agrupar datos por alguna función o clave. Podemos hacer esto con el método `.groupby`, que es sencillo pero obliga a una mezcla completa de los datos (costoso) o con el método` .foldby`, más difícil de usar pero más rápido, que hace una transmisión combinada groupby y reducción.\n",
    "\n",
    "* `groupby`: Mezcla los datos para que todos los elementos con la misma clave estén en el mismo par clave-valor\n",
    "* `foldby`: recorre los datos acumulando un resultado por clave\n",
    "\n",
    "* Nota: el `groupby` completo es particularmente malo. En cargas de trabajo reales, haría bien en usar `foldby` o cambiar a` DataFrame`s si es posible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupby`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby recopila elementos de su colección para que todos los elementos con el mismo valor en alguna función se recopilen juntos en un par clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, ['Charlie']), (3, ['Dan', 'Bob']), (5, ['Edith', 'Frank', 'Alice'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = db.from_sequence(['Alice', 'Bob', 'Charlie', 'Dan','Edith', 'Frank'])\n",
    "b.groupby(len).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [4, 6, 8, 0, 2]), (1, [3, 5, 7, 9, 1])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = db.from_sequence(list(range(10)))\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (1, 9)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.groupby(lambda x: x % 2).starmap(lambda k, v: (k, max(v))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `foldby`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foldby puede resultar bastante extraño al principio. Es similar a las siguientes funciones de otras bibliotecas:\n",
    "\n",
    "* [`toolz.reduceby`] (http://toolz.readthedocs.io/en/latest/streaming-analytics.html#streaming-split-apply-combine)\n",
    "* [`pyspark.RDD.combineByKey`] (http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)\n",
    "\n",
    "Cuando usa `foldby` usted proporciona\n",
    "\n",
    "1. Una función clave en la que agrupar elementos\n",
    "2. Un operador binario como el que pasaría a `reduce` que usa para realizar la reducción por cada grupo\n",
    "3. Un operador binario combinado que puede combinar los resultados de dos llamadas de `reduce` en diferentes partes de su conjunto de datos.\n",
    "\n",
    "La reducción debe ser asociativa. Ocurrirá en paralelo en cada una de las particiones de su conjunto de datos. Entonces, todos estos resultados intermedios serán combinados por el operador binario `combine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (1, 9)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.foldby(lambda x: x % 2, binop=max, combine=max).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con los datos de account.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar el número de personas con el mismo nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Warning, this one takes a while...\n",
    "result = js.groupby(lambda item: item['name']).starmap(lambda k, v: (k, len(v))).compute()\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 1350), ('Bob', 1044), ('Charlie', 700), ('Dan', 1246), ('Edith', 1200), ('Frank', 800), ('George', 1250), ('Hannah', 950), ('Ingrid', 950), ('Jerry', 850), ('Kevin', 1110), ('Laura', 800), ('Michael', 1089), ('Norbert', 1300), ('Oliver', 800), ('Patricia', 1150), ('Quinn', 850), ('Ray', 1150), ('Sarah', 862), ('Tim', 1126), ('Ursula', 800), ('Victor', 500), ('Wendy', 849), ('Xavier', 578), ('Yvonne', 700), ('Zelda', 844)]\n",
      "CPU times: user 474 ms, sys: 85.9 ms, total: 560 ms\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This one is comparatively fast and produces the same result.\n",
    "from operator import add\n",
    "def incr(tot, _):\n",
    "    return tot + 1\n",
    "\n",
    "result = js.foldby(key='name', \n",
    "                   binop=incr, \n",
    "                   initial=0, \n",
    "                   combine=add, \n",
    "                   combine_initial=0).compute()\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercicio: Calcular el monto total por nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea agrupar con groupby (or foldby) la clave `name`, ty sumar todos los nombes por cada `name`.\n",
    "\n",
    "Pasos\n",
    "\n",
    "1. Cree una pequeña función que, dado un diccionario como \n",
    "\n",
    "        {'name': 'Alice', 'transactions': [{'amount': 1, 'id': 123}, {'amount': 2, 'id': 456}]}\n",
    "        \n",
    "    produce las uma de los montos, por ejemplo `3`\n",
    "    \n",
    "2.  Cambie ligeramente el operador binario del ejemplo `foldby` anterior para que el operador binario no cuente el número de entradas, sino que acumule la suma de las cantidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">DataFrames</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por las mismas razones por las que Pandas es a menudo más rápido que Python puro, `dask.dataframe` puede ser más rápido que` dask.bag`. Trabajaremos más con DataFrames más adelante, pero desde el punto de vista de un Bag, con frecuencia es el punto final de la parte \"desordenada\" (`messy`) de la ingestión de datos: una vez que los datos se pueden convertir en un marco de datos, luego una división compleja -aplicar-combinar la lógica será mucho más sencilla y eficiente.\n",
    "\n",
    "Puede transformar una bolsa con una tupla simple o una estructura de diccionario plano en un `dask.dataframe` con el método` to_dataframe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wendy</td>\n",
       "      <td>[{'transaction-id': 167, 'amount': 1471}, {'tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oliver</td>\n",
       "      <td>[{'transaction-id': 111, 'amount': 338}, {'tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>[{'transaction-id': 11579, 'amount': 1615}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ursula</td>\n",
       "      <td>[{'transaction-id': 258, 'amount': 4056}, {'tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dan</td>\n",
       "      <td>[{'transaction-id': 3422, 'amount': 197}, {'tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     name                                       transactions\n",
       "0   0    Wendy  [{'transaction-id': 167, 'amount': 1471}, {'tr...\n",
       "1   1   Oliver  [{'transaction-id': 111, 'amount': 338}, {'tra...\n",
       "2   2  Charlie  [{'transaction-id': 11579, 'amount': 1615}, {'...\n",
       "3   3   Ursula  [{'transaction-id': 258, 'amount': 4056}, {'tr...\n",
       "4   4      Dan  [{'transaction-id': 3422, 'amount': 197}, {'tr..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = js.to_dataframe()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ahora parece un DataFrame bien definido, y podemos aplicarle cálculos similares a Pandas de manera eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un Dask DataFrame, ¿cuánto tiempo lleva hacer nuestro cálculo previo de la cantidad de personas con el mismo nombre? Resulta que `dask.dataframe.groupby ()` supera a `dask.bag.groupby ()` en más de un orden de magnitud; pero todavía no puede coincidir con `dask.bag.foldby ()` para este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 593 ms, sys: 75.6 ms, total: 669 ms\n",
      "Wall time: 4.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name\n",
       "Alice      1350\n",
       "Bob        1044\n",
       "Charlie     700\n",
       "Dan        1246\n",
       "Edith      1200\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df1.groupby('name').id.count().compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desnormalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este formato de DataFrame es menos que óptimo porque la columna de transacciones está llena de datos anidados, por lo que Pandas tiene que volver al tipo de objeto de objeto, que es bastante lento en Pandas. Idealmente, queremos transformarnos en un marco de datos solo después de haber aplanado nuestros datos para que cada registro sea un solo `int`,` string`, `float`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 0, 'name': 'Wendy', 'amount': 1471, 'transaction-id': 167},\n",
       " {'id': 0, 'name': 'Wendy', 'amount': 1348, 'transaction-id': 211},\n",
       " {'id': 0, 'name': 'Wendy', 'amount': 1528, 'transaction-id': 389})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denormalize(record):\n",
    "    # returns a list for each person, one item per transaction\n",
    "    return [{'id': record['id'], \n",
    "             'name': record['name'], \n",
    "             'amount': transaction['amount'], \n",
    "             'transaction-id': transaction['transaction-id']}\n",
    "            for transaction in record['transactions']]\n",
    "\n",
    "transactions = js.map(denormalize).flatten()\n",
    "transactions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transactions.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# number of transactions per name\n",
    "# note that the time here includes the data load and ingestion\n",
    "df.groupby('name')['transaction-id'].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Limitaciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las colecciones bag proporcionan cálculos muy generales (cualquier función de Python). Esta generalidad\n",
    "tiene un costo. Las bolsas tienen las siguientes limitaciones conocidas\n",
    "\n",
    "1. Las operaciones de bag tienden a ser más lentas que los cálculos de *matrices/marcos de datos* (dataframes)    de la misma manera que Python tiende a ser más lento que *NumPy/Pandas*\n",
    "2. `Bag.groupby` es lento. Debería intentar usar `Bag.foldby` si es posible. El uso de `Bag.foldby` requiere más reflexión. Aún mejor, considere la posibilidad de crear un marco de datos normalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Aprender más</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Bag documentation](https://docs.dask.org/en/latest/bag.html)\n",
    "* [Bag screencast](https://youtu.be/-qIiJ1XtSv0)\n",
    "* [Bag API](https://docs.dask.org/en/latest/bag-api.html)\n",
    "* [Bag examples](https://examples.dask.org/bag.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Shutdown</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
