{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Aprendizaje de máquinas en Paralelo y distribuido <center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask_horizontal.svg\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Introducción](#Introducción)\n",
    "* [Scikit-Learn en 5 minutos](#Scikit-Learn-en-5-minutos)\n",
    "* [Hiperparámetros](#Hiperparámetros)\n",
    "* [Optimización de hiperparámetros](#Optimización-de-hiperparámetros)\n",
    "* [Paralelización con una sola máquina y scikit-learn](#Paralelización-con-una-sola-máquina-y-scikit-learn)\n",
    "* [Paralelización con  varias máquinas con Dask](#Paralelización-con-varias-máquinas-y-Dask)\n",
    "* [Entrenamiento sobre grandes conjuntos de datos](#Entrenamiento-sobre-grandes-conjuntos-de-datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Fuente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una traducción libre del tutorial disponible en [dask-tutorial](https://github.com/dask/dask-tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask-ML](https://dask-ml.readthedocs.io) tiene recursos para aprendizaje de máquinas (`machine learning`).\n",
    "\n",
    "Un panorama de la arquitectura de  trabajo de big data con Python y Dask, o apreciamos en esta imagen\n",
    "![](../images/architecture-1536x947.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hay un par de problemas de escala distintos a los que podemos enfrentarnos.\n",
    "\n",
    "La estrategia de escala depende del problema que se enfrente.\n",
    "\n",
    "1. Vinculado a la CPU: los datos caben en la RAM, pero el entrenamiento lleva demasiado tiempo. Muchas combinaciones de hiperparámetros, un gran conjunto de muchos modelos, etc.\n",
    "2. Límite de memoria: los datos son más grandes que la RAM y el muestreo no es una opción.\n",
    "\n",
    "![](../images/ml-dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para problemas en la memoria, simplemente use scikit-learn (o su biblioteca ML favorita).\n",
    "* Para modelos grandes, use `dask_ml.joblib` y su estimador scikit-learn favorito\n",
    "* Para conjuntos de datos grandes, use estimadores `dask_ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Scikit-Learn en 5 minutos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn tiene una API agradable y consistente.\n",
    "\n",
    "1. usted crea una instancia de un `Estimator` (por ejemplo,` LinearRegression`, `RandomForestClassifier`, etc.). Todos los modelos *hiperparámetros* (parámetros especificados por el usuario, no los aprendidos por el estimador) se pasan al estimador cuando se crea.\n",
    "2. Se llama *estimator.fit (X, y)* para entrenar al estimador.\n",
    "3. Utilice el *estimador* para inspeccionar atributos, hacer predicciones, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generemos algunos datos aleatoriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a ajustar un clasificador de soprte de vectores(support vector machine, `SVM`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un estimados y lo ajustamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(random_state=0)\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionamos los atributos aprendidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequemos la exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Hiperparámetros</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los modelos tienen *hiperparámetros*. Estos afectan al ajuste, pero se especifican por adelantado en lugar de aprenderlos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(C=0.00001, shrinking=False, random_state=0)\n",
    "estimator.fit(X, y)\n",
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Optimización de hiperparámetros</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunas formas de aprender los mejores * hiper * parámetros durante el entrenamiento. Uno es \"GridSearchCV\".\n",
    "Como su nombre lo indica, esto hace una búsqueda de fuerza bruta sobre una cuadrícula de combinaciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Paralelización con una sola máquina y scikit-learn</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn tiene un buen paralelismo  con *una sola máquina*, a través de `Joblib`.\n",
    "Cualquier estimador de scikit-learn que pueda operar en paralelo expone una palabra clave `n_jobs`.\n",
    "Esto controla la cantidad de núcleos de CPU que se utilizarán.![](../images/unmerged_grid_search_graph.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Paralelización con varias máquinas y Dask</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/merged_grid_search_graph.svg)\n",
    "\n",
    "Dask puede hablar con scikit-learn (a través de joblib) para que su *clúster* se use para entrenar un modelo.\n",
    "\n",
    "Si ejecuta esto en una computadora portátil, tomará bastante tiempo, pero el uso de la CPU será satisfactoriamente cercano al 100% mientras dure. Para ejecutar más rápido, necesitaría un clúster distribuido. Eso significaría poner algo en la llamada a \"Cliente\" algo como\n",
    "\n",
    "```\n",
    "c = Client('tcp://my.scheduler.address:8786')\n",
    "```\n",
    "\n",
    "Se pueden encontrar detalles sobre las muchas formas de crear un clúster. [aquí](https://docs.dask.org/en/latest/setup/single-distributed.html).\n",
    "\n",
    "También puede usar un cluster de `Coiled`, `Saturn`, etc, como vimos en la lección anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probémos en un problema mayor (más hiperparámetros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import dask.distributed\n",
    "\n",
    "c = dask.distributed.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    # Uncomment this for larger Grid searches on a cluster\n",
    "    # 'kernel': ['rbf', 'poly', 'linear'],\n",
    "    # 'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Entrenamiento sobre grandes conjuntos de datos</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces querrá entrenar en un conjunto de datos más grande que la memoria. `dask-ml` ha implementado estimadores que funcionan bien con  `dask.array` y  `dask.dataframes` que pueden ser más grandes que la RAM de su máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un pequeño conjunto de datos (aleatorio) localmente usando scikit-learn. `make_blobs`crea muestras de distribuciones Gaussianas con distintos centros. Para entrenamiento de clasificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pequeño conjunto de datos será la plantilla para nuestro gran conjunto de datos aleatorios.\n",
    "Usaremos `dask.delayed` para adaptar` sklearn.datasets.make_blobs`, de modo que el conjunto de datos real se genere en nuestros trabajadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 200000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype=X.dtype)\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.persist()  # Only run this on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos implementados en Dask-ML son escalables. Manejan bien conjuntos de datos más grandes que la memoria.\n",
    "\n",
    "Siguen la API scikit-learn, por lo que si está familiarizado con scikit-learn, se sentirá como en casa con Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
