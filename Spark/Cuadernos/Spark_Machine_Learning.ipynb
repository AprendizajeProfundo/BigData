{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadaaddd-2ed5-4a39-b899-c56bb1fd8428",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbf7e4-76ea-4395-b051-70c5bae45b86",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Spark - Aprendizaje de máquinas<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04432e50-0d01-46cd-804a-ba16c0fda94d",
   "metadata": {},
   "source": [
    "<img src=\"../Imagenes/spark_logo.png\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4fb0eb-908f-4198-a5ce-184984c456e3",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9f85-539e-47c0-90ae-b705111d93f2",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5c6f0-3027-4d1e-a6b3-0a08feb29284",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209f0fe-b922-4cc8-94f5-190a44a8bdc8",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a392a-9dcf-4f18-8dd6-8335db77a0ec",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24e117-fe5f-41e3-986e-3f05322c7f21",
   "metadata": {},
   "source": [
    "\n",
    "* [Introducción](#Introducción)\n",
    "* [Pasos básicos del proceso de aprendizaje de máquinas](#Pasos-básicos-del-proceso-de-aprendizaje-de-máquinas)\n",
    "* [Aprendizaje Supervisado, no supervisado y por refuerzo](#Aprendizaje-Supervizado,-no-supervizado-y-por-refuerzo)\n",
    "* [Spark MLib](#Spark-MLib)\n",
    "* [Aprendizaje supervisado con Spark](#Aprendizaje-supervisado-con-Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c6a4d-0541-4837-bfb2-37030d668474",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3e64d-d43c-4ca8-8749-98737fdd2b4b",
   "metadata": {},
   "source": [
    "1. Basado en [Guía de instalación de Spark en Ubuntu](https://phoenixnap.com/kb/install-spark-on-ubuntu)\n",
    "1. Athul Dev, [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)\n",
    "1. [Guía de instalación de Spark en Windows 10](https://phoenixnap.com/kb/install-spark-on-windows-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746531c-5378-4ca9-a720-6982dfd266ad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb68c7-4a3e-49d1-b6f6-7c58b3adfbe5",
   "metadata": {},
   "source": [
    "Apache Spark es un marco utilizado en entornos de computación en clúster para analizar big data. Esta plataforma se hizo muy popular debido a su facilidad de uso y las velocidades de procesamiento de datos mejoradas en Hadoop.\n",
    "\n",
    "Apache Spark puede distribuir una carga de trabajo en un grupo de computadoras en un clúster para procesar de manera más efectiva grandes conjuntos de datos. Este motor de código abierto admite una amplia gama de lenguajes de programación. Esto incluye Java, Scala, Python y R.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8afb7-b5d6-48de-9680-60781c5fb63d",
   "metadata": {},
   "source": [
    "El concepto de aprendizaje de máquinas   hoy toca muchos segmentos  de la industria y la ciencia básica. Hoy en día hay muchísimos problemas el parendizaje de máquinas puede afrontar.\n",
    "\n",
    "Algunos   problemas importantes que puedan resolverse mediante el aprendizaje automático son\n",
    "\n",
    "+ Resultados de búsqueda web\n",
    "+ Anuncios en tiempo real en páginas web e interfaces móviles\n",
    "+ Predicción de fallas de equipos\n",
    "+ Detección de fraudes\n",
    "+ Motores de recomendación\n",
    "+ Filtro de correo no deseado\n",
    "+ Puntuación crediticia\n",
    "+ Diagnóstico médico\n",
    "+ Análisis de sentimiento de texto\n",
    "\n",
    "La definición tradicional de aprendizaje de máquinas se debe a Arthur Samuel (1950): \"El aprendizaje de máquinas o *aprendizaje automático* es el campo de estudio que le da a las computadoras la capacidad de aprender sin tener que ser programadas para los propósitos específicos del aprendizaje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc2ed5-6490-4a56-a580-ce27a7a20dfe",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Pasos básicos del proceso de aprendizaje de máquinas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4eb5e-c32c-4c3c-8f35-ae66b9312e05",
   "metadata": {},
   "source": [
    "La siguiente imagen muestra el flujo del proceso de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910bce3-79c7-4327-9cb5-54c689cdd038",
   "metadata": {},
   "source": [
    "![](../Imagenes/flujo_ML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36558f7a-ff27-42e8-aa3e-420a955fdc86",
   "metadata": {},
   "source": [
    "### Recopilación o adquisición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4514f-421b-43b1-b2d1-6ad900e0bf01",
   "metadata": {},
   "source": [
    "El ingrediente fundamental del aprendizaje automático son los datos. El proceso de recopilación de datos es un\n",
    "proyecto o tarea específica. El conjunto de datos se puede recopilar de varias fuentes, como un \n",
    "archivo, una  base de datos, sensores y muchas otras fuentes similares, pero los datos recopilados no pueden ser\n",
    "utilizados directamente para realizar cualquier proceso de aprendizaje automático, ya que puede haber una gran cantidad de\n",
    "datos faltantes, valores basura, etc. Por lo tanto, para resolver este problema, se realiza el preprocesamiento o la limpieza de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e2571-1c85-4b83-b5b5-67fba4c41df6",
   "metadata": {},
   "source": [
    "### Preprocesamiento o limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bbafd-8af0-4396-a596-0636d87d8463",
   "metadata": {},
   "source": [
    "El preprocesamiento de datos es el proceso de limpieza de los datos sin procesar. El preprocesamiento de datos es uno de los\n",
    "pasos más importantes en el proceso de aprendizaje automático. Si le damos basura al modelo, obtendremos basura a cambio, es decir, el modelo entrenado proporcionará predicciones incorrectas. Este paso es principalmente responsable de construir una máquina de aprendizaje precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851777f-1d09-4752-9ead-167eb24c1fc2",
   "metadata": {},
   "source": [
    "### Dividir los datos para entrenamiento y pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145a3ff-ae2b-4f0c-9ba7-3b6ea9aff111",
   "metadata": {},
   "source": [
    "En este paso, dividimos aleatoriamente los datos limpios o preprocesados, principalmente en dos\n",
    "fragmentos de datos: un `conjunto de entrenamiento` y un\n",
    "`conjunto de validación` o prueba. El conjunto de entrenamiento se usa para construir nuestro modelo de aprendizaje automático y una vez que el\n",
    "el modelo está entrenado, usamos el conjunto de validación para probar nuestro modelo. Si no dividimos nuestros datos\n",
    "y realizamos el entrenamiento y la validación de nuestro modelo con los mismos datos, es posible que no obtengamos\n",
    "predicciones precisas cuando le proporcionamos datos nuevos o reales, como el alcance de\n",
    "el aprendizaje y las predicciones se limitarán a estos datos no divididos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99550b47-8d8d-4fc1-8ee2-5a8e3817eae9",
   "metadata": {},
   "source": [
    "### Selección de algoritmo / modelo y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3add2-c4fd-426c-bbff-ec1dab3067df",
   "metadata": {},
   "source": [
    "Después de dividir nuestros datos en un conjunto de datos de entrenamiento, tenemos que pensar en un algoritmo que\n",
    "es el mejor traje para nuestro problema. Hay varios algoritmos que se utilizan para entrenar\n",
    "modelo y cada uno de ellos tiene un propósito específico. Aprenderemos sobre algunos de los\n",
    "los algoritmos más comunes en detalle en las próximas secciones. Al decidir un algoritmo\n",
    "o técnica, nos metemos en el proceso de entrenamiento del modelo y para eso pasamos o encajamos\n",
    "nuestros datos a estos algoritmos que luego nos da el objeto de modelo entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bb09e-21fa-4ac2-9ad9-cedbfb81a8f3",
   "metadata": {},
   "source": [
    "### Prueba y evaluación el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ed929-1870-407f-8c16-9699881f7f6b",
   "metadata": {},
   "source": [
    "El modelo entrenado del paso anterior se utiliza para realizar algunas predicciones y\n",
    "evaluaciones sobre el conjunto de datos de prueba. Eso es para evaluar qué tan bien nuestro modelo\n",
    "realizado, lo comparamos con los datos de prueba, y luego realizamos este proceso\n",
    "iterativamente hasta que descubramos los mejores parámetros para nuestro modelo ML.\n",
    "La evaluación del modelo es una parte integral del proceso de desarrollo del modelo. Nos ayuda\n",
    "encontrar el mejor modelo que represente nuestros datos y qué tan bien funcionará el modelo elegido\n",
    "en el futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591929f-e43a-4beb-8fbf-7d320f12d4be",
   "metadata": {},
   "source": [
    "### Despliege (deployment) de la implementación y generación de  conocimientos o predicciones utilizando datos reales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3563e4-4b90-4c64-a9a3-c4d5dfbe6485",
   "metadata": {},
   "source": [
    "Finalmente, sobre la exitosa formación de nuestro modelo y su evaluación. El siguiente paso es\n",
    "llevar este modelo adelante y usarlo con datos reales para obtener varios conocimientos\n",
    "y predicciones basadas en nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e1886-ca27-4f31-84e0-d0c9297b0b68",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Aprendizaje Supervisado, no supervisado y por refuerzo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d9c08-4efa-4ad8-945a-51573ddde202",
   "metadata": {},
   "source": [
    "La teoría moderna de aprendizaje automático comprende tres grandes tipos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58caea-60b7-408f-ba2d-df59cebd10e1",
   "metadata": {},
   "source": [
    "### Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16423faa-4c01-409f-a81b-649c4093cbd0",
   "metadata": {},
   "source": [
    "En los algoritmos de aprendizaje supervisado el entrenamiento se hace utilizando ejemplos etiquetados, como una entrada donde\n",
    "se conoce el resultado deseado, como por ejemplo en los algortimos de regresión o de clasificación. \n",
    "Los algoritmos de aprendizaje supervisado reciben un conjunto de entradas junto con los correspondientes\n",
    "salidas correctas, y el algoritmo aprende comparando su salida predicha con la real\n",
    "salida de los datos para encontrar errores y luego corrige o modifica el modelo en consecuencia.\n",
    "A través de varios métodos como regresión y clasificación, el aprendizaje supervisado utiliza patrones\n",
    "para predecir los valores de la etiqueta en datos adicionales sin etiqueta. Se usa comúnmente en\n",
    "aplicaciones donde los datos históricos predicen probables eventos futuros o etiquetas para datos novedosos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6493ea0-7adf-4360-8620-873dfd932eb4",
   "metadata": {},
   "source": [
    "### Aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66223abb-30e1-42ac-b077-50b572296710",
   "metadata": {},
   "source": [
    "El segundo tipo principal de problema de aprendizaje automático se llama aprendizaje no supervisado.\n",
    "En el aprendizaje supervisado, se nos dijo explícitamente cuál era la llamada respuesta correcta.\n",
    "A diferencia de la técnica de aprendizaje supervisado, en el aprendizaje no supervisado, tratamos con datos que\n",
    "no tiene etiquetas ni datos históricos de etiquetas. Al modelo no se le dice cuál es la \"respuesta correcta\"\n",
    "Pero el objetivo es explorar los datos y encontrar alguna estructura dentro. Los ejemplo clásicos de aprendizaje no supervisado son los algotimos de agrupamiento (clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f392e2-17df-4821-a723-dbc7cf777962",
   "metadata": {},
   "source": [
    "### Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd5c1f-71c5-4bc1-b1de-952be3056ffd",
   "metadata": {},
   "source": [
    "En el aprendizajemopr refuerzo o reforzado, un agente va aprendiendo a traves de la experiencia mediante un sistema de premios y recompensas, que le otorga al agente (la máquina de aprendizaje) una recomponza cuando hace bien una traea y lo castiga cuando no lo hace. Esta área esta más cercana al concepto clasico de inteligencia artificial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bfad0-a01a-4bbc-8df8-09184eae87be",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Spark MLib</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb73da7-3157-4b28-87fc-73336744ed60",
   "metadata": {},
   "source": [
    "Spark tiene su propia biblioteca `MLlib` dedicada al aprendizaje automático. La biblioteca MLlib principalmente\n",
    "utiliza la sintaxis `Spark Dataframe` que hemos visto en la lección anterior. Una cosa importante a tener en cuenta cuando se trata de MLlib es que tendremos que formatear\n",
    "nuestros datos de tal manera que terminamos con una o dos columnas solamente.\n",
    "\n",
    "Al realizar el aprendizaje supervisado, deberíamos tener solo dos columnas independientemente del número\n",
    "de features (variables explicativas), lo que significa que, aunque tenemos muchos features  disponibles, tenemos que\n",
    "condensarlo en solo dos columnas, una es la columna de características y las etiquetas son\n",
    "la segunda columna. \n",
    "\n",
    "\n",
    "Del mismo modo, para el aprendizaje no supervisado deberíamos tener los datos\n",
    "formateado de tal manera que solo tengamos una sola columna de todas los features disponibles. Dado que no hay etiquetas para el aprendizaje no supervisado, se usa este único feature agregado en una columna.\n",
    "columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d4d1e-d895-4052-831f-8d731f2a6a68",
   "metadata": {},
   "source": [
    "Para un referencia completa por favor consulte [Guía MLib ](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "Para  modelos específicos de clasificación y regresión  consulte [Modelos de regresión y clasificación con Spark](https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833afcdf-2947-4bd4-9816-8a0bd40d0d64",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Aprendizaje supervisado con Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7689b-0a5b-4487-ad9a-04963b21115b",
   "metadata": {},
   "source": [
    "En esta sección nos sumergimos en el aprendizaje supervisado, Introducimos la regresión lineal simple, para ilustrar uno de los conceptos claves del aprendizaje automático que la optimización de funciones de costo, mediante el uso del gradiente descendiente estocástico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c9a62-69a2-4151-ba1a-cc2f4d2e08b6",
   "metadata": {},
   "source": [
    "### Regresión lineal simple y gradiente descendiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd03ff2-afd5-48d3-b3f5-9235bca86f63",
   "metadata": {},
   "source": [
    "Asumimos que el lector está familizarizado con el concepto de regresión lineal simple. Recordemos que disponemos de dos variables: una explicativa (`feature`) y una variable explicada (`etiqueta`). En el ejemplo deseamos predecir el costo de las viviendas (etiqueta) en función del tamaño de las vivienda(feature).\n",
    "\n",
    "Para construir el modelo disponemos de un conjunto de observaciones: features  $x_i$  y etiquetas $y_i$, $i=1,\\ldots,n$. La máquina de aprendizaje en este caso es una función de la forma\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x = y,\n",
    "$$\n",
    "\n",
    "en donde lo que debe aprender la máquina es el parámetro $\\mathbf{\\theta} = (\\theta_0, \\theta_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca28fb-083d-4e96-a5d2-82372ce9ba91",
   "metadata": {},
   "source": [
    "### Función de pérdida. Error cuadadrático medio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67482630-2acf-4c9e-a9d5-c6c952999929",
   "metadata": {},
   "source": [
    "Para entrenar nuestra máquina de aprendizaje se requiere de una función objetivo o de costo (*target*), la cual en el lenguaje del aprendizaje automático se conoce como `función de pérdida`.  La función de pérdida incluye los datos recolectados y el parámetro investigado. La función se construye minimizado un criterio predefinido. \n",
    "\n",
    "En los problemas de regresión es común utlizar el error cuadrático medio. Definimos la función de pérdida en el caso de regresión lineal simple como sigue.\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta_0, \\theta_1) =\\frac{1}{2n}\\sum_{i=1}^{n}(y_i - h_{\\theta}(x) )^2 =  \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - \\theta_0 - \\theta_1 x )^2\n",
    "$$\n",
    "\n",
    "El propósito del algortimo de aprendizaje es encontrar el valor del parámetro $\\mathbf{\\theta} = (\\theta_0, \\theta_1)$ que minimiza la función de pérdida $\\mathfrak{L}$. La imagen ilustra una función de pérdida en un problema de regresión lineal simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a857d8b-f687-4054-a429-1811881b19d8",
   "metadata": {},
   "source": [
    "![](../Imagenes/funcion_perdida_regresion.png)\n",
    "\n",
    "Función de pérdida. Regresión lineal simple. Fuente: https://towardsdatascience.com/linear-regression-cost-function-gradient-descent-normal-equations-1d2a6c878e2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982ff28-e89e-4504-9ea4-ed5601f0098e",
   "metadata": {},
   "source": [
    "### Gradiente descendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc434ecd-39a4-4652-b63b-540b84946693",
   "metadata": {},
   "source": [
    "El método del gradiente descendiente consiste en buscar la dirección en la cual función baja más rapidamente y seguir ese camino. Tal dirección se dada por la gradiente de la función de pérdida, el cual denotamos genéricamente por $\\nabla_{\\theta}$. En este caso se tiene que $\\nabla_{\\theta} = (\\partial \\mathcal{J}/ \\partial \\theta_0, \\partial \\mathcal{J}/ \\partial \\theta_1)$.\n",
    "\n",
    "El algoritmo del gradiente descendiente es el siguiente\n",
    "\n",
    "1. Paso 0. Obtener valores iniciales para $\\mathbf{\\theta} = (\\theta_0, \\theta_1)$. Denotamos tales valores se denotaran como $(\\theta_0^{(0)}, \\theta_1{(0)})=$.\n",
    "2. Paso $t+1$. Actualiza $\\mathbf{\\theta}$ así: $\\theta_j^{(t+1)} = \\theta_j^{(t)} - \\alpha_t \\partial \\mathcal{J}(\\mathbf{\\theta}^{(t)})/ \\partial \\theta_j)$, para $j=1,2$.\n",
    "\n",
    "El valor $\\alpha_t$ es un número positivo que se conoce como rata de aprendizaje y que puede cambiar a lo largo de las iteraciones algortimo. Para los detalles revise la lección de [gradiente descendiente estocástico](https://nbviewer.jupyter.org/github/AprendizajeProfundo/Diplomado/blob/master/Temas/M%C3%B3dulo%203-%20Introducci%C3%B3n%20al%20aprendizaje%20de%20m%C3%A1quinas/1.%20Introducci%C3%B3n%20a%20IA/Cuadernos/am-sdg.ipynb). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793a39-dbde-432b-9d3e-ca3c2e2d8daf",
   "metadata": {},
   "source": [
    "### Gradiente descendiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ced7d-32b6-4570-9948-db5694f476ca",
   "metadata": {},
   "source": [
    "El problema con grandes cantidades de datos (big data) es que el cálculo de la función de pérdida es muy pesado, debido a la cantidad de sumandos. Para solucionar esto se usa una variante del gradiente descendiente en el cual en cada paso $t$ se toma una muestra de los datos para la actualización del gradiente. Para los detalles consulte la lección de [gradiente descendiente estocástico](https://nbviewer.jupyter.org/github/AprendizajeProfundo/Diplomado/blob/master/Temas/M%C3%B3dulo%203-%20Introducci%C3%B3n%20al%20aprendizaje%20de%20m%C3%A1quinas/1.%20Introducci%C3%B3n%20a%20IA/Cuadernos/am-sdg.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd92602-dd48-4306-9b75-be7b7ad5579c",
   "metadata": {},
   "source": [
    "### Metricas de evaluación para regresión lineal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f472f-7cca-4176-a0de-aa03d2e0776c",
   "metadata": {},
   "source": [
    "Para evaluar la claidad del modelo se usan distintas métricas, las cuales se aplican tanto a los datos de entrenamiento como a los datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e942aa-45d6-4a94-891a-abb46e1178de",
   "metadata": {},
   "source": [
    "#### Error Absoluto medio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9d4ea-f71a-4b7b-8557-512b397e2c68",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MAE} = \\frac{1}{m} \\sum_{j=1}^{m} |y_i - h_{\\theta}(x_i)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b0d77-cfa7-4949-bbbe-82a3fbec152f",
   "metadata": {},
   "source": [
    "#### Error Cuadrático Medio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2386023-ef15-41d7-8817-288b001f2f0d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{j=1}^{m} (y_i - h_{\\theta}(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d69231-b55c-478e-9a64-d359f0a9e71b",
   "metadata": {},
   "source": [
    "#### Raíz Error Cuadrático Medio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208d332-3343-48f7-a099-8a7651e00326",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{m} \\sum_{j=1}^{m} (y_i - h_{\\theta}(x_i))^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83702b6b-71e5-422f-98c9-285a15fc8e06",
   "metadata": {},
   "source": [
    "#### Valores de tipo $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23374533-b13c-4a51-9ce8-bb158a7ff12a",
   "metadata": {},
   "source": [
    "Los valores de tipo  $R^2$ son básicamente  medidas estadísticas de nuestro modelo de regresión. Tambien son\n",
    "conocidos como el coeficientes de determinación. Mide cuánta varianza nuestro modelo representa, en relación con la varianza total de los datos, en el rango 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2835d0f7-104f-493f-86f2-7df32b415440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+----------+----------+\n",
      "|house_size|price_sold|\n",
      "+----------+----------+\n",
      "|      1490|        60|\n",
      "|      2500|        95|\n",
      "|      1200|        55|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Data after adding house_size column as a spark accepted feature\n",
      "+----------+----------+------------------+\n",
      "|house_size|price_sold|house_size_feature|\n",
      "+----------+----------+------------------+\n",
      "|      1490|        60|          [1490.0]|\n",
      "|      2500|        95|          [2500.0]|\n",
      "+----------+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- house_size: integer (nullable = true)\n",
      " |-- price_sold: integer (nullable = true)\n",
      " |-- house_size_feature: vector (nullable = true)\n",
      "\n",
      "Consolidated Data with accepted features and labels\n",
      "+------------------+----------+\n",
      "|house_size_feature|price_sold|\n",
      "+------------------+----------+\n",
      "|          [1490.0]|        60|\n",
      "|          [2500.0]|        95|\n",
      "|          [1200.0]|        55|\n",
      "+------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Residuals info - distance between data points and fitted regression line\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-0.2637338379060168|\n",
      "| 1.0541469567959751|\n",
      "|  3.846010722169666|\n",
      "| 1.3777041942605024|\n",
      "+-------------------+\n",
      "\n",
      "Root Mean Square Error 2.113684500209638\n",
      "R square value 0.9881396059958812\n",
      "\n",
      "Predictions for Novel Data\n",
      "+------------------+------------------+\n",
      "|house_size_feature|        prediction|\n",
      "+------------------+------------------+\n",
      "|           [850.0]| 43.26373383790602|\n",
      "|          [1200.0]|53.945853043204025|\n",
      "|          [1600.0]| 66.15398927783033|\n",
      "|          [2500.0]|  93.6222958057395|\n",
      "+------------------+------------------+\n",
      "\n",
      "Coeffecient is 0.03052034058656575\n",
      "Intercept is 17.32144433932513\n",
      "\n",
      "Predicted house price for house size 950 is 46.31576789656259\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Importación de datos y bibliotecas esencialess\n",
    "\n",
    "# busca Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# sesión Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SingleVariableLinearReg').getOrCreate()\n",
    "\n",
    "# objetos para regresión lineal\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# datos\n",
    "data = spark.read.csv('../Datos/single_variable_regression.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "#importa the VectorAssembler para convertir  features en le formato aceptado por Spark \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "# Paso 2: preprocesamiento de datos y conversión de datos al formato aceptado de Spark\n",
    "\n",
    "# conversión de la(s) variables(s) en formato de datos aceptado por Spark\n",
    "assembler_object = VectorAssembler(inputCols=['house_size'],outputCol='house_size_feature')\n",
    "feature_vector_dataframe = assembler_object.transform(data)\n",
    "print(\"Data after adding house_size column as a spark accepted feature\")\n",
    "\n",
    "feature_vector_dataframe.show(2)\n",
    "feature_vector_dataframe.printSchema()\n",
    "formatted_data = feature_vector_dataframe.select('house_size_feature','price_sold')\n",
    "print(\"Consolidated Data with accepted features and labels\")\n",
    "formatted_data.show(3)\n",
    "\n",
    "# Paso 3: Entrenamiento de nuestro modelo de regresión lineal con una sola variable\n",
    "\n",
    "# Dividir los datos en 70 y 30 por ciento\n",
    "\n",
    "train_data, test_data = formatted_data.randomSplit([0.7,0.3]) \n",
    "\n",
    "# Definiendo la regresión lineal\n",
    "lireg = LinearRegression(featuresCol='house_size_feature',labelCol='price_sold')\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "lireg_model = lireg.fit(train_data)\n",
    "\n",
    "# Paso 4 - Evaluación del modelo entrenado\n",
    "\n",
    "# Evaluar nuestro modelo con datos de prueba\n",
    "test_results = lireg_model.evaluate(test_data)\n",
    "print(\"Residuals info - distance between data points and fitted regression line\")\n",
    "test_results.residuals.show(4)\n",
    "print(\"Root Mean Square Error {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"R square value {}\".format(test_results.r2))\n",
    "\n",
    "# Paso 5 - Realización de predicciones con datos novedosos\n",
    "\n",
    "# Crear datos sin etiquetar a partir de datos de prueba quitando la etiqueta para obtener predicciones\n",
    "unlabeled_data =  test_data.select('house_size_feature')\n",
    "predictions = lireg_model.transform(unlabeled_data)\n",
    "print(\"\\nPredictions for Novel Data\")\n",
    "predictions.show(4)\n",
    "\n",
    "#Checking our model with new value manually\n",
    "house_size_coeff=lireg_model.coefficients[0]\n",
    "intercept = lireg_model.intercept\n",
    "print(\"Coeffecient is {}\".format(house_size_coeff))\n",
    "print(\"Intercept is {}\".format(intercept))\n",
    "new_house_size = 950\n",
    "\n",
    "#Mimicking the hypothesis function to get a prediction\n",
    "price = (intercept) + (house_size_coeff)*new_house_size\n",
    "print(\"\\nPredicted house price for house size {} is {}\".format(new_house_size,price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dede-fb7b-4bd9-9330-abedd53a3d99",
   "metadata": {},
   "source": [
    "### Regresión lineal múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d42e72-ea6b-4909-8de5-5bcb4a57236e",
   "metadata": {},
   "source": [
    "En este caso consideramos los siguientes features:\n",
    "\n",
    "* Tamaño de la casa: $x_1$\n",
    "* Número de cuartos: $x_2$\n",
    "* Número de pisos: $x_3$\n",
    "* Edad de la vivienda: $x_4$\n",
    "* Area residencial (Categórica): $x_5$\n",
    "\n",
    "Y la estiqueta es \n",
    "\n",
    "* Precio de venta: $y_i$\n",
    "\n",
    "La función $h_{\\theta}(x)$ es dada por\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2x_2 +\\theta_3x_3 +\\theta_4x_4 +\\theta_5x_5= y,\n",
    "$$\n",
    "\n",
    "Veámos el código Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be79793e-bf19-4ded-bd98-95e6098fd6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "|house_size|bedrooms|floors|house_age|      area|price_sold|\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "|      1490|       2|     2|       10|Ave Avenue|        60|\n",
      "|      2500|       3|     2|       20|Ave Avenue|        95|\n",
      "|      1200|       2|     1|        5|   MG Road|        55|\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Data after converting the string column locality into spark accepted feature\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "|house_size|bedrooms|floors|house_age|          area|price_sold|area_feature|\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "|      1490|       2|     2|       10|    Ave Avenue|        60|         0.0|\n",
      "|      2500|       3|     2|       20|    Ave Avenue|        95|         0.0|\n",
      "|      1200|       2|     1|        5|       MG Road|        55|         1.0|\n",
      "|       900|       2|     2|       15|       MG Road|        45|         1.0|\n",
      "|      1350|       2|     2|        5|Dollors Colony|        65|         2.0|\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Columns present in our Data and a sample row value\n",
      "\n",
      "['house_size', 'bedrooms', 'floors', 'house_age', 'area', 'price_sold', 'area_feature']\n",
      "[Row(house_size=1490, bedrooms=2, floors=2, house_age=10, area='Ave Avenue', price_sold=60, area_feature=0.0, house_features=DenseVector([1490.0, 2.0, 2.0, 10.0, 0.0]))]\n",
      "root\n",
      " |-- house_size: integer (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- house_age: integer (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- price_sold: integer (nullable = true)\n",
      " |-- area_feature: double (nullable = false)\n",
      " |-- house_features: vector (nullable = true)\n",
      "\n",
      "Consolidated Data with accepted features and labels\n",
      "+--------------------+----------+\n",
      "|      house_features|price_sold|\n",
      "+--------------------+----------+\n",
      "|[1490.0,2.0,2.0,1...|        60|\n",
      "|[2500.0,3.0,2.0,2...|        95|\n",
      "|[1200.0,2.0,1.0,5...|        55|\n",
      "+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Residuals info - distance between data points and fitted regression line\n",
      "+------------------+\n",
      "|         residuals|\n",
      "+------------------+\n",
      "|11.709886697250496|\n",
      "|13.505388372201566|\n",
      "| 4.329170547281549|\n",
      "| 12.53869985215649|\n",
      "+------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Root Mean Square Error 14.36928208415197\n",
      "R square value 0.3676664304323368\n",
      "\n",
      "Predictions for Novel Data\n",
      "+--------------------+------------------+\n",
      "|      house_features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[750.0,1.0,1.0,1....|28.290113302749504|\n",
      "|[850.0,1.0,1.0,5....|26.494611627798434|\n",
      "|[900.0,2.0,2.0,15...| 40.67082945271845|\n",
      "|[1000.0,2.0,1.0,2...| 47.46130014784351|\n",
      "+--------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Coeffecients are [0.013897402279883706,9.344995928099594,-2.1968451053418634,-0.7963104757348596,-7.14815082275835]\n",
      "\n",
      "Intercept is 32.9596737140889\n",
      "\n",
      "Predicted house price for the house of size 1750, having 3 bedrooms ,2 floors and the age of the house being 5 is 76.93987289882615\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: importar los datos y las bibliotecas esenciales\n",
    "\n",
    "# busca Spark\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('MultiVariableLinearReg').getOrCreate()\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "data = spark.read.csv('../Datos/multi_variable_regression.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "# importar el VectorAssembler para convertir las características al formato aceptado por Spark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Paso 2: preprocesamiento de datos y conversión de cualquier cadena de datos al formato aceptado de Spark\n",
    "\n",
    "# importar el StringIndexer para convertir la característica de localidad en el formato aceptado de Spark\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# convertir la característica de localidad del tipo de cadena en formato de datos aceptado por Spark\n",
    "string_index_object = StringIndexer(inputCol='area',outputCol='area_feature')\n",
    "string_indexed_df_object = string_index_object.fit(data)\n",
    "final_data = string_indexed_df_object.transform(data)\n",
    "print(\"Data after converting the string column locality into spark accepted feature\")\n",
    "final_data.show(5)\n",
    "print(\"Columns present in our Data and a sample row value\\n\")\n",
    "print(final_data.columns)\n",
    "\n",
    "# Paso 3: preprocesamiento de datos y conversión de datos numéricos al formato aceptado de Spark\n",
    "\n",
    "# convirtiendo la (s) característica (s) en formato de datos aceptado por Spark\n",
    "# Pasar varias columnas como columnas de entrada\n",
    "assembler_object = VectorAssembler(inputCols=['house_size', 'bedrooms', 'floors','house_age', 'area_feature'], outputCol='house_features')\n",
    "feature_vector_dataframe = assembler_object.transform(final_data)\n",
    "print(feature_vector_dataframe.head(1))\n",
    "feature_vector_dataframe.printSchema()\n",
    "formatted_data = feature_vector_dataframe.select('house_features','price_sold')\n",
    "print(\"Consolidated Data with accepted features and labels\")\n",
    "formatted_data.show(3)\n",
    "\n",
    "# # Paso 4: Entrenamiento de nuestro modelo de regresión lineal con múltiples variables\n",
    "\n",
    "# Dividir los datos en 60 y 40 por ciento\n",
    "train_data, test_data = formatted_data.randomSplit([0.6,0.4]) \n",
    "\n",
    "# Definir la regresión lineal\n",
    "lireg = LinearRegression(featuresCol='house_features',labelCol='price_sold')\n",
    "\n",
    "# Training our model with training data\n",
    "lireg_model = lireg.fit(train_data)\n",
    "\n",
    "# Paso 5: evaluación del modelo capacitado\n",
    "\n",
    "# Evaluar el modelo con datos de prueba\n",
    "test_results = lireg_model.evaluate(test_data)\n",
    "print(\"Residuals info - distance between data points and fitted regression line\")\n",
    "test_results.residuals.show(4)\n",
    "print(\"Root Mean Square Error {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"R square value {}\".format(test_results.r2))\n",
    "\n",
    "# Paso 6: realizar predicciones con datos novedosos\n",
    "\n",
    "#Crear datos sin etiquetar a partir de datos de prueba quitando la etiqueta para obtener predicciones\n",
    "unlabeled_data =  test_data.select('house_features')\n",
    "predictions = lireg_model.transform(unlabeled_data)\n",
    "print(\"\\nPredictions for Novel Data\")\n",
    "predictions.show(4)\n",
    "\n",
    "# Comprobando nuestro modelo con nuevo valor manualmente\n",
    "print(\"Coeffecients are {}\".format(lireg_model.coefficients))\n",
    "print(\"\\nIntercept is {}\".format(lireg_model.intercept))\n",
    "new_house_size = 1750\n",
    "new_house_number_of_bedrooms = 3\n",
    "new_house_number_of_floors = 2\n",
    "new_house_age = 5\n",
    "\n",
    "# Imitando la función de hipótesis para obtener una predicción\n",
    "new_price = ((lireg_model.intercept) + (lireg_model.coefficients[0])*new_house_size +(lireg_model.coefficients[1])*new_house_number_of_bedrooms +      \n",
    "             (lireg_model.coefficients[2])*new_house_number_of_floors + (lireg_model.coefficients[3])*new_house_age)\n",
    "print(\"\\nPredicted house price for the house of size {}, having {} bedrooms ,{} floors and the age of the house being {} is {}\".format(new_house_size,new_house_number_of_bedrooms,new_house_number_of_floors,new_house_age,new_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455bc53-7243-4b8a-8745-ac5b0f41adaa",
   "metadata": {},
   "source": [
    "### Regresión y clasificación Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814c072-2cf9-4654-8474-cb86d0e4277e",
   "metadata": {},
   "source": [
    "En este caso el problema se plantea a partir de modelar la probabilidad de un suceso de interés. En el ejemplo de esta sección desarrollamos un predictor binario de que un tumor sea maligno, o no, con base en tres variables explicativas (features): edad, sexo y tamaño del tumor. \n",
    "\n",
    "Supongamos que $h_{\\theta}(x)$ se define mediante\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\frac{1}{ 1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3)}}\n",
    "$$\n",
    "\n",
    "La probabilidad condicional  de que un un tumor sea maligno (suceso de interés) dados las variables $x$ y el parámetro de interés $\\theta$ es definda por\n",
    "\n",
    "$$\n",
    "Prob(Y_i=1|x, \\theta) = h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "La función de pérdida en este caso es definida por el negativo de la log verosimilitud promedio. Si $x_i = (x_{1i}, x_{2i}, x_{3i})$ son las variables observadas y $y_i$ las etiquetas, para $i = 1,\\ldots, N$, se tiene que la función de pérdida es dada por\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} y_i\\log(h_{\\theta}(x_i)) + (1-y_i)\\log(1- h_{\\theta}(x_i))\n",
    "$$\n",
    "en donde $m$ es el tamaño del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebc163-8c95-43cf-b21a-305e571d55fe",
   "metadata": {},
   "source": [
    "### Frontera de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1baee-9bfb-4727-b42b-1af12021a789",
   "metadata": {},
   "source": [
    "Como $h_{\\theta}(x)$ es la probabilidad de detectar la condición especial (maligno) en este caso, las predicciones se hacen de acuerdo con la siguiente regla:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} \n",
    "&1, \\text{ si } h_{\\theta}(x)>0.5 \\\\\n",
    "&0, \\text{ si } h_{\\theta}(x)<0.5 \\end{cases}\n",
    "$$\n",
    "\n",
    "Si $h_{\\theta}(x)=0$, no hay decisión. Por lo general es prudente establecer un banda alrededor de 0.5 para tomar decisiones más seguras. Digamos, no hay decision si $0.49<h_{\\theta}(x)<0.51$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51c263-10e2-449e-8a5b-5b8052a29e99",
   "metadata": {},
   "source": [
    "### Evaluación del modelo logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759622e2-34d9-4962-8205-29c495532a9c",
   "metadata": {},
   "source": [
    "Podemos evaluar la regresión logística o, en ese sentido, cualquier algoritmo de clasificación o\n",
    "técnicas que utilizan la matriz de confusión.\n",
    "\n",
    "Matriz de confusión es básicamente una verificación de medición entre la condición pronosticada o lo que\n",
    "predijimos que el valor de la etiqueta sería frente a las condiciones reales o lo que la etiqueta real\n",
    "los valores eran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61423e18-e176-45ef-b76e-da8e9e3339ae",
   "metadata": {},
   "source": [
    "![](../Imagenes/matriz_confusion.png)\n",
    "\n",
    "Matriz de confusión. Fuente: [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc351c7-18df-49eb-93e4-8a40fc1be2ea",
   "metadata": {},
   "source": [
    "#### Terminología"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3169052-8b6b-4de3-a66a-c1e6103c835a",
   "metadata": {},
   "source": [
    "* Verdadero positivo (TP): Nuestro modelo predijo positivo y la etiqueta real también fue positiva. Ejemplo: nuestro modelo predijo que el tumor es maligno y la persona tiene cáncer.\n",
    "* Verdadero negativo (TN): Nuestro modelo predijo negativo y la etiqueta real también fue negativa. Ejemplo: nuestro modelo predijo que el tumor sería benigno y la persona no tiene cáncer.\n",
    "* Falso positivo (FP): Nuestro modelo predijo positivo y la etiqueta real fue negativa. Ejemplo: nuestro modelo predijo que el tumor sería maligno, pero la persona no tiene cáncer.\n",
    "* Falso negativo (FN): Nuestro modelo predijo negativo, pero la etiqueta real fue positiva. Ejemplo: nuestro modelo predijo que el tumor sería benigno pero la persona tiene cáncer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2f411-d749-444d-be0e-71d87f5e9697",
   "metadata": {},
   "source": [
    "### Métricas de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52355008-9e2c-4eee-9e9a-57938086dd8c",
   "metadata": {},
   "source": [
    "* `Exactitud`. La exactitud nos dice con qué frecuencia nuestro modelo es correcto, cuanto mayor sea el valor, mejor. Es calculado como la suma de verdaderos positivos y verdaderos negativos dividida por el total población. Eso significa cuántos acertamos de la población total.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum TP + \\sum TN}{ \\sum \\text{Total Poblacion}}\n",
    "$$\n",
    "\n",
    "* `Recuperación o sensibilidad o tasa de verdaderos positivos (TPR)`. De todas las muestras positivas, cuánto predijimos correctamente, eso se predice positivo. Debe ser lo más alto posible. La recuperación se calcula como el total positivo dividido por la condición positiva o la suma de verdaderos positivos y falsos negativos.\n",
    "\n",
    "$$\n",
    "\\text{Recall = TPR} = \\frac{\\sum TP }{ \\sum \\text{Condición positiva}} = \\frac{\\sum TP }{ \\sum TP + \\sum FN}\n",
    "$$\n",
    "\n",
    "* `Precisión`. De todas las muestras  que hemos predicho como positivas, ¿Cuántas en porcentaje son realmente positivas?. La precisión se calcula como el total positivo dividido por la condición predicha positivo o la suma de verdaderos positivos y falsos positivos.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\sum TP }{ \\sum \\text{Condición positiva}} = \\frac{\\sum TP }{ \\sum TP + \\sum FP}\n",
    "$$\n",
    "\n",
    "\n",
    "* `Especificidad`. De todas las muestras negativas, cuánto predijimos correctamente. La `sensibilidad` se calcula como el total negativo dividido por la condición  negativo o la suma de falso positivo y verdadero negativo.\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\frac{\\sum TN }{ \\sum \\text{Condición negativa}} = \\frac{\\sum TN }{ \\sum TN + \\sum FP}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "*  `Tasa Falso positivo (FPR)`. Se calcula como la proporción de falsos positivos totales por la condición negativa o la suma de falso positivo y verdadero negativo.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\sum FP }{ \\sum \\text{Condición negativa}} = \\frac{\\sum FP }{ \\sum TN + \\sum FP}\n",
    "$$\n",
    "\n",
    "Todas estas métricas de evaluación son formas fundamentales de comparar nuestros valores predichos con\n",
    "los verdaderos valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbeffd-1a86-419d-ab20-c883c85cb8a1",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4920af5-5666-4c3d-8eb3-9582e5aa8994",
   "metadata": {},
   "source": [
    "ROC - Receiver Operator Curve es una métrica de evaluación que se puede utilizar para Clasificación. Básicamente es una visualización de métricas derivadas de la matriz de confusión. La curva ROC es básicamente una gráfica de la Sensibilidad (tasa de verdaderos positivos) contra la Especificidad (tasa de falsos positivos), como se muestra en la Figura.\n",
    "\n",
    "Lo ideal es que el area bajo la curva (`AUC`) sea lo máximo posible. Nótese que el área bajo la curva está entre 0 y 1. Para ver como se construye esta curva y todos los detalles de interpretación revise por ejemplo [Entendiendo las Curvas ROC](https://www.youtube.com/watch?v=Y1XAP6omGzo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c7911-6d16-4ed3-ab80-f3731031a15f",
   "metadata": {},
   "source": [
    "![](../Imagenes/roc.png)\n",
    "\n",
    "Curva ROC. Fuente: [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8fa548-b347-4bcc-8307-003155b52c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+------+---+----+----------+---------+\n",
      "|  name|age| sex|tumor_size|cancerous|\n",
      "+------+---+----+----------+---------+\n",
      "|Roland| 58|Male|       7.0|        1|\n",
      "| Adolf| 65|Male|       9.0|        1|\n",
      "| Klaus| 50|Male|       3.0|        0|\n",
      "+------+---+----+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Data after OneHotEncoding\n",
      "+-------+---+------+----------+---------+----------+-------------+\n",
      "|   name|age|   sex|tumor_size|cancerous|sexIndexer|    sexVector|\n",
      "+-------+---+------+----------+---------+----------+-------------+\n",
      "| Roland| 58|  Male|       7.0|        1|       0.0|(1,[0],[1.0])|\n",
      "|  Adolf| 65|  Male|       9.0|        1|       0.0|(1,[0],[1.0])|\n",
      "|  Klaus| 50|  Male|       3.0|        0|       0.0|(1,[0],[1.0])|\n",
      "|   Rosh| 26|Female|       2.0|        0|       1.0|    (1,[],[])|\n",
      "|Li Ning| 39|Female|       9.0|        1|       1.0|    (1,[],[])|\n",
      "|  Glory| 55|Female|       1.0|        0|       1.0|    (1,[],[])|\n",
      "|    Kim| 23|  Male|       4.0|        0|       0.0|(1,[0],[1.0])|\n",
      "|Gebhard| 40|  Male|       6.0|        1|       0.0|(1,[0],[1.0])|\n",
      "+-------+---+------+----------+---------+----------+-------------+\n",
      "only showing top 8 rows\n",
      "\n",
      "Consolidated Data with accepted features and labels\n",
      "+--------------+---------+\n",
      "|      features|cancerous|\n",
      "+--------------+---------+\n",
      "|[58.0,1.0,7.0]|        1|\n",
      "|[65.0,1.0,9.0]|        1|\n",
      "|[50.0,1.0,3.0]|        0|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Prediction Data\n",
      "+---------------+---------+----------+\n",
      "|       features|cancerous|prediction|\n",
      "+---------------+---------+----------+\n",
      "| [26.0,0.0,2.0]|        0|       0.0|\n",
      "| [27.0,0.0,7.2]|        1|       1.0|\n",
      "|[33.0,1.0,10.5]|        1|       1.0|\n",
      "+---------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Area Under the Curve value is 0.9\n",
      "\n",
      "Coeffecients are [-0.17322181621385932,-30.25433368024052,8.266251051501898]\n",
      "\n",
      "Intercept is -15.876386233067702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paso 1: importar los datos y las bibliotecas esenciales\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkLogReg').getOrCreate()\n",
    "data = spark.read.csv('../Datos/brain_tumor_dataset.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "# Paso 2: preprocesamiento de datos y conversión de cualquier cadena de datos al formato aceptado de Spark\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer,StringIndexer,OneHotEncoder\n",
    "\n",
    "# Dar formato a la columna categórica: sex \n",
    "# Creación de un indexador de cadenas: para convertir cada cadena en un número único\n",
    "sex_string_indexer_direct = StringIndexer(inputCol='sex',outputCol='sexIndexer')\n",
    "indexed_data = sex_string_indexer_direct.fit(data)\n",
    "final_string_indexed_data = indexed_data.transform(data)\n",
    "\n",
    "# Hombre - 1 and Mujer 0 or viceversa\n",
    "# Ejecutando  OneHotEncoding - convertir este valor en una forma de array\n",
    "sex_encoder_direct = OneHotEncoder(inputCol='sexIndexer',outputCol='sexVector')\n",
    "\n",
    "sex_encoder_direct = sex_encoder_direct.fit(final_string_indexed_data)\n",
    "\n",
    "\n",
    "#single_col_model = single_col_ohe.fit(df)\n",
    "# |  >>> single_col_model.transform(df).head().output\n",
    "\n",
    "\n",
    "encoded_data = sex_encoder_direct.transform(final_string_indexed_data)\n",
    "\n",
    "# Hombre - [1,0] y Mujer - [0,1] or viceversa\n",
    "print(\"Data after OneHotEncoding\")\n",
    "encoded_data.show(8)\n",
    "assembler_direct = VectorAssembler(inputCols=['age','sexVector','tumor_size'],outputCol='features')\n",
    "assembler_data = assembler_direct.transform(encoded_data)\n",
    "final_data_direct = assembler_data.select('features','cancerous')\n",
    "print(\"Consolidated Data with accepted features and labels\")\n",
    "final_data_direct.show(3)\n",
    "\n",
    "#Step 3 - Entrenamiento del modelo logístico\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logreg_direct = LogisticRegression(featuresCol='features',labelCol='cancerous')\n",
    "train_data_direct,test_data_direct = final_data_direct.randomSplit([0.6,0.4])\n",
    "logreg_model_direct = logreg_direct.fit(train_data_direct)\n",
    "\n",
    "#Step 4 - Evaluar y realizar predicciones con el  modelo\n",
    "\n",
    "# Evaluación de el  modelo con datos de prueba\n",
    "# Evaluación directa usando el método Trivial\n",
    "predictions_labels = logreg_model_direct.evaluate(test_data_direct)\n",
    "print(\"Prediction Data\")\n",
    "predictions_labels.predictions.select(['features','cancerous','prediction']).show(3)\n",
    "\n",
    "# Evaluación mediante BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "direct_evaluation = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='cancerous')\n",
    "AUC_direct = direct_evaluation.evaluate(predictions_labels.predictions)\n",
    "print(\"Area Under the Curve value is {}\".format(AUC_direct))\n",
    "print(\"\\nCoeffecients are {}\".format(logreg_model_direct.coefficients))\n",
    "print(\"\\nIntercept is {}\".format(logreg_model_direct.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b21817-8157-4192-ba97-2a5a0fc97a51",
   "metadata": {},
   "source": [
    "### Modelos lineales generalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b72cc-580b-4471-8833-31951e0a9162",
   "metadata": {},
   "source": [
    "Aquí repetimos el problema de regresión lineal múltiple para introducir los modelos lineales generalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bd74da-500c-4ef1-bc87-2f139865b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "|house_size|bedrooms|floors|house_age|      area|price_sold|\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "|      1490|       2|     2|       10|Ave Avenue|        60|\n",
      "|      2500|       3|     2|       20|Ave Avenue|        95|\n",
      "|      1200|       2|     1|        5|   MG Road|        55|\n",
      "+----------+--------+------+---------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Data after converting the string column locality into spark accepted feature\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "|house_size|bedrooms|floors|house_age|          area|price_sold|area_feature|\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "|      1490|       2|     2|       10|    Ave Avenue|        60|         0.0|\n",
      "|      2500|       3|     2|       20|    Ave Avenue|        95|         0.0|\n",
      "|      1200|       2|     1|        5|       MG Road|        55|         1.0|\n",
      "|       900|       2|     2|       15|       MG Road|        45|         1.0|\n",
      "|      1350|       2|     2|        5|Dollors Colony|        65|         2.0|\n",
      "+----------+--------+------+---------+--------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Columns present in our Data and a sample row value\n",
      "\n",
      "['house_size', 'bedrooms', 'floors', 'house_age', 'area', 'price_sold', 'area_feature']\n",
      "[Row(house_size=1490, bedrooms=2, floors=2, house_age=10, area='Ave Avenue', price_sold=60, area_feature=0.0, house_features=DenseVector([1490.0, 2.0, 2.0, 10.0, 0.0]))]\n",
      "root\n",
      " |-- house_size: integer (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- house_age: integer (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- price_sold: integer (nullable = true)\n",
      " |-- area_feature: double (nullable = false)\n",
      " |-- house_features: vector (nullable = true)\n",
      "\n",
      "Consolidated Data with accepted features and labels\n",
      "+--------------------+----------+\n",
      "|      house_features|price_sold|\n",
      "+--------------------+----------+\n",
      "|[1490.0,2.0,2.0,1...|        60|\n",
      "|[2500.0,3.0,2.0,2...|        95|\n",
      "|[1200.0,2.0,1.0,5...|        55|\n",
      "+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Coefficients: [0.023602045904839938,14.143836606760098,-0.20326728984150289,-0.17673507138088432,3.846271624045552]\n",
      "Intercept: -2.6597514131500386\n",
      "Coefficient Standard Errors: [0.0043933182306061256, 4.507964077683095, 2.5308610466489396, 0.20709091030458388, 2.0287943053007167, 10.113387120040088]\n",
      "T Values: [5.372259569183012, 3.1375220305724882, -0.08031546817263827, -0.8534178111484807, 1.895841098329306, -0.26299313786571393]\n",
      "P Values: [0.012627475172098057, 0.051762772959023096, 0.9410441507453726, 0.4561508886787906, 0.1542628008346565, 0.8095835081527305]\n",
      "Dispersion: 11.536063940357941\n",
      "Null Deviance: 2450.0\n",
      "Residual Degree Of Freedom Null: 8\n",
      "Deviance: 34.608191821073824\n",
      "Residual Degree Of Freedom: 3\n",
      "AIC: 51.66268610853735\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-1.9976964381147368|\n",
      "|-2.6584740883753852|\n",
      "|  3.634226479302079|\n",
      "|-1.7097058634777085|\n",
      "| 1.1069829165922513|\n",
      "|  1.378915094910191|\n",
      "| 1.5005018452995884|\n",
      "|-1.4191127842068738|\n",
      "| 0.1643628380705877|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: importar los datos y las bibliotecas esenciales\n",
    "\n",
    "# busca Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Spark-GLM').getOrCreate()\n",
    "\n",
    "# datos\n",
    "data = spark.read.csv('../Datos/multi_variable_regression.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "# importar el VectorAssembler para convertir las características al formato aceptado por Spark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Paso 2: preprocesamiento de datos y conversión de cualquier cadena de datos al formato aceptado de Spark\n",
    "\n",
    "# importar el StringIndexer para convertir la característica de localidad en el formato aceptado de Spark\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# convertir la característica de localidad del tipo de cadena en formato de datos aceptado por Spark\n",
    "string_index_object = StringIndexer(inputCol='area',outputCol='area_feature')\n",
    "string_indexed_df_object = string_index_object.fit(data)\n",
    "final_data = string_indexed_df_object.transform(data)\n",
    "print(\"Data after converting the string column locality into spark accepted feature\")\n",
    "final_data.show(5)\n",
    "print(\"Columns present in our Data and a sample row value\\n\")\n",
    "print(final_data.columns)\n",
    "\n",
    "# Paso 3: preprocesamiento de datos y conversión de datos numéricos al formato aceptado de Spark\n",
    "\n",
    "# convirtiendo la (s) característica (s) en formato de datos aceptado por Spark\n",
    "# Pasar varias columnas como columnas de entrada\n",
    "assembler_object = VectorAssembler(inputCols=['house_size', 'bedrooms', 'floors','house_age', 'area_feature'], outputCol='house_features')\n",
    "feature_vector_dataframe = assembler_object.transform(final_data)\n",
    "print(feature_vector_dataframe.head(1))\n",
    "feature_vector_dataframe.printSchema()\n",
    "formatted_data = feature_vector_dataframe.select('house_features','price_sold')\n",
    "print(\"Consolidated Data with accepted features and labels\")\n",
    "formatted_data.show(3)\n",
    "\n",
    "# Paso 4: Entrenamiento de nuestro modelo de regresión lineal con múltiples variables \n",
    "# usando GLM\n",
    "\n",
    "# Dividir los datos en 90 y 10 por ciento\n",
    "train_data, test_data = formatted_data.randomSplit([0.9,0.1]) \n",
    "\n",
    "\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3,featuresCol='house_features',labelCol='price_sold')\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
