{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadaaddd-2ed5-4a39-b899-c56bb1fd8428",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbf7e4-76ea-4395-b051-70c5bae45b86",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><center>Spark - Aprendizaje de máquinas-Arboles<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04432e50-0d01-46cd-804a-ba16c0fda94d",
   "metadata": {},
   "source": [
    "<img src=\"../Imagenes/spark_logo.png\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4fb0eb-908f-4198-a5ce-184984c456e3",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9f85-539e-47c0-90ae-b705111d93f2",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5c6f0-3027-4d1e-a6b3-0a08feb29284",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209f0fe-b922-4cc8-94f5-190a44a8bdc8",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a392a-9dcf-4f18-8dd6-8335db77a0ec",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24e117-fe5f-41e3-986e-3f05322c7f21",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Arboles de decisión](#Arboles-de-decisión)\n",
    "* [Entropía](#Entropía)\n",
    "* [Arboles de clasificación en Spark](#Arboles-de-clasificación-en-Spark)\n",
    "* [Regresor de árboles de decisión en Spark](#Regresor-de-árboles-de-decisión-en-Spark)\n",
    "* [Bosques aleatorios con Spark](#Bosques-aleatorios-con-Spark)\n",
    "* [Árboles con potenciación del gradiente (Gradient Boosted Trees)](#Árboles-con-potenciación-del-gradiente-(Gradient-Booste-Trees))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c6a4d-0541-4837-bfb2-37030d668474",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3e64d-d43c-4ca8-8749-98737fdd2b4b",
   "metadata": {},
   "source": [
    "1. Basado en [Guía de instalación de Spark en Ubuntu](https://phoenixnap.com/kb/install-spark-on-ubuntu)\n",
    "1. Athul Dev, [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)\n",
    "1. [Guía de instalación de Spark en Windows 10](https://phoenixnap.com/kb/install-spark-on-windows-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746531c-5378-4ca9-a720-6982dfd266ad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6474d-44e5-4b3b-9cf6-1e304ae4a8fe",
   "metadata": {},
   "source": [
    "Un árbol de decisión es un diagrama de flujo, como una estructura de árbol, donde cada nodo interno denota una prueba o una condición en un atributo o característica, cada rama representa un resultado de la prueba, y cada nodo hoja (nodo terminal) tiene una etiqueta de clase. Los árboles de decisión se construyen a través de un enfoque algorítmico que identifica formas de dividir un conjunto de datos en función de diferentes condiciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac7ff2-b810-4508-9fa3-98bc0c65bc63",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Arboles de decisión</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d24f22-9e44-4abc-b395-b6dc302e4102",
   "metadata": {},
   "source": [
    "Para comprender mejor los árboles de decisión y su terminología, veamos un ejemplo\n",
    "donde tratamos de predecir si nuestros compañeros de equipo de fútbol se presentarán o no para nuestro \n",
    "juego  el domingo basado en las experiencias del año anterior de diversas condiciones climáticas y si\n",
    "jugó el juego o no. Es decir, hemos registrado las condiciones meteorológicas y, además, si el\n",
    "los compañeros de equipo se presentaron o no los domingos del año anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed0a6e-7be0-46bb-bb82-05ff88edebfc",
   "metadata": {},
   "source": [
    "![](../Imagenes/ejemplo_arbol.png) ![](../Imagenes/ejemplo_arbol_2.png)\n",
    "\n",
    "Ejemplo árbol de decisión. Fuente [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8afb7-b5d6-48de-9680-60781c5fb63d",
   "metadata": {},
   "source": [
    "En el árbol de decisión que hemos generado para el escenario anterior, tenemos nodos, bordes (aristas),\n",
    "raíces y hojas.\n",
    "\n",
    "* `Nodos`: los nodos son las características que participan en el proceso de división. Ejemplo: humedad, viento, etc.\n",
    "* `Bordes`: resultado de una división al siguiente nodo. Ejemplo: división del tiempo -> división de humedad y clima -> viento\n",
    "* `Nodo raíz`: el nodo que realiza la división inicial. Ejemplo: clima\n",
    "* `Nodo hoja`: nodos terminales que predicen el resultado. Ejemplo - jugado -> sí o no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc2ed5-6490-4a56-a580-ce27a7a20dfe",
   "metadata": {},
   "source": [
    "### Proceso de división"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4eb5e-c32c-4c3c-8f35-ae66b9312e05",
   "metadata": {},
   "source": [
    "\n",
    "Uno podría preguntarse sobre qué base realizamos la división, o cómo decidimos sobre qué características para dividir y en función de qué orden. Por ejemplo, en el escenario anterior, ¿cómo pudimos determinar que nuestro nodo raíz era el clima, así es como descubrimos que dividir\n",
    "la característica meteorológica primero tuvo más sentido que dividirse por la característica de humedad.\n",
    "\n",
    "Podemos entender esto mejor si pasamos por un escenario simple en el que decidimos cuál es la mejor división. Para esto, consideremos un conjunto de datos simple con tres características A, B y C que\n",
    "consta de dos posibles clases de etiquetas X o Y, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910bce3-79c7-4327-9cb5-54c689cdd038",
   "metadata": {},
   "source": [
    "![](../Imagenes/ejemplo_arbol_3.png) ![](../Imagenes/ejemplo_arbol_4.png)\n",
    "\n",
    "Conjunto simple  y su ideal representación en árbol. Fuente [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36558f7a-ff27-42e8-aa3e-420a955fdc86",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Entropía</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c7ed1-d39f-4e4b-86b0-b4433368c071",
   "metadata": {},
   "source": [
    "Para un introducción a los conceptos básicos revise la lección [Teoría de la información](https://nbviewer.jupyter.org/github/AprendizajeProfundo/Diplomado/blob/master/Temas/M%C3%B3dulo%201-%20Matem%C3%A1ticas%20y%20Estad%C3%ADstica/2.%20Estad%C3%ADsica%2C%20Teor%C3%ADa%20de%20la%20Decisi%C3%B3n%20y%20Teor%C3%ADa%20de%20la%20informaci%C3%B3n/Cuadernos/ti_Teoria_Informacion.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e5f01-821b-4fdb-ba40-8db0bd0af960",
   "metadata": {},
   "source": [
    "La entropía controla cómo un árbol de decisión decide dividir los datos. En realidad, afecta cómo un\n",
    "árbol de decisión traza sus límites. La entropía no es más que la medida del desorden de uan distribuición (discreta en esta lección), también podemos tomarla como una medida de pureza. La fórmula matemática de la entropía es:\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=1}^n -p_i\\log_2 p_i\n",
    "$$\n",
    "\n",
    "en donde $p_i$ es la probabilidad de la etiqueta $i$.\n",
    "\n",
    "Por ejemplo si tenemos 100 datos en el conjunto de datos, en donde 70 pertenecen a la clase 0 y 30 a la clase 1, la entropía en este caso es \n",
    "\n",
    "$$\n",
    "E = -\\tfrac{3}{10}\\log_2\\tfrac{3}{10}- \\tfrac{7}{10}\\log_2\\tfrac{7}{10} = 0.88,\n",
    "$$\n",
    "\n",
    "que se considera alta. La entropia usando el logaritmo en base 2 , está entre 0 y 1. Observe que la máxima entropia en este caso binario se alcanza cuado $p_i = 0.5$. La entroía mide el grado de desorden de la distribución. Note que en caso $p_i = 0.5$ se tiene la distribución uniforme. Por lo general se define $0\\log 0 = 0$, con lo cual la entropía mínima de la distribución de Bernoulli se tiene cuando $p_1=0$, 0, $p_1=1$. En general, en para las distribuciones discretas, la distribución  uniforme es la que tiene entropía maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09a8c1-8b97-49d9-a450-8c3001e1e1aa",
   "metadata": {},
   "source": [
    "### Entropía Conjunta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb5183-c046-4e2f-8e5f-87467e253a26",
   "metadata": {},
   "source": [
    "Sean $X$ y $Y$   variables aleatorias discretas que tienen una función de probabilidad conjunta\n",
    "\n",
    "\n",
    "$$\n",
    "p_{ij}=p(x_i,y_j) = p\\{X=x_i,Y=y_j \\}; i=1\\ldots,M; j=1,\\ldots,L.\n",
    "$$\n",
    "\n",
    "Es natural definir la entropía conjunta de $ X $ y $ Y $ como\n",
    "\n",
    "$$\n",
    "E(X,Y) = - \\sum_{i=1}^M \\sum_{j=1}^L p(x_i,y_j) \\log p(x_i,y_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7199cd14-41fa-4d85-8dbb-baed31f818d9",
   "metadata": {},
   "source": [
    "Se puede verificar que\n",
    "\n",
    "$$\n",
    "E(X,Y) \\le E(X) + E(Y),\n",
    "$$\n",
    "\n",
    "*la igualdad se tiene si y solo si  $X$ y $Y$ son idependientes*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8c4c0-3c32-493d-ba5e-8d7ab6a00bed",
   "metadata": {},
   "source": [
    "### Entropía Condicional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff8c83-fe2f-4a63-8be8-3d02448b1f54",
   "metadata": {},
   "source": [
    "Supongamos que $ X $ y $ Y $ sean variables aleatorias discretas con distribución conjunta $ p (x_i, y_j) $. Si se sabe que $ X = x_i $, la distribución de $ Y $ se caracteriza por el conjunto de probabilidades condicionales $ p (y_j | x_i) $. Por lo tanto definimos la entropía condicional de $ Y $ dado $ X = x_i $ como\n",
    "\n",
    "$$\n",
    "E(Y|X=x_i) = -\\sum_{j=1}^L p(y_j|x_i) \\log p(y_j|x_i).\n",
    "$$\n",
    "\n",
    "La entropía condicional de $ Y $ dado $ X $ es el promedio ponderado promedio de $ H (Y | X = x_i) $, es decir\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E(Y|X) = - \\sum_{i=1}^M \\sum_{j=1}^L p(x_i,y_j) \\log p(y_j|x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92a85d-9dec-4341-9bec-646249731785",
   "metadata": {},
   "source": [
    "Se puede verificar que\n",
    "\n",
    "$$\n",
    "E(X,Y) = E(X) + E(Y|X) = E(Y) + E(X|Y).\n",
    "$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\n",
    "E(Y|X) \\le E(Y,X),\n",
    "$$\n",
    "\n",
    "*La igualdad se tiene si y solo si $X$ y $Y$ son independientes*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb580ca3-b322-4dd5-b5f4-db9b2c5feb5f",
   "metadata": {},
   "source": [
    "### Ganancia de información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9bef8-4832-41a3-bd07-4f3d41ab766a",
   "metadata": {},
   "source": [
    "La ganancia de información calcula la reducción en la entropía al transformar un conjunto de datos de alguna manera.\n",
    "\n",
    "Se usa comúnmente en la construcción de árboles de decisión a partir de un conjunto de datos de entrenamiento, evaluando la ganancia de información para cada variable y seleccionando la variable que maximiza la ganancia de información, lo que a su vez minimiza la entropía y divide mejor el conjunto de datos en grupos para una clasificación efectiva. .\n",
    "\n",
    "La ganancia de información se define mediante\n",
    "\n",
    "$$\n",
    "\\text{Ganancia información} =  E(Y) - E(Y|X)\n",
    "$$\n",
    "\n",
    "\n",
    "Aqui podemos interpretar la ganancia de información al selecciona un nodo padre para subdividirlo como\n",
    "\n",
    "**Ganancia de información = entropía(nodo padre) - (promedio ponderado) * entropía(nodos hijos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deab14-46fe-4b88-ad99-5130d26fb6b6",
   "metadata": {},
   "source": [
    "El algoritmo de árbol de decisión elige la mayor ganancia de información para dividir / construir una decisión\n",
    "Árbol. Por lo tanto, debemos verificar todas las características para dividir el árbol.\n",
    "\n",
    "\n",
    "El algortimo divide la variable con la cual se obtiene una mayor ganancia de información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e2571-1c85-4b83-b5b5-67fba4c41df6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Arboles de clasificación en Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851777f-1d09-4752-9ead-167eb24c1fc2",
   "metadata": {},
   "source": [
    "Pasaremos a la implementación de regresión de arboles de decisión. Para realizar la clasificación del árbol de decisión, consideremos el conjunto de datos de cáncer de mama que contiene características como el grosor de la masa de la célula y la forma de la célula, y la etiqueta de destino o etiqueta de clase es benigna o maligna donde benigna y maligno corresponde al valor 2 y 4 respectivamente en el conjunto de datos. También veremos qué característica es la principal responsable de determinar si la etiqueta de clase pertenece a benigno (2) o maligno (4) utilizando el método featureImportance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6594f4-7a4a-4493-a852-b8705dd1029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+---------+---------+----------+-----+\n",
      "|thickness|cell_size|cell_shape|label|\n",
      "+---------+---------+----------+-----+\n",
      "|        5|        1|         1|    2|\n",
      "|        5|        4|         4|    2|\n",
      "|        3|        1|         1|    2|\n",
      "|        6|        8|         8|    2|\n",
      "|        4|        1|         1|    2|\n",
      "|        8|       10|        10|    4|\n",
      "|        1|        1|         1|    2|\n",
      "|        2|        1|         2|    2|\n",
      "|        2|        1|         1|    2|\n",
      "|        4|        2|         1|    2|\n",
      "+---------+---------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Consolidated Data with features and labels\n",
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "|  [5.0,1.0,1.0]|    2|\n",
      "|  [5.0,4.0,4.0]|    2|\n",
      "|  [3.0,1.0,1.0]|    2|\n",
      "|  [6.0,8.0,8.0]|    2|\n",
      "|  [4.0,1.0,1.0]|    2|\n",
      "|[8.0,10.0,10.0]|    4|\n",
      "|  [1.0,1.0,1.0]|    2|\n",
      "|  [2.0,1.0,2.0]|    2|\n",
      "|  [2.0,1.0,1.0]|    2|\n",
      "|  [4.0,2.0,1.0]|    2|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Area Under the Curve value is 1.0\n",
      "\n",
      "Accuracy of Decision Tree is 0.9230769230769231\n",
      "\n",
      "Prediction Data\n",
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "|     features|label|       rawPrediction|         probability|prediction|\n",
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "|[1.0,1.0,1.0]|    2|[0.0,0.0,313.0,0....|[0.0,0.0,0.996815...|       2.0|\n",
      "|[1.0,1.0,1.0]|    2|[0.0,0.0,313.0,0....|[0.0,0.0,0.996815...|       2.0|\n",
      "|[1.0,1.0,1.0]|    2|[0.0,0.0,313.0,0....|[0.0,0.0,0.996815...|       2.0|\n",
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Detemining which feature played a major role in Decision Making\n",
      "\n",
      "(3,[0,1,2],[0.09132816439677091,0.8379396397448011,0.07073219585842797])\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Importación de datos y bibliotecas esenciales\n",
    "\n",
    "# busca Spark\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkTrees').getOrCreate()\n",
    "\n",
    "# datos\n",
    "data = spark.read.csv('../Datos/breast-cancer.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(10)\n",
    "\n",
    "# Paso 2: preprocesamiento de datos y conexión de datos para generar \n",
    "# un formato aceptado por Spark\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['thickness','cell_size','cell_shape'],outputCol='features')\n",
    "assembler_data = assembler.transform(data)\n",
    "final_data = assembler_data.select('features','label')\n",
    "print(\"Consolidated Data with features and labels\")\n",
    "final_data.show(10)\n",
    "\n",
    "\n",
    "# Paso 3 - Entrenamiento del modelo de decisión\n",
    "\n",
    "# Dividir los datos en 80 y 20 por ciento\n",
    "train_data,test_data = final_data.randomSplit([0.8,0.2])\n",
    "\n",
    "# Clasificador\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol='label',featuresCol='features')\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Paso 4- Evaluación del modelo entrenado\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "eval_obj = BinaryClassificationEvaluator(labelCol='label')\n",
    "print(\"Area Under the Curve value is {}\".format(eval_obj.evaluate(dt_predictions)))\n",
    "\n",
    "mul_eval_obj = MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy')\n",
    "print(\"\\nAccuracy of Decision Tree is {}\".format(mul_eval_obj.evaluate(dt_predictions)))\n",
    "print(\"\\nPrediction Data\")\n",
    "\n",
    "dt_predictions.show(3)\n",
    "print(\"Detemining which feature played a major role in Decision Making\\n\")\n",
    "print(dt_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07179c7-89dc-4be9-a489-200e747a91ed",
   "metadata": {},
   "source": [
    "Aquí podemos ver que el método \"featureImportance\" nos da el resultado como:\n",
    "(3, [0,1,2], [0.06362893724973079,0.857071363800069,0.07929969895020007]).\n",
    "\n",
    "Esto puede ser interpretado como:\n",
    "\n",
    "* el primer elemento, es decir, 3 es el número de características presentes, \n",
    "* el segundo elemento [0,1,2] los índices de estas características y \n",
    "* el elemento final es la importancia de esa característica. \n",
    "\n",
    "Puede leerse y entenderse como:\n",
    "\n",
    "Importancia = {'tickness': 0.06362893724973079, 'cell size': 0.857071363800069, '' cell shape': 0.07929969895020007}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73030e-1edd-4355-a009-b48644fdcc71",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Regresor de árboles de decisión en Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e167d8d-9a1a-4692-b388-3e5524f0e698",
   "metadata": {},
   "source": [
    "Para realizar la regresión del árbol de decisión, consideremos el conjunto de datos de precios de automóviles que contiene las características de las dimensiones del automóvil, como la distancia entre ejes, la altura, la longitud,\n",
    "ancho, y la etiqueta de destino o etiqueta de clase es el precio del automóvil.\n",
    "\n",
    "Aquí intentamos determinar qué característica de dimensión es la principal responsable del precio del coche y también pasar por los Evaluadores de Regresión para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ac6e6a7-67fd-414f-aedc-f30a200f3185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+----------+------+-----+------+-----+\n",
      "|wheel-base|length|width|height|price|\n",
      "+----------+------+-----+------+-----+\n",
      "|      88.6| 168.8| 64.1|  48.8|13495|\n",
      "|      88.6| 168.8| 64.1|  48.8|16500|\n",
      "|      94.5| 171.2| 65.5|  52.4|16500|\n",
      "+----------+------+-----+------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Consolidated Data with features and labels\n",
      "+--------------------+-----+\n",
      "|            features|price|\n",
      "+--------------------+-----+\n",
      "|[88.6,168.8,64.1,...|13495|\n",
      "|[88.6,168.8,64.1,...|16500|\n",
      "|[94.5,171.2,65.5,...|16500|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[88.4,141.1,60.3,...| 5151|            6349.5|\n",
      "|[88.6,168.8,64.1,...|13495| 8126.785714285715|\n",
      "|[88.6,168.8,64.1,...|16500| 8126.785714285715|\n",
      "|[91.3,170.7,67.9,...|17199|           20858.5|\n",
      "|[93.7,150.0,64.0,...| 6529|            6264.0|\n",
      "|[93.7,156.9,63.4,...| 5118|            6349.5|\n",
      "|[93.7,157.3,63.8,...| 6229|            7144.0|\n",
      "|[93.7,157.3,63.8,...| 5572|            7144.0|\n",
      "|[93.7,157.3,64.4,...| 6189|            6028.6|\n",
      "|[93.7,167.3,63.8,...| 7609|7137.2307692307695|\n",
      "|[94.5,165.6,63.8,...| 7799|7137.2307692307695|\n",
      "|[95.3,169.0,65.7,...|11845|13827.777777777777|\n",
      "|[95.7,166.3,64.4,...| 7898| 8126.785714285715|\n",
      "|[95.7,169.7,63.6,...| 8778|7137.2307692307695|\n",
      "|[96.0,172.6,65.2,...|11048|13827.777777777777|\n",
      "|[96.3,172.4,65.4,...| 6989| 9548.333333333334|\n",
      "|[96.3,173.0,65.4,...| 8499| 9548.333333333334|\n",
      "|[96.5,175.4,65.2,...| 8845| 9548.333333333334|\n",
      "|[96.5,175.4,65.2,...|12945| 9548.333333333334|\n",
      "|[97.2,172.0,65.4,...| 7775| 9548.333333333334|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The R Square value is 0.7566622201610896\n",
      "\n",
      "Determining which feature played a major role in Decision Making\n",
      "(4,[0,1,2,3],[0.24925439537472688,0.03915528140836024,0.6948306821687207,0.016759641048192303])\n"
     ]
    }
   ],
   "source": [
    "#Step 1 - Importing the data and essential libraries \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkDTRegression').getOrCreate()\n",
    "\n",
    "# datos\n",
    "data = spark.read.csv('../Datos/car-dimension-price.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "#Step 2 - Data pre-processing and converting data to spark accepted format\n",
    "data.columns\n",
    "data = data.na.drop()\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['wheel-base','length','width','height'],outputCol='features')\n",
    "assembler_data = assembler.transform(data)\n",
    "final_data = assembler_data.select('features','price')\n",
    "print(\"Consolidated Data with features and labels\")\n",
    "final_data.show(3)\n",
    "\n",
    "#Step 3 - Training our Decision model \n",
    "\n",
    "# Splitting the data into 80 and 20 percent\n",
    "train_data,test_data=final_data.randomSplit([0.8,0.2])\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol='price',featuresCol='features')\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "dt_predictions.show()\n",
    "\n",
    "#Step 4 - Evaluating our Trained Model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regression_evaluator_r2 = RegressionEvaluator(predictionCol='prediction',labelCol='price',metricName=\"r2\")\n",
    "R2 = regression_evaluator_r2.evaluate(dt_predictions)\n",
    "print(\"The R Square value is {}\".format(R2))\n",
    "print(\"\\nDetermining which feature played a major role in Decision Making\")\n",
    "print(dt_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145a3ff-ae2b-4f0c-9ba7-3b6ea9aff111",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bosques aleatorios con Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59814d7-ce7a-4257-bb98-f1f985462a57",
   "metadata": {},
   "source": [
    "El bosque aleatorio, como su nombre indica, consta de una gran cantidad de árboles de decisión individuales que operan como un conjunto. Básicamente se usan muchos árboles, cada uno elaborado con una muestra aleatoria de variables elegidas para la división. Se elige una nueva muestra aleatoria de variables para cada árbol en cada división.\n",
    "\n",
    "Se puede utilizar para problemas de clasificación y regresión. En un problema de clasificación  cada árbol individual en el bosque aleatorio entrega una predicción de clase y la clase con más votos se convierte en la predicción del modelo.\n",
    "\n",
    "Similarmente,  si es un problema de regresión, se toma el promedio de los valores pronosticados por cada árbol y lo establece como su etiqueta continua pronosticada.\n",
    "\n",
    "\n",
    "Podría preguntarse por qué es necesario utilizar una nueva muestra aleatoria de variables  para cada árbol único en cada división. Y la respuesta es evitar árboles altamente correlacionados. Suponiendo que hay una variable muy fuerte en el conjunto de datos la mayoría de los árboles usarán como su primera división o división superior, y esto da como resultado un grupo o conjunto de árboles que están altamente correlacionados. Y al promediar estos árboles altamente correlacionados, la varianza no es\n",
    "reducido de manera significativa. \n",
    "\n",
    "Por lo tanto, al elegir la muestra de variables al azar para cada árbol en cada división, Random Forests decorrelaciona los árboles, lo que resulta en una varianza reducida en nuestro modelo durante el proceso de promediado.\n",
    "\n",
    "Vamos a implementar un regresor de bosque aleatorio que predeciría el precio de un\n",
    "coche en función de su rendimiento y kilometraje. El conjunto de datos consta de características como\n",
    "caballos de fuerza, rpm pico, kilometraje en ciudad, kilometraje en carretera y la etiqueta objetivo es el precio de\n",
    "el coche en sí. También veremos qué característica de rendimiento es la principal responsable del precio.\n",
    "del carro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "665b5f0a-9a0f-4e12-8c12-4c68f69f75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+----------+--------+------------+---------------+-----+\n",
      "|horsepower|peak-rpm|city-mileage|highway-mileage|price|\n",
      "+----------+--------+------------+---------------+-----+\n",
      "|       111|    5000|          21|             27|13495|\n",
      "|       111|    5000|          21|             27|16500|\n",
      "|       154|    5000|          19|             26|16500|\n",
      "+----------+--------+------------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Consolidated Data with features and labels\n",
      "+--------------------+-----+\n",
      "|            features|price|\n",
      "+--------------------+-----+\n",
      "|[111.0,5000.0,21....|13495|\n",
      "|[111.0,5000.0,21....|16500|\n",
      "|[154.0,5000.0,19....|16500|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[62.0,4800.0,27.0...| 8778|8056.7166863277125|\n",
      "|[62.0,4800.0,31.0...| 6918| 6960.990267749157|\n",
      "|[68.0,5000.0,31.0...| 7395| 6852.413285618031|\n",
      "|[68.0,5500.0,31.0...| 6189| 6862.769468844527|\n",
      "|[68.0,5500.0,31.0...| 6692| 6862.769468844527|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The R Square value is 0.9150411679580567\n",
      "\n",
      "Detemining which feature played a major role in Decision Making\n",
      "(4,[0,1,2,3],[0.3086468203910117,0.08081758849464588,0.30585164374288376,0.30468394737145865])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Step 1 - Importing the data and essential libraries \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkRFRegression').getOrCreate()\n",
    "data = spark.read.csv('../Datos/car-performance-price.csv',header=True,inferSchema=True)\n",
    "print(\"Initial Data\")\n",
    "data.show(3)\n",
    "\n",
    "#Step 2 - Data pre-processing and converting data to spark accepted format\n",
    "\n",
    "data = data.na.drop()\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['horsepower','peak-rpm','city-mileage','highway-mileage'],outputCol='features')\n",
    "assembler_data = assembler.transform(data)\n",
    "final_data = assembler_data.select('features','price')\n",
    "print(\"Consolidated Data with features and labels\")\n",
    "final_data.show(3)\n",
    "\n",
    "#Step 3 - Training our Decision model \n",
    "\n",
    "# Splliting the data into 80 and 20 percent\n",
    "train_data,test_data=final_data.randomSplit([0.8,0.2])\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol='price',featuresCol='features',numTrees=120)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Print predictions\n",
    "rf_predictions.show(5)\n",
    "\n",
    "#Step 4 - Evaluating our Trained Model\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regression_evaluator_r2 = RegressionEvaluator(predictionCol='prediction',labelCol='price',metricName=\"r2\")\n",
    "R2 = regression_evaluator_r2.evaluate(rf_predictions)\n",
    "print(\"The R Square value is {}\".format(R2))\n",
    "print(\"\\nDetemining which feature played a major role in Decision Making\")\n",
    "print(rf_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fa99d-69e5-4465-95b1-cd508bea0e43",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Árboles con potenciación del gradiente (Gradient Boosted Trees)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace3804-10cb-4edb-a836-db7cf9600ae4",
   "metadata": {},
   "source": [
    "\n",
    "El término potenciacion (`boosting`) es un método para convertir máquinas de aprendizaje débiles en máquinas fuertes. En este caso se\n",
    "tiene la intención de realizar una potenciación en los árboles.\n",
    "\n",
    "\n",
    "Los árboles potenciados con gradiente (Gradient Boosted Trees) involucran tres entidades principales:\n",
    "\n",
    "\n",
    "1. Una función de pérdida que debe optimizarse. La función de costo cuantifica el error entre los valores predichos y los valores esperados y lo presenta en forma de un solo número real. Básicamente se utiliza para determinar que tanto las predicciones están lejos de la etiqueta real o del valor objetivo.\n",
    "\n",
    "1. Una máquina  débil para hacer predicciones. La máquina de aprendizaje débil puede ser un clasificador, predictor o algo así que se desempeña por encima del promedio, su precisión está por encima del azar. No importa cuál sea la distribución sobre los datos de entrenamiento, es decir, siempre será mejor que el azar cuando intente etiquetar los datos. Haciéndolo mejor que el azar significa que siempre tendremos una tasa de error menor a 0.5. Los árboles de decisión se utilizan como aprendices débiles en la mejora de gradientes. También podríamos restringir varios componentes de las máquinas débiles como sus nodos, máximo número de capas, divisiones o nodos de hojas.\n",
    "1. Un modelo aditivo para agregar máquinas débiles para minimizar la función de pérdida. En el modelo aditivo para árboles con Gradient Boosted, los árboles se agregan uno a la vez, y los árboles existentes en el modelo se conservan o no se modifican. Más tarde un procedimiento de descenso del gradiente se utiliza para minimizar la función de costo al agregar los árboles.\n",
    "\n",
    "Un procedimiento simple general para árboles potenciados con gradiente es como sigue:\n",
    "\n",
    "1. En primer lugar, entrenamos un modelo débil utilizando muestras de datos extraídas de acuerdo con algún tipo de distribución de peso en la que cada muestra tiene un peso asociado.\n",
    "2. A continuación, aumentamos los pesos de las muestras que el modelo clasifica erróneamente, básicamente castigándolo por errores, y disminuir el peso de las muestras que son clasificado correctamente por el modelo.\n",
    "3. Por último, entrene el siguiente modelo débil utilizando las muestras extraídas de acuerdo con la actualización distribución del peso.\n",
    "\n",
    "\n",
    "Según este procedimiento, el algoritmo siempre entrena el modelo utilizando las muestras de los datos que fueron difíciles de aprender en el ciclo anterior, esto daría como resultado un conjunto de modelos que son buenos para aprender diferentes partes de los datos de entrenamiento. Básicamente\n",
    "*aumentar* los pesos de aquellas muestras que predijeron valores falsos o resultados incorrectos.\n",
    "\n",
    "Ahora vayamos a implementar una técnica de árbol reforzado con degradado y también realicemos una comparación de los árboles reforzados con degradado, el bosque aleatorio y los árboles de decisión basados sobre su precisión.\n",
    "\n",
    "Usaremos el conjunto de datos de cáncer de mama que contiene características como id_number,  \n",
    "grosor del tumos, tamaño de celda, forma de celda, etc., y la etiqueta de destino que pertenece a la clase: benigna (2)\n",
    "o maligno (4). Y para esta implementación veremos cómo importar y usar el \"libsvm\" conjunto de datos de formato, el conjunto de datos de cáncer de mama que estaríamos usando es del formato libsvm y este El conjunto de datos ya estaría preprocesado y es un tipo de conjunto de datos listo para usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1b39e-caaa-4de1-b6fe-71c16482b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Step 1 - Importing the data and all the necessary libraries\n",
    "\n",
    "# busca Spark\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkTreeComparisions').getOrCreate()\n",
    "data = spark.read.format('libsvm').load('../Datos/libsvm-breast-cancer')\n",
    "print(\"Libsvm format Data - Fully formatted and ready to use data\")\n",
    "data.show(3)\n",
    "\n",
    "#Step 2 - Training our Tree models\n",
    "\n",
    "# Splitting the data into 70 and 30 percent\n",
    "train_data,test_data = data.randomSplit([0.9,0.1])\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier,DecisionTreeClassifier,RandomForestClassifier\n",
    "gbt = GBTClassifier() #Gradient Boosted Trees\n",
    "rf = RandomForestClassifier(numTrees=150) #Random Forest with 150 Trees\n",
    "dt = DecisionTreeClassifier() #Decision Trees\n",
    "\n",
    "gbt_model = gbt.fit(train_data)\n",
    "rf_model = rf.fit(train_data)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "print(\"Gradient Boosted Tree Predictions\")\n",
    "gbt_predictions.show(3)\n",
    "\n",
    "#Step 3 - Evaluating our Trained Model\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "mul_eval_obj = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "print(\"Accuracy of Decision Tree is {}\".format(mul_eval_obj.evaluate(dt_predictions)))\n",
    "print(\"Feature Importances of Decision Tree {}\\n\".format(dt_model.featureImportances))\n",
    "\n",
    "print(\"Accuracy of Random Forest is {}\".format(mul_eval_obj.evaluate(rf_predictions)))\n",
    "print(\"Feature Importances of Decision Tree {}\\n\".format(rf_model.featureImportances))\n",
    "\n",
    "print(\"Accuracy of GBT is {}\".format(mul_eval_obj.evaluate(rf_predictions)))\n",
    "print(\"Feature Importances of GBT {}\\n\".format(rf_model.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6421ca3-f142-41c8-8397-6f6e11a5e452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40b7b0-1b4b-4c88-8817-6ab12a86f5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
