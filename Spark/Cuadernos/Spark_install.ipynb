{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadaaddd-2ed5-4a39-b899-c56bb1fd8428",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbf7e4-76ea-4395-b051-70c5bae45b86",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Spark - Instalación <center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04432e50-0d01-46cd-804a-ba16c0fda94d",
   "metadata": {},
   "source": [
    "<img src=\"../Imagenes/spark_logo.png\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4fb0eb-908f-4198-a5ce-184984c456e3",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9f85-539e-47c0-90ae-b705111d93f2",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5c6f0-3027-4d1e-a6b3-0a08feb29284",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209f0fe-b922-4cc8-94f5-190a44a8bdc8",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a392a-9dcf-4f18-8dd6-8335db77a0ec",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24e117-fe5f-41e3-986e-3f05322c7f21",
   "metadata": {},
   "source": [
    "\n",
    "* [Introducción](#Introducción)\n",
    "* [Tutorial sobre cómo instalar Spark en una máquina Ubuntu](#Tutorial-sobre-cómo-instalar-Spark-en-una-máquina-Ubuntu)\n",
    "* [nstalación  para correr Spark desde Jupyterlab](#nstalación-para-correr-Spark-desde-Jupyterlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c6a4d-0541-4837-bfb2-37030d668474",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3e64d-d43c-4ca8-8749-98737fdd2b4b",
   "metadata": {},
   "source": [
    "1. Basado en [Guía de instalación de Spark en Ubuntu](https://phoenixnap.com/kb/install-spark-on-ubuntu)\n",
    "1. Athul Dev, [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)\n",
    "1. [Guía de instalción de Spark en Windows 10](https://phoenixnap.com/kb/install-spark-on-windows-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746531c-5378-4ca9-a720-6982dfd266ad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce32f99-fa85-46ee-a755-c520c5fbd9a1",
   "metadata": {},
   "source": [
    "Apache Spark es un marco utilizado en entornos de computación en clúster para analizar big data. Esta plataforma se hizo muy popular debido a su facilidad de uso y las velocidades de procesamiento de datos mejoradas en Hadoop.\n",
    "\n",
    "Apache Spark puede distribuir una carga de trabajo en un grupo de computadoras en un clúster para procesar de manera más efectiva grandes conjuntos de datos. Este motor de código abierto admite una amplia gama de lenguajes de programación. Esto incluye Java, Scala, Python y R.\n",
    "\n",
    "En la primera perte de este tutorial, aprenderá cómo instalar Spark en una máquina Ubuntu. La guía le mostrará cómo iniciar un servidor maestro y esclavo y cómo cargar shells Scala y Python. También proporciona los comandos Spark más importantes. Si requiere instalar Spark sobre Windows 10 vaya por ejemplo a  [Guía de instalción de Spark en Windows 10](https://phoenixnap.com/kb/install-spark-on-windows-10)\n",
    "\n",
    "En la segunda parte creamos un ambiente de Conda y hasmoes la instalación requerida para correr Spark usando Pyspark sobre Jupyterlab.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55a294-9ad6-4a11-ad86-3d28a09b0cff",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Tutorial sobre cómo instalar Spark en una máquina Ubuntu</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9ca72-9443-4977-a0a3-b351fda8e0b1",
   "metadata": {},
   "source": [
    "- Prerrequisitos\n",
    "    - Un sistema Ubuntu.\n",
    "    - Acceso a una terminal o línea de comandos.\n",
    "    - Un usuario con permisos de root o sudo.\n",
    "\n",
    "\n",
    "Instalar paquetes necesarios para Spark\n",
    "Antes de descargar y configurar Spark, debe instalar las dependencias necesarias. Este paso incluye la instalación de los siguientes paquetes:\n",
    "\n",
    "- JDK\n",
    "- Scala\n",
    "- Git\n",
    "Abra una ventana de terminal y ejecute el siguiente comando para instalar los tres paquetes a la vez:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dede-fb7b-4bd9-9330-abedd53a3d99",
   "metadata": {},
   "source": [
    "### Instala prerrequisitos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "baf16c7e-7ac7-45ee-9b78-083495ba8061",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ sudo apt install default-jdk scala git -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d42e72-ea6b-4909-8de5-5bcb4a57236e",
   "metadata": {},
   "source": [
    "Verifique la instalación"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d459d998-27ae-45da-9096-a03267793823",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ java -version; javac -version; scala version; git -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59375d-148d-4030-ace9-15e73ed62687",
   "metadata": {},
   "source": [
    "### Descarga y descompresión de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e347b20-fd89-4c1d-bf5e-cf2637a67ee5",
   "metadata": {},
   "source": [
    "Hora desacrgue la última versión de desde [Spark](https://spark.apache.org/downloads.html). O alternativamente desde la consola con al orden (actualizada  junio 18 de 2021)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5327c6bc-fd1f-4d79-a4df-dd92b3edc8b7",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f7fd8-7309-495e-8f27-34bc1f08500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahora ejecute este comando de descompresión"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3a5e949-74ea-411a-abc1-1256ce7d47de",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$  tar xvf spark-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d28bc3-719d-470b-8ab3-251a071556cf",
   "metadata": {},
   "source": [
    "mueva la carpeta creada a su  destino final así:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ca24bbd-6ab2-41e8-8640-9caf3974181a",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ sudo mv spark-3.1.2-bin-hadoop3.2 /opt/spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adfcd6-1a73-4093-82e2-3a6d81a7b1b5",
   "metadata": {},
   "source": [
    "### Configurar el entorno de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ab9cf-d678-4abc-85cc-fff94d38035e",
   "metadata": {},
   "source": [
    "Antes de iniciar un servidor maestro, debe configurar las variables de entorno. Hay algunas rutas de inicio de Spark que debe agregar al perfil de usuario.\n",
    "\n",
    "Utilice el comando echo para agregar estas tres líneas a .profile:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "177b5887-db72-4a14-968f-18a827b71868",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile\n",
    "usuario@ubuntu:~$ echo \"export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\" >> ~/.profile\n",
    "usuario@ubuntu:~$ echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2a87b-e718-4adc-8f84-2acbd8eab06e",
   "metadata": {},
   "source": [
    "Salga y guarde los cambios cuando se le solicite.\n",
    "\n",
    "Cuando termine de agregar las rutas, cargue el archivo .profile en la línea de comando escribiendo:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45fd2b55-4e61-4a52-9fe2-85395f75faf5",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ source ~/.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3694e-ddb3-43ab-bb27-ed7e6e3f4708",
   "metadata": {},
   "source": [
    "### Iniciar el servidor maestro autónomo de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fd5d1-12e4-43fe-9f39-a08cef3723aa",
   "metadata": {},
   "source": [
    "Ahora que ha completado la configuración de su entorno para Spark, puede iniciar un servidor maestro.\n",
    "\n",
    "En la terminal, escriba:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9fe0ad4-6a66-40d9-bec6-3c618b1f1811",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ start-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd716acc-77bd-4f23-aac8-90f4221e5fa5",
   "metadata": {},
   "source": [
    "Para ver la interfaz de usuario de Spark Web, abra un navegador web e ingrese la dirección IP del host local en el puerto 8080."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b27ae11-e756-4f8f-8cd0-9d9381f5654e",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ http://127.0.0.1:8080/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afddc32-89cb-4131-a536-9d93ec55ac3f",
   "metadata": {},
   "source": [
    "La página muestra su URL de Spark, información de estado de los trabajadores, utilización de recursos de hardware, etc.\n",
    "\n",
    "La página de inicio de la interfaz de usuario web de Spark.\n",
    "La URL de Spark Master es el nombre de su dispositivo en el puerto 8080. \n",
    "\n",
    "\n",
    "Hay tres formas posibles de cargar la interfaz de usuario web de Spark Master:\n",
    "\n",
    "- 127.0.0.1:8080\n",
    "- localhost: 8080\n",
    "- deviceName: 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37987a6c-e3d4-4f94-b7a3-61df6607ce80",
   "metadata": {},
   "source": [
    "### Iniciar Spark Slave Server (Iniciar un proceso de trabajo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b379cc-d227-4e1b-bf50-62a7b15e9d8a",
   "metadata": {},
   "source": [
    "En esta configuración independiente de un solo servidor, iniciaremos un servidor esclavo junto con el servidor maestro.\n",
    "\n",
    "Para hacerlo, ejecute el siguiente comando en este formato:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d89b0386-2283-4558-a2d5-969731416443",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ start-slave.sh spark://master:port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c384d6-7f26-4c37-9a94-06569a5ae947",
   "metadata": {},
   "source": [
    "El maestro en el comando puede ser una IP o un nombre de host.\n",
    "\n",
    "En mi caso es ubuntu1:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "261eaac7-bdd9-4026-b8f7-225af4e37f1a",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ start-slave.sh spark://ubuntu1:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc21e51-9004-4752-83a2-cda54dc872cc",
   "metadata": {},
   "source": [
    "Recargue la página y ahora debe ver el worker asociado aosociado a su que acaba de lanzar.\n",
    "\n",
    "Debería ver algo como lo siguiente:\n",
    "\n",
    "![](../Imagenes/spark-workers-alive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd563e91-1837-433b-b7f0-86436b0af56a",
   "metadata": {},
   "source": [
    "### Prueba del Shell de Spark  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e790f-96c1-4f37-a5f5-2634bbfaa6bd",
   "metadata": {},
   "source": [
    "Después de finalizar la configuración e iniciar el servidor maestro y esclavo, pruebe si el shell Spark funciona.\n",
    "\n",
    "Cargue el shell ingresando:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4e8d22c-f523-4422-b7bf-666507634b19",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b231c1-67d2-4cd0-a4bb-a2687cf46a7d",
   "metadata": {},
   "source": [
    "Debería obtener una pantalla con notificaciones e información de Spark.\n",
    "\n",
    "Scala es la interfaz predeterminada, por lo que el shell se carga cuando ejecuta Spark-Shell.\n",
    "\n",
    "El final de la salida se ve así para la versión que estamos usando al momento de escribir esta guía:\n",
    "\n",
    "![](../Imagenes/spark-shell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ef9b5-2831-40bf-a263-5ec4da8edbe2",
   "metadata": {},
   "source": [
    "Escriba  `:q` y presione `Enter` para salir de Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ead66-eee8-4a36-ab4e-bf8e1e9b9126",
   "metadata": {},
   "source": [
    "### Prueba de Python en Spark: PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d546ca1-a577-4940-9b5c-0da3dbd7a78b",
   "metadata": {},
   "source": [
    "Si no desea utilizar la interfaz predeterminada de Scala, puede cambiar a Python.\n",
    "\n",
    "Asegúrese de salir de Scala y luego ejecute este comando:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4b04878-a18b-47de-9727-ac3c211e7f87",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dfa04-9c98-4e14-ad76-5866a22afa86",
   "metadata": {},
   "source": [
    "Si todo ha idos bien debería aparecer la siguiente salida es su consola (su versión de Pytohn y Spark pueden ser diferentes):\n",
    "\n",
    "![](../Imagenes/pyspark-command-shell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f9282-298e-455b-b326-6ca4f6b8d71b",
   "metadata": {},
   "source": [
    "Para salir de este shell, escriba `quit()` y presione `Enter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef39b2-c69d-4485-b4b6-b64d7218715f",
   "metadata": {},
   "source": [
    "### Comandos básicos para iniciar y detener el servidor maestro y los trabajadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b765e-746b-4ba4-a658-1a4940f5bab2",
   "metadata": {},
   "source": [
    "A continuación, se muestran los comandos básicos para iniciar y detener el `servidor maestro Apache Spark` y los trabajadores. Dado que esta configuración es solo para una máquina, las secuencias de comandos que ejecuta se establecen de forma predeterminada en el host local.\n",
    "\n",
    "Para iniciar una instancia de servidor maestro en la máquina actual, ejecute el comando que usamos anteriormente en este tutoria. \n",
    "\n",
    "- `start-master.sh`\n",
    "\n",
    "Para pararlo use \n",
    "\n",
    "- `stop-master.sh`\n",
    "\n",
    "Para lanzar y detener instancias de workers aosicadoa a su masters sue respectivamente\n",
    "\n",
    "- `start-slave.sh`\n",
    "- `stop-slave.sh`\n",
    "\n",
    "Finalmente¿, para lanzar y detener instancias  tanto de master como slave escriba\n",
    "\n",
    "-  `start-all.sh`\n",
    "- `stop-all.sh`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63830d4e-a08a-4126-aeca-f30128a6ce60",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Instalación  para correr Spark desde Jupyterlab</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c13ed-b144-49d3-91f0-1057c5855ae6",
   "metadata": {},
   "source": [
    "Para correr Spark usando Python desde Jupyterlab siga los siguentes pasos:\n",
    "\n",
    "1.  Cree un nuevo ambiente e instale de una vez la version de Python que requiera. Por ejemplo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eef23c9-df22-4de5-a7a4-697e3502b6d7",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$ create -n spark python=3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3bbb6-0f91-4f87-9181-779154c01969",
   "metadata": {},
   "source": [
    "2. Vaya al ambiente creado e instale los paquetes findspark y pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b448c55b-c0f9-4ce7-8120-4c35aaf0a0ff",
   "metadata": {},
   "source": [
    "usuario@ubuntu:~$  conda activate spark\n",
    "usuario@ubuntu:~$ conda install -c conda-forge findspark \n",
    "usuario@ubuntu:~$ conda install -c conda-forge pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbdc2f-5b4e-4ac8-8329-98c6a210ba6c",
   "metadata": {},
   "source": [
    "Eso es todo. Ahora pruebe su instalación. Corra la siguientes dos celdas. Si todo salió bien, debe verse algo como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0bd7f4-d042-4eea-993a-6dc79093f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1764c069-306a-4609-ade8-4ed4adc23be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14174904\n"
     ]
    }
   ],
   "source": [
    "# crea un contexto Spark\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "# genera 100 millones muestras para estimar $\\pi$.\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "# detiene el contexto\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538047c5-9506-4de9-b32e-53be79df47a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
