{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadaaddd-2ed5-4a39-b899-c56bb1fd8428",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbf7e4-76ea-4395-b051-70c5bae45b86",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Spark - Introducción<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04432e50-0d01-46cd-804a-ba16c0fda94d",
   "metadata": {},
   "source": [
    "<img src=\"../Imagenes/spark_logo.png\" align=\"right\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4fb0eb-908f-4198-a5ce-184984c456e3",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9f85-539e-47c0-90ae-b705111d93f2",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5c6f0-3027-4d1e-a6b3-0a08feb29284",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209f0fe-b922-4cc8-94f5-190a44a8bdc8",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a392a-9dcf-4f18-8dd6-8335db77a0ec",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24e117-fe5f-41e3-986e-3f05322c7f21",
   "metadata": {},
   "source": [
    "\n",
    "* [Introducción](#Introducción)\n",
    "* [Funciones lambda, y funciones map, filter y reduce de Python](#Funciones-lambda,-y-funciones-map,-filter-y-reduce-de-Python)\n",
    "* [Introducción a los dataframes de Spark](#Introducción-a-los-dataframes-de-Spark)\n",
    "* [SQL con los dataframes de Spark](#SQL-con-los-dataframes-de-Spark)\n",
    "* [Operaciones y funciones con los dataframes de Spark Dataframes](#Operaciones-y-funciones-con-los-dataframes-de-Spark)\n",
    "* [Funciones estándar en nuestras operaciones](#Funciones-estándar-en-nuestras-operaciones)\n",
    "* [Datos faltantes en los Dataframes  de Spark](#Datos-faltantes-en-los-Dataframes-de-Spark)\n",
    "* [Trabajar con fecha y hora en Spark Dataframe](#Trabajar-con-fecha-y-hora-en-Spark-Dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c6a4d-0541-4837-bfb2-37030d668474",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3e64d-d43c-4ca8-8749-98737fdd2b4b",
   "metadata": {},
   "source": [
    "1. Basado en [Guía de instalación de Spark en Ubuntu](https://phoenixnap.com/kb/install-spark-on-ubuntu)\n",
    "1. Athul Dev, [Spark with Python](http://libgen.rs/search.php?req=spark+with+python+dev&open=0&res=25&view=simple&phrase=1&column=def)\n",
    "1. [Guía de instalción de Spark en Windows 10](https://phoenixnap.com/kb/install-spark-on-windows-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746531c-5378-4ca9-a720-6982dfd266ad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce32f99-fa85-46ee-a755-c520c5fbd9a1",
   "metadata": {},
   "source": [
    "Apache Spark es un marco utilizado en entornos de computación en clúster para analizar big data. Esta plataforma se hizo muy popular debido a su facilidad de uso y las velocidades de procesamiento de datos mejoradas en Hadoop.\n",
    "\n",
    "Apache Spark puede distribuir una carga de trabajo en un grupo de computadoras en un clúster para procesar de manera más efectiva grandes conjuntos de datos. Este motor de código abierto admite una amplia gama de lenguajes de programación. Esto incluye Java, Scala, Python y R.\n",
    "\n",
    "Primero revisaremos algunos conceptos de Pyhton que será útiles para lo sigue. Luego ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d33564-282f-43a6-b5f8-8056d9814215",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Funciones lambda, y funciones  map, filter y reduce de Python</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8e9d7-3f04-4528-9206-cdb4d8336c3f",
   "metadata": {},
   "source": [
    "### Funciones lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe851d-05f5-4177-a6ad-1d5e05c8d061",
   "metadata": {},
   "source": [
    "Una función Lambda es como una función normal pero sin un nombre, es más como una función anónima. Puede tomar cualquier número de argumentos, pero solo puede tener una sola expresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd92602-dd48-4306-9b75-be7b7ad5579c",
   "metadata": {},
   "source": [
    "#### Sintaxis de una función lambda\n",
    "\n",
    "`lambda  argumentos: expresión`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab17b4d-22e0-4f34-9bd0-b4960ca2aeea",
   "metadata": {},
   "source": [
    "Las funciones Lambda son muy eficientes y su ventaja es que se puede utilizar fácilmente dentro de\n",
    "otra función. Entonces, es básicamente como tener una función anónima dentro de una función.\n",
    "\n",
    "Podemos revisar el siguiente fragmento de código y comprender mucho el uso de las funciones lambda\n",
    "mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2835d0f7-104f-493f-86f2-7df32b415440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# función lambda simple\n",
    "multiply  = lambda a,b: a * b\n",
    "print(multiply(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dede-fb7b-4bd9-9330-abedd53a3d99",
   "metadata": {},
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d42e72-ea6b-4909-8de5-5bcb4a57236e",
   "metadata": {},
   "source": [
    "La función `map()`es usada para aplicar una función a cada uno de los elementos de una secuencia. *map()* toma dos argumentos: la función a aplicar y la secuencia a la cual se aplicara la función"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59375d-148d-4030-ace9-15e73ed62687",
   "metadata": {},
   "source": [
    "#### Sintaxis de map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e347b20-fd89-4c1d-bf5e-cf2637a67ee5",
   "metadata": {},
   "source": [
    "+ `result_map = map(función, secuencia)`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5327c6bc-fd1f-4d79-a4df-dd92b3edc8b7",
   "metadata": {},
   "source": [
    "Veámos el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b115ee-76ef-43d0-9223-fe50781579d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.0, 91.4, 97.52, 104.18, 86.18]\n",
      "[7, 28, 63]\n"
     ]
    }
   ],
   "source": [
    "#Convierte Celcius a Farenheit\n",
    "Celcius = [40, 33, 36.4, 40.1, 30.1]\n",
    "Farenheit = map(lambda x: (float(9)/5)*x +32, Celcius)\n",
    "print(list(Farenheit))\n",
    "\n",
    "# mapeo con  varias listas\n",
    "l1 = [5,10,15]\n",
    "l2 = [1, 2, 3]\n",
    "l3 = [2, 4, 6]\n",
    "print(list(map(lambda x,y,z: (x+z)*y,l1,l2,l3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dadee-a780-4efb-ba3d-7dc8870e73a1",
   "metadata": {},
   "source": [
    "### Función filter()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d4381-60a3-4209-b148-e26cc0262ddd",
   "metadata": {},
   "source": [
    "La función de filter, como sugiere el nombre, se usa para filtrar todos los elementos de\n",
    "la lista, para la cual la función devuelve True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997aab3-427d-4e30-82b2-892aedd0a89e",
   "metadata": {},
   "source": [
    "#### Sintaxis - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e2403-913c-4c92-b448-abecd97620c5",
   "metadata": {},
   "source": [
    "+ `filter(función, lista)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e59a5b-1b00-419c-88cc-5f217300467d",
   "metadata": {},
   "source": [
    "La función, *filter(F, L)* necesita una función F como primer argumento. La función F devuelve un\n",
    "Valor booleano, que es Verdadero o Falso. Esta función se aplicará a todos y cada uno\n",
    "elemento de la lista L y solo si la función F devuelve Verdadero, el elemento de la lista será\n",
    "incluido en la lista de resultados.\n",
    "\n",
    "Podemos revisar el siguiente código para comprender el uso de *filter()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07541fce-740d-44cc-b95a-4db8c91ff433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 9, 11, 23, 45, 77]\n"
     ]
    }
   ],
   "source": [
    "numbers = [0,0,1,2,3,5,6,9,11,23,34,45,50,77]\n",
    "#obtaining the odd numbers from the numbers list\n",
    "odd_numbers = filter(lambda x: x % 2, numbers)\n",
    "print(list(odd_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8adc6-2adb-466f-8eeb-ec756f859c51",
   "metadata": {},
   "source": [
    "### Función reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334377b-0bf9-4a39-92ca-3214d1f2b579",
   "metadata": {},
   "source": [
    "La función `reduce()` se usa para aplicar continuamente la lógica de una  función a los elementos de la lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b42891-978c-42ed-b556-38e4ffb06900",
   "metadata": {},
   "source": [
    "#### Sintaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf78a6-af1e-48a9-a362-b7aca19d83fd",
   "metadata": {},
   "source": [
    "+ `reduce(función, lista)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc21ec5-1bd3-4a79-a27f-0b7decec634d",
   "metadata": {},
   "source": [
    "La función *reduce()*, inicialmente toma los dos primeros elementos de la lista, aplica  la lógica de la función\n",
    "en estos dos elementos, entonces toma el resultado de estos dos elementos y \n",
    "la lógica de función se realiza de nuevo con el resultado de los dos primeros elementos y el tercer elemento de la lista y esto continúa hasta que se aborda el último elemento de la lista.\n",
    "\n",
    "\n",
    "Suponiendo que tenemos una Lista, L = [L1, L2, L3, ......, Ln], `reduce(F, L)` funciona de esta manera:\n",
    "\n",
    "* Al principio, los dos primeros elementos de la lista L se aplicarán a la función F, es decir F(L1, L2). En esta etapa, la lista en la que trabaja reduce () se puede pensar así: [F (L1, L2), L3, ..., Ln]\n",
    "* En el siguiente paso, se aplicará F sobre el resultado anterior y el tercer elemento de la lista, que es F(F (L1, L2), L3).  Ahora, la lista se puede pensar así: [F(F(L1, L2), L3), ..., Ln]\n",
    "* De manera similar, continúa este proceso hasta que se hayan abordado todos los elementos de la lista y devuelve el valor final de la función \"F\" como resultado de *reduce()*. \n",
    "\n",
    "Podemos revisar el código a continuación y comprender el funcionamiento y el uso de *reduce()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783ff95b-114c-4288-9c0f-94c1a8b08d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de los números 1 a 50 is 1225\n",
      "La multiplicación de l a lista es 50\n",
      "El elemento más grande de las lista es 210\n"
     ]
    }
   ],
   "source": [
    "#importa la librería reduce\n",
    "from functools import reduce \n",
    "\n",
    "counting=reduce(lambda x, y: x+y, range(1,50))\n",
    "print(\"Suma de los números 1 a 50 is {}\".format(counting))\n",
    "\n",
    "multiplying=reduce(lambda x,y: x*y, [1,5,10])\n",
    "print(\"La multiplicación de l a lista es {}\".format(multiplying))\n",
    "\n",
    "largest_in_list = lambda a,b: a if (a > b) else b\n",
    "largest=reduce(largest_in_list, [100,101,155,122,210])\n",
    "print(\"El elemento más grande de las lista es {}\".format(largest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adfcd6-1a73-4093-82e2-3a6d81a7b1b5",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción a los dataframes de Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ab9cf-d678-4abc-85cc-fff94d38035e",
   "metadata": {},
   "source": [
    "Los *Dataframes* de Spark puede tratar con varias fuentes de datos, lo que significa que puede ingresar y\n",
    "generar datos de una amplia variedad de fuentes como csv, json, etc. \n",
    "\n",
    "También podemos realizar varias transformaciones en los datos y recopilar los resultados para visualizar, registrar o para algún otro proceso.\n",
    "\n",
    "Para comenzar a trabajar con `Spark Dataframes`, tenemos que crear una sesión de Spark. La sesión es como un punto de entrada unificado de una aplicación Spark, proporciona una forma de interactuar con varias funcionalidades de Spark. Así que veamos cómo podemos crear una sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfaf79a-a5e4-4026-b98d-a79a9722a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#Importa la sesión Spark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Crea una sesión Spark\n",
    "spark = SparkSession.builder.appName('myFirstSparkSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2a87b-e718-4adc-8f84-2acbd8eab06e",
   "metadata": {},
   "source": [
    "A continuación, trabajemos con algunos datos reales para hacer eso, primero necesitamos leer un conjunto de datos, por esto, podemos usar el método de lectura del contexto de chispa. También podemos seleccionar el tipo de\n",
    "archivo de conjunto de datos que necesitamos cargar, y para esto, el método de lectura tiene varias opciones disponibles como csv, json y demás.\n",
    "\n",
    "Podemos cargar el archivo del conjunto de datos que está presente en nuestra computadora como"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45fd2b55-4e61-4a52-9fe2-85395f75faf5",
   "metadata": {},
   "source": [
    "dataFrame_name = spark_session_variable.read.csv(‘filename’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24370cb0-9a7e-44e1-883c-496f0b22b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- hours: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- hours: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['employee_id', 'employee_name', 'age', 'location', 'hours']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# Importa la sesión Spark \n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "# Crea una sesión Spark\n",
    "#spark = SparkSession.builder.appName('myFirstSparkSession').getOrCreate()\n",
    "\n",
    "# Lee los datos sin el parámetro inferSchema\n",
    "df = spark.read.csv(\"../Datos/employee_data.csv\",header=True) \n",
    "\n",
    "# despliega el dataframe\n",
    "df.show() \n",
    "\n",
    "# imprime el esquema del tipo de datos sin  inferSchema\n",
    "df.printSchema() \n",
    "\n",
    "# Lee los datos con el parámetro inferSchema\n",
    "df = spark.read.csv(\"../Datos/employee_data.csv\",header=True,inferSchema=True) \n",
    "\n",
    "# imprime el esquema del tipo de datos con  inferSchema\n",
    "df.printSchema()\n",
    "\n",
    "# obtiene la lista de columnas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3694e-ddb3-43ab-bb27-ed7e6e3f4708",
   "metadata": {},
   "source": [
    "### Trabajando con columnas en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d008831-0166-4ff1-861c-bc6af59ae513",
   "metadata": {},
   "source": [
    "#### Recuperando columnas y su contenido\n",
    "\n",
    "+ Usando el dataframe directamente: `df['column_name']`\n",
    "+ Usando la opción select en el dataframe: `df.select(column_name)`\n",
    "\n",
    "Algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4a7968-fb58-45cd-a056-3994569a00ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|  47|\n",
      "|  64|\n",
      "|  56|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f20034e-f82b-4908-b2d7-246adb092569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|employee_name| age|\n",
      "+-------------+----+\n",
      "|       Pichai|  47|\n",
      "|         Bill|  64|\n",
      "|         Jeff|  56|\n",
      "|         null|null|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['employee_name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6811748c-ebf9-4785-bc36-d34f35dc1a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|employee_name| age|\n",
      "+-------------+----+\n",
      "|       Pichai|  47|\n",
      "|         Bill|  64|\n",
      "|         Jeff|  56|\n",
      "|         null|null|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['employee_name', 'age'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fd5d1-12e4-43fe-9f39-a08cef3723aa",
   "metadata": {},
   "source": [
    "#### Adicionando nuevas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff27fd7-18b4-4275-aee0-77cbceff38e5",
   "metadata": {},
   "source": [
    "+ `df.withColumn('nombre_nueva_columna', valores_nueva_columna)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad5c700c-5cea-4572-b8f1-87d80f2392c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+------------+\n",
      "|employee_id|employee_name| age|  location|hours|horas_extras|\n",
      "+-----------+-------------+----+----------+-----+------------+\n",
      "|       G001|       Pichai|  47|California|   14|          14|\n",
      "|       M002|         Bill|  64|Washington| null|        null|\n",
      "|       A003|         Jeff|  56|      null| null|        null|\n",
      "|       A004|         null|null|      null|   12|          12|\n",
      "+-----------+-------------+----+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df.withColumn('horas_extras', df['hours'])\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd716acc-77bd-4f23-aac8-90f4221e5fa5",
   "metadata": {},
   "source": [
    "#### Renombrando columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c06c97c-b5a4-4995-b6fb-cfc7eb974bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|horas|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.withColumnRenamed('hours', 'horas')\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25561a8-a497-48e8-b5bb-d28fda54862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afddc32-89cb-4131-a536-9d93ec55ac3f",
   "metadata": {},
   "source": [
    "#### Eliminando una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f471e33f-0d33-4cdb-b98d-3f0f7fd1cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+------------+\n",
      "|employee_id|employee_name| age|  location|horas_extras|\n",
      "+-----------+-------------+----+----------+------------+\n",
      "|       G001|       Pichai|  47|California|          14|\n",
      "|       M002|         Bill|  64|Washington|        null|\n",
      "|       A003|         Jeff|  56|      null|        null|\n",
      "|       A004|         null|null|      null|          12|\n",
      "+-----------+-------------+----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.drop('hours')\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc9142-f461-4572-80df-42694768bec2",
   "metadata": {},
   "source": [
    "### Trabajando con filas y columnas en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1429d5-9cc0-43ce-b813-f08642fa8236",
   "metadata": {},
   "source": [
    "#### Cálculo de estadísticas globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d8e4f5-3f7c-4a1b-bec0-5d32cd930a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Single Column Data\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|  47|\n",
      "|  64|\n",
      "|  56|\n",
      "|null|\n",
      "+----+\n",
      "\n",
      "Multiple Column Data\n",
      "+-------------+----+\n",
      "|employee_name| age|\n",
      "+-------------+----+\n",
      "|       Pichai|  47|\n",
      "|         Bill|  64|\n",
      "|         Jeff|  56|\n",
      "|         null|null|\n",
      "+-------------+----+\n",
      "\n",
      "Data with the new column productive_hours, hours Column Renamed to working_hours and dropped the location column\n",
      "+-----------+-------------+----+-------------+----------------+\n",
      "|employee_id|employee_name| age|working_hours|productive_hours|\n",
      "+-----------+-------------+----+-------------+----------------+\n",
      "|       G001|       Pichai|  47|           14|              11|\n",
      "|       M002|         Bill|  64|         null|            null|\n",
      "|       A003|         Jeff|  56|         null|            null|\n",
      "|       A004|         null|null|           12|               9|\n",
      "+-----------+-------------+----+-------------+----------------+\n",
      "\n",
      "First three Data points\n",
      "\n",
      "[Row(employee_id='G001', employee_name='Pichai', age=47, working_hours=14, productive_hours=11), Row(employee_id='M002', employee_name='Bill', age=64, working_hours=None, productive_hours=None), Row(employee_id='A003', employee_name='Jeff', age=56, working_hours=None, productive_hours=None)]\n",
      "\n",
      "Summary of Age, Working hours and Productive hours\n",
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               age|     working_hours|  productive_hours|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 3|                 2|                 2|\n",
      "|   mean|55.666666666666664|              13.0|              10.0|\n",
      "| stddev| 8.504900548115382|1.4142135623730951|1.4142135623730951|\n",
      "|    min|                47|                12|                 9|\n",
      "|    max|                64|                14|                11|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# importa sesión  Spark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# creating a Spark Session\n",
    "spark = SparkSession.builder.appName(\"Rows&Columns\").getOrCreate()\n",
    "\n",
    "# sube datos a un dataframe de Spark\n",
    "df = spark.read.csv(\"../Datos/employee_data.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# despliega datos\n",
    "df.show()\n",
    "\n",
    "#accesa el objeto columna age via dataframe\n",
    "df[\"age\"] \n",
    "print(\"Single Column Data\")\n",
    "\n",
    "#accessing single column using select, returns a dataframe\n",
    "df.select(\"age\").show()\n",
    "\n",
    "#accessing multiple column using select, returns a dataframe\n",
    "print(\"Multiple Column Data\")\n",
    "\n",
    "df.select([\"employee_name\",\"age\"]).show() \n",
    "\n",
    "# Adding a new column from existing one with some operation\n",
    "df_new = df.withColumn(\"productive_hours\",df[\"hours\"]-3)\n",
    "\n",
    "#Renamed hours to working_hours\n",
    "df_renamed = df_new.withColumnRenamed(\"hours\",\"working_hours\")\n",
    "\n",
    "#dropping the column location\n",
    "df_final = df_renamed.drop(\"location\") \n",
    "\n",
    "print(\"Data with the new column productive_hours, hours Column Renamed to working_hours and dropped the location column\")\n",
    "df_final.show()\n",
    "print(\"First three Data points\\n\")\n",
    "\n",
    "#Fetching the list of the first 3 values in the dataframe\n",
    "print(df_final.head(3)) \n",
    "\n",
    "print(\"\\nSummary of Age, Working hours and Productive hours\")\n",
    "summary_data = df_final.select([\"age\",\"working_hours\",\"productive_hours\"])\n",
    "summary_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b379cc-d227-4e1b-bf50-62a7b15e9d8a",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">SQL con los dataframes de Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c874dd0-2607-40bd-b778-e45648d46540",
   "metadata": {},
   "source": [
    "Spark Dataframes nos da la opción de usar las consultas SQL para trabajar e interactuar directamente con el DataFrame. La sintaxis es conocida. Observe el siguiente ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50f82c5-5b8b-4166-953b-d95cf7f239a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the results of the select query\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Showing the results after using Where clause in select query\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# datos\n",
    "df = spark.read.csv(\"../Datos/employee_data.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Crea de una vista y la llama associates\n",
    "df.createOrReplaceTempView(\"associates\") \n",
    "\n",
    "#Crea una consulta SQL sobre associates\n",
    "sql_result_1 = spark.sql(\"SELECT * FROM associates\") \n",
    "print(\"Showing the results of the select query\")\n",
    "sql_result_1.show()\n",
    "\n",
    "#Crea una segunda consulta SQL sobre associates\n",
    "sql_result_2 = spark.sql(\"SELECT * FROM associates WHERE age BETWEEN 45 AND 60 AND location='California'\")\n",
    "print(\"Showing the results after using Where clause in select query\")\n",
    "sql_result_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c384d6-7f26-4c37-9a94-06569a5ae947",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Operaciones y funciones con los dataframes de Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe4a90-d637-47e8-89ec-16cd58b47f64",
   "metadata": {},
   "source": [
    "### Función filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a80d35b-971d-4144-98f0-fbe2ec65ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|11-10-2018|     Beer|     110.5|       2|     53.04|      163.54|\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "|05-10-2018|      Rum|     550.0|       2|     264.0|       814.0|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Result after filtering total_amount greater than 1500\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "\n",
      "Result after filtering based on item_price and tax_amount\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "\n",
      "The collected data point's date value is 23-03-2020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'date': '23-03-2020',\n",
       " 'item_name': 'Whisky',\n",
       " 'item_price': 1300.5,\n",
       " 'quantity': 2,\n",
       " 'tax_amount': 624.24,\n",
       " 'total_amount': 1924.74}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkFilters\").getOrCreate()\n",
    "\n",
    "# datos\n",
    "df = spark.read.csv(\"../Datos/items_bought.csv\",header=True,inferSchema=True)\n",
    "df.show(4)\n",
    "\n",
    "print(\"Result after filtering total_amount greater than 1500\")\n",
    "df.filter(\"total_amount>1500\").show() #filtering based on SQL type syntax\n",
    "\n",
    "#filtering based on multiple column conditions\n",
    "print(\"Result after filtering based on item_price and tax_amount\")\n",
    "df.filter((df[\"item_price\"]>1000)&(df[\"tax_amount\"]>500)).show() \n",
    "\n",
    "#collecting the record based on a condition\n",
    "result_data = df.filter((df[\"total_amount\"]==1924.74)).collect()\n",
    "\n",
    "#fetching data from the collected result\n",
    "resulting_date = result_data[0][\"date\"] \n",
    "print(\"The collected data point's date value is \" + resulting_date)\n",
    "\n",
    "#converting the result to a dictionary\n",
    "result_data[0].asDict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc21e51-9004-4752-83a2-cda54dc872cc",
   "metadata": {},
   "source": [
    "### Función groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd563e91-1837-433b-b7f0-86436b0af56a",
   "metadata": {},
   "source": [
    "GroupBy nos permite agrupar filas en función de algún valor común. Generalmente si hay\n",
    "es una columna que se repite, podemos aplicar groupBy en esa columna y podemos obtener cada uno y\n",
    "cada valor repetido como un grupo distinto y podemos aplicar nuestra lógica o requisitos basados\n",
    "sobre estos ciertos grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22abcc7-c52e-41c8-a5e8-b137cd09e664",
   "metadata": {},
   "source": [
    "Hay varios otros métodos que podemos usar con la función groupBy y son:\n",
    "\n",
    "* `count()`: devuelve el recuento de filas para cada grupo.\n",
    "* `mean()`: devuelve la media de los valores de cada grupo.\n",
    "* `max()`: devuelve el máximo de valores para cada grupo.\n",
    "* `min()`: devuelve el mínimo de valores para cada grupo.\n",
    "* `sum()`: devuelve el total o la suma de los valores de cada grupo.\n",
    "* `avg()`: devuelve el promedio de los valores de cada grupo.\n",
    "* `agg()`: se puede usar para calcular más de un agregado a la vez, veremos más\n",
    "\n",
    "\n",
    "Acerca de la función `agg()` en el siguiente tema de la función agregada y también aprenda cómo\n",
    "para usarlo con la función groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361819b8-3d9e-4190-aeb3-df46a1c9309e",
   "metadata": {},
   "source": [
    "### La función de agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ceeaec-8e8e-4f63-99a8-9dd35dfa6ff4",
   "metadata": {},
   "source": [
    "Como su nombre indica, se utiliza para realizar algunas operaciones de agregación.\n",
    "como suma, promedio y así, en un conjunto de valores de las columnas y salidas un consolidado o\n",
    "resultado de un solo valor.\n",
    "\n",
    "\n",
    "La función de agregación se puede usar para agregar entre todas las filas del DataFrame y también\n",
    "como se puede usar con la función groupBy para agregar dentro de las filas de cada grupo.\n",
    "Un punto importante a tener en cuenta es que la función agregada toma en un diccionario como su\n",
    "parámetro. Es decir, podemos dar la clave y el valor del diccionario como nombre de la columna.\n",
    "y el nombre de la operación (como suma, media y así) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63ac05eb-1ede-4d57-a4a6-fb109a412d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|         Audi|          A4|          450|\n",
      "|Mercedes Benz|     G Class|         1200|\n",
      "|          BMW|          X1|          425|\n",
      "|     Mahindra|     XUV 500|          850|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Total revenue_sales per company\n",
      "+-------------+------------------+\n",
      "| company_name|sum(revenue_sales)|\n",
      "+-------------+------------------+\n",
      "|          Kia|              1140|\n",
      "|         Audi|              2275|\n",
      "|     Mahindra|              1640|\n",
      "|          BMW|              1975|\n",
      "|Mercedes Benz|              2570|\n",
      "+-------------+------------------+\n",
      "\n",
      "Total revenue_sales for the entire data\n",
      "+------------------+\n",
      "|sum(revenue_sales)|\n",
      "+------------------+\n",
      "|              9600|\n",
      "+------------------+\n",
      "\n",
      "Max revenue_sales value per company\n",
      "+-------------+------------------+\n",
      "| company_name|max(revenue_sales)|\n",
      "+-------------+------------------+\n",
      "|          Kia|               690|\n",
      "|         Audi|               725|\n",
      "|     Mahindra|               850|\n",
      "|          BMW|               850|\n",
      "|Mercedes Benz|              1200|\n",
      "+-------------+------------------+\n",
      "\n",
      "Ordering the data based on the revenue_sales in ascending order\n",
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|          BMW|          X1|          425|\n",
      "|         Audi|          A4|          450|\n",
      "|          Kia|    Carnival|          450|\n",
      "|Mercedes Benz|     C Class|          470|\n",
      "|         Audi|          Q7|          500|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Ordering the data based on the revenue_sales in descending order\n",
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|Mercedes Benz|     G Class|         1200|\n",
      "|Mercedes Benz|         GLS|          900|\n",
      "|     Mahindra|     XUV 500|          850|\n",
      "|          BMW|          X5|          850|\n",
      "|     Mahindra|     XUV 300|          790|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión  Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkGroupBy&Agg').getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../Datos/company_product_revenue.csv\",header=True,inferSchema=True)\n",
    "df.show(4)\n",
    "\n",
    "print(\"Total revenue_sales per company\")\n",
    "df.groupBy(\"company_name\").sum().show()\n",
    "\n",
    "print(\"Total revenue_sales for the entire data\")\n",
    "df.agg({\"revenue_sales\":\"sum\"}).show()\n",
    "\n",
    "print(\"Max revenue_sales value per company\")\n",
    "df.groupBy(\"company_name\").agg({\"revenue_sales\":\"max\"}).show()\n",
    "\n",
    "print(\"Ordering the data based on the revenue_sales in ascending order\")\n",
    "df.orderBy(\"revenue_sales\").show(5)\n",
    "\n",
    "print(\"Ordering the data based on the revenue_sales in descending order\")\n",
    "df.orderBy(df[\"revenue_sales\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b231c1-67d2-4cd0-a4bb-a2687cf46a7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "038ef9b5-2831-40bf-a263-5ec4da8edbe2",
   "metadata": {},
   "source": [
    "Hay varias funciones estándar que son muy útiles en nuestra creación de lógica y secuencias de comandos,\n",
    "que nos ayudará a manipular y jugar fácilmente con los datos.\n",
    "\n",
    "Hay varias funciones estándar y funciones complejas como la correlación, desviación estándar que podamos usar fácilmente. Para hacer esto, tenemos que importar la funciones estándar queremos usar de `pyspark.sql.functions` y se puede hacer asi por ejemplo:\n",
    "\n",
    "+ `from pyspark.sql.functions import stddev,avg, format_number`\n",
    "\n",
    "Esta funciones se pueden combinar como por ejemplo de esta forma:\n",
    "\n",
    "+ `df.select(avg(“revenue_sales”).alias(“Average”).show()`\n",
    "\n",
    "Veámos el ejemplo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf510e5-c6ff-4f48-afd8-be60407577b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Mean Revenue Sales|\n",
      "+------------------+\n",
      "| 685.7142857142857|\n",
      "+------------------+\n",
      "\n",
      "Average Revenue Sales value is 685.7142857142857\n",
      "+-----------------+\n",
      "|Formatted Average|\n",
      "+-----------------+\n",
      "|           685.71|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión  Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkInbuiltFunctions\").getOrCreate()\n",
    "\n",
    "# Importa funciones preconstruidas\n",
    "from pyspark.sql.functions import mean,avg,format_number \n",
    "\n",
    "df = spark.read.csv(\"../Datos/company_product_revenue.csv\",header=True,inferSchema =True)\n",
    "\n",
    "#Usando la función mean \n",
    "df.select(mean(\"revenue_sales\").alias(\"Mean Revenue Sales\")).show()\n",
    "\n",
    "#Usando la función average\n",
    "result_avg = df.select(avg(\"revenue_sales\").alias(\"Average Revenue Sales\")) \n",
    "\n",
    "#formateando  números a 2 decimales                                                \n",
    "print(\"Average Revenue Sales value is {0}\".format(result_avg.head()[0]))\n",
    "result_avg.select(format_number(\"Average revenue Sales\",2).alias(\"Formatted Average\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ead66-eee8-4a36-ab4e-bf8e1e9b9126",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Datos faltantes en los Dataframes  de Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d546ca1-a577-4940-9b5c-0da3dbd7a78b",
   "metadata": {},
   "source": [
    "Hay varias formas de manejar este problema de datos faltantes. Podríamos eliminar el punto de datos que contiene datos faltantes o completarlo con algún valor calculado o predeterminado y así.\n",
    "\n",
    "Vamos a aprender las diversas formas en que podemos manejar estos datos faltantes en detalle. Y para comprender mejor, trabajemos con algunos datos reales que contienen pocos valores nulos o faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf673d-f26b-4411-af03-dc49deed2129",
   "metadata": {},
   "source": [
    "### Descartando las filas o puntos de datos que contienen valores nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c205235-ab9d-4a24-bcc0-da8a28065add",
   "metadata": {},
   "source": [
    "Podemos eliminar los valores faltantes de nuestro DataFrame usando el método `drop()` que está asociado con el método \"na\" en el DataFrame. \n",
    "\n",
    "La función *drop()* devuelve un nuevo DataFrame omitiendo todas las filas con valores nulos. Por ejemplo: \n",
    "\n",
    "+ `df.na.drop().show()`\n",
    "\n",
    "Esto elimina todas las columnas que tienen valores nulos presentes en el DataFrame \"df\".\n",
    "Las funciones *drop()* tienen 3 parámetros importantes:\n",
    "\n",
    "* how:  el parámetro *how* tiene dos opciones principales \"cualquiera\" y \"todos\". De forma predeterminada, *how='any'*, y significa eliminar todas esas filas, si alguno de los valores de las filas tiene un valor nulo. La segunda opción es how='all', que elimina todos los filas que tienen todos los valores de fila como nulos. Ejemplo: \n",
    "\n",
    "+ `df.na.drop(how = ’all’).show()`\n",
    "\n",
    "\n",
    "* thresh:  el parámetro *thresh* se puede asignar con un número natural y basado en el valor que considere todas las filas que tienen valores no nulos iguales o superiores al número de umbral que se han  especificado. Ejemplo: \n",
    "\n",
    "+ `df.na.drop(thresh = 2).show ()`\n",
    "\n",
    "+ subset-: El parámetro subset se puede utilizar para especificar las columnas que queremos considerar para comprobar o validar los valores nulos. Ejemplo: \n",
    "\n",
    "+ `df.na.drop(subset = [column_name(s)]).show()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dfa04-9c98-4e14-ad76-5866a22afa86",
   "metadata": {},
   "source": [
    "### Llenado de valores nulos con otros valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f9282-298e-455b-b326-6ca4f6b8d71b",
   "metadata": {},
   "source": [
    "Sintaxis: `df.na.fill(value, subset=None)` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef39b2-c69d-4485-b4b6-b64d7218715f",
   "metadata": {},
   "source": [
    "### Reemplazo de valores específicos en el dataframe con otros valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b765e-746b-4ba4-a658-1a4940f5bab2",
   "metadata": {},
   "source": [
    "Sintaxis: `df.na.replace(to_replace, replace, subset=None)` \n",
    "\n",
    "Veamos el ejemplo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed38e65e-3983-4947-a388-ad816787e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Data after dropping the rows having null values\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n",
      "Data after dropping the rows having atleast 4 non-null values\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       M002|         Bill| 64|Washington| null|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n",
      "Data after dropping the rows having null values in hours column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Data after filling the rows having null values in hours column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington|   12|\n",
      "|       A003|         Jeff|  56|      null|   12|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Data after filling the rows having null values in hours column with calculated mean value\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington|   13|\n",
      "|       A003|         Jeff|  56|      null|   13|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n",
      "Data after replacing a specific rows value in employee_name column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Sundar|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión  Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkMisingData\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../Datos/employee_data.csv\",header=True,inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "print(\"Data after dropping the rows having null values\")\n",
    "df.na.drop().show()\n",
    "\n",
    "print(\"Data after dropping the rows having atleast 4 non-null values\")\n",
    "df.na.drop(thresh=4).show()\n",
    "\n",
    "print(\"Data after dropping the rows having null values in hours column\")\n",
    "df.na.drop(subset=\"hours\").show()\n",
    "\n",
    "print(\"Data after filling the rows having null values in hours column\")\n",
    "\n",
    "df.na.fill(12,subset=\"hours\").show()\n",
    "\n",
    "#importa la función mean\n",
    "from pyspark.sql.functions import mean \n",
    "mean_value = df.select(mean(\"hours\")).collect()[0][0]\n",
    "\n",
    "#calcula el valor medio\n",
    "print(\"Data after filling the rows having null values in hours column with calculated mean value\")\n",
    "df.na.fill(mean_value,subset=\"hours\").show()\n",
    "\n",
    "print(\"Data after replacing a specific rows value in employee_name column\")\n",
    "df.na.replace(\"Pichai\",\"Sundar\",subset=\"employee_name\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63830d4e-a08a-4126-aeca-f30128a6ce60",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Trabajar con fecha y hora en Spark Dataframe</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c13ed-b144-49d3-91f0-1057c5855ae6",
   "metadata": {},
   "source": [
    "Es muy importante comprender cómo lidiar con la fecha y la hora usando dataframes de Spark.\n",
    "\n",
    "La mayoría de los conjuntos de datos pueden tener una característica o columna basada en fecha y hora en su esquema. Y algunos de los requisitos también se basarían en el criterio de fecha y hora.\n",
    "\n",
    "Para lidiar con estos escenarios de fecha y hora, veamos algunas funciones incorporadas que\n",
    "están disponibles para nosotros con la biblioteca pyspark.\n",
    "\n",
    "Algunas de las funciones de tiempo de datos más utilizadas:\n",
    "\n",
    "+ *dayofmounth()*: Extraer el día del mes de una fecha determinada.\n",
    "+ *mounth*: Extraer mes de la fecha\n",
    "+ *year*: Extraer año de la fecha\n",
    "+ *weekofyear*: Extraer semana del año en la fecha\n",
    "+ *date_format(date, format)*: Convertir una fecha, marca de tiempo o cadena en un valor de cadena en el formato especificado por el formato de fecha dado por el segundo argumento.\n",
    "\n",
    "Veamos el ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f405d2db-cb5c-43cb-bee7-ec4f38552609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|11-10-2018|     Beer|     110.5|       2|     53.04|      163.54|\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Schema with date as string datatype\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "Schema with date column string datatype converted to date datatype\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- formatted_date: date (nullable = true)\n",
      "\n",
      "Data after dropping the date column which was of string type\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "|item_name|item_price|quantity|tax_amount|total_amount|formatted_date|\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "|     Beer|     110.5|       2|     53.04|      163.54|    2018-10-11|\n",
      "|   Whisky|    1250.0|       1|     300.0|      1550.0|    2018-02-14|\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Data Extraction from dates\n",
      "+---------+-----------+----------+-----+----+\n",
      "|item_name|week_number|day_number|month|year|\n",
      "+---------+-----------+----------+-----+----+\n",
      "|     Beer|         41|        11|   10|2018|\n",
      "|   Whisky|          7|        14|    2|2018|\n",
      "|   Whisky|         13|        23|    3|2020|\n",
      "+---------+-----------+----------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------------------------------------+\n",
      "|item_name|date_format(formatted_date, MM/dd/yyyy)|\n",
      "+---------+---------------------------------------+\n",
      "|     Beer|                             10/11/2018|\n",
      "|   Whisky|                             02/14/2018|\n",
      "+---------+---------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Usecase - Total amount of items purchased in that particular year\n",
      "+----+-----------------+\n",
      "|year|Total Expenditure|\n",
      "+----+-----------------+\n",
      "|2018|             6054|\n",
      "|2019|             4038|\n",
      "|2020|             8080|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busca Spark\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# sesión  Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkDateTime\").getOrCreate()\n",
    "\n",
    "# datos\n",
    "df = spark.read.csv(\"../Datos/items_bought.csv\",header=True,inferSchema=True)\n",
    "df.show(3)\n",
    "\n",
    "print(\"Schema with date as string datatype\")\n",
    "df.printSchema()\n",
    "\n",
    "#importar las funciones necesarias para convertir la cadena de fecha al tipo de fecha\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date \n",
    "updated_df = df.withColumn('formatted_date',to_date(unix_timestamp(df['date'],'dd-MM-yyyy').cast('timestamp')))\n",
    "\n",
    "print(\"Schema with date column string datatype converted to date datatype\")\n",
    "updated_df.printSchema()\n",
    "\n",
    "print(\"Data after dropping the date column which was of string type\")\n",
    "updated_df=updated_df.drop(\"date\") #dropping the date column with string type\n",
    "updated_df.show(2)\n",
    "\n",
    "#extraer datos de fechas\n",
    "from pyspark.sql.functions import weekofyear, dayofmonth,month,year,date_format \n",
    "\n",
    "print(\"Data Extraction from dates\")\n",
    "final_df = updated_df.select(updated_df[\"item_name\"],\n",
    "weekofyear(updated_df[\"formatted_date\"]).alias(\"week_number\"),\n",
    "dayofmonth(updated_df[\"formatted_date\"]).alias(\"day_number\"),\n",
    "month(updated_df[\"formatted_date\"]).alias(\"month\"),\n",
    "year(updated_df[\"formatted_date\"]).alias(\"year\"))\n",
    "final_df.show(3)\n",
    "\n",
    "#convertir el tipo de fecha a una cadena de formato de fecha diferente\n",
    "date_string_value = updated_df.select(df[\"item_name\"],date_format(updated_df[\"formatted_date\"],'MM/dd/yyyy')) \n",
    "date_string_value.show(2)\n",
    "\n",
    "print(\"Usecase - Total amount of items purchased in that particular year\")\n",
    "final_format=final_df.groupBy(\"year\").sum().select([\"year\",\"sum(year)\"])\n",
    "final_format.withColumnRenamed(\"sum(year)\",\"Total Expenditure\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
