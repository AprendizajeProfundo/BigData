{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solar-basin",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-regular",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Archivos HDF5<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-sixth",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-allen",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "1. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-auction",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-trailer",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-vinyl",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31594735-f362-4f0f-a338-934d37149996",
   "metadata": {},
   "source": [
    "1. Oleg Jarma, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88034b-4589-4922-9d28-69af6bfa7bfb",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab7c55-36f5-4476-83ee-90bca8144c82",
   "metadata": {},
   "source": [
    "1. Basado en Andrew Collete, Python and HDF5, O'Reilly, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-provision",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-terminal",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Primeros pasos con archivos HDF5](#Primeros-pasos-con-archivos-HDF5)\n",
    "* [Datasets HDF5](#Datasets-HDF5)\n",
    "* [Fragmentación (chunking) y compresión](#Fragmentación-(chunking)-y-compresión)\n",
    "* [Grupos- enlaces e iteración](#Grupos,-enlaces-e-iteración)\n",
    "* [Iteración y contenedor](#Iteración-y-contenedor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-phrase",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-qualification",
   "metadata": {},
   "source": [
    "En esta lección revisamos uno de lso formatos más usados para el almacenamiento de datos de tipo científicos. HDF5 es recomendado para registrar informaciones secuenciales, por ejemplo proveniente de medidas de equipos científicos, salud, etc.\n",
    "\n",
    "Los datos que se almacenan en archivos de tipo HDF5 se llaman datasets y básicamente corresponden a arrays multidimensionales.\n",
    "\n",
    "Cuando los datos son de tipo tabular (tipos tabla de datos o dataframes) otrso formatos son recomendados, como por ejemplo el formato *parquet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-brief",
   "metadata": {},
   "source": [
    "### Importar modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reserved-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "joint-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define carpeta de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-recording",
   "metadata": {},
   "source": [
    "### Organización de datos y metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-invite",
   "metadata": {},
   "source": [
    "### Genera datos simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "voluntary-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibilidad\n",
    "np.random.seed(100)\n",
    "\n",
    "# Temperatura\n",
    "temperatura_15 = np.random.random(1024) # temperaturas estación. 15\n",
    "temperatura_10 = np.random.random(1024)\n",
    "\n",
    "# Viento\n",
    "viento_15 = np.random.random(2048)\n",
    "viento_10 = np.random.random(2048)\n",
    "\n",
    "estacion_15 = 15  # estación 15\n",
    "estacion_10 = 10\n",
    "\n",
    "dt_temp = 10 # delta-T temperatura\n",
    "dt_viento = 20\n",
    "\n",
    "start_time_temp_15 = 1375204299 # en Unix time\n",
    "start_time_temp_10 = 1375204500 # en Unix time\n",
    "start_time_viento_15 = 137550000\n",
    "start_time_viento_10 = 137550000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54340494 0.27836939 0.42451759 ... 0.24908888 0.27119505 0.05880634]\n"
     ]
    }
   ],
   "source": [
    "print(temperatura_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-diabetes",
   "metadata": {},
   "source": [
    "### Vamos a crear la siguiente estructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-assets",
   "metadata": {},
   "source": [
    "* Archivo : clima.hdf5\n",
    "* Estación (15 para el ejemplo): /15/\n",
    "* Temperatura: /15/temperatura\n",
    "* Viento: /15/viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "variable-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datos/clima.hdf5\n",
      "<HDF5 file \"clima.hdf5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "folder = '../Datos/'\n",
    "file_name = \"clima.hdf5\" # archivo\n",
    "file = os.path.join(folder, file_name)\n",
    "\n",
    "f = h5py.File(file, mode='w') \n",
    "print(file)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "f['/15/temperatura'] = temperatura_15\n",
    "f['/15/temperatura'].attrs[\"dt\"] = dt_temp\n",
    "f['/15/temperatura'].attrs['star_time'] = start_time_temp_15\n",
    "\n",
    "f['/15/viento'] = viento_15\n",
    "f['/15/viento'].attrs[\"dt\"] = dt_viento\n",
    "f['/15/viento'].attrs['star_time'] = start_time_viento_15\n",
    "\n",
    "f['/10/temperatura'] = temperatura_10\n",
    "f['/10/temperatura'].attrs[\"dt\"] = dt_temp\n",
    "f['/10/temperatura'].attrs['star_time'] = start_time_temp_10\n",
    "\n",
    "f['/10/viento'] = viento_10\n",
    "f['/10/viento'].attrs[\"dt\"] = dt_viento\n",
    "f['/10/viento'].attrs['star_time'] = start_time_viento_10\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-consensus",
   "metadata": {},
   "source": [
    "### Recupera datos del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "banner-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(folder, file_name)\n",
    "f = h5py.File(file, mode='r') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriented-sandwich",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['10', '15']>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "concerned-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_15 = f['/15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sacred-march",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/15\" (2 members)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diverse-drilling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['temperatura', 'viento']>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_15.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valued-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: 10\n",
      "star_time: 1375204299\n"
     ]
    }
   ],
   "source": [
    "dataset = f['/15/temperatura']\n",
    "\n",
    "for key, value in dataset.attrs.items():\n",
    "    print('%s: %s' %  (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-kidney",
   "metadata": {},
   "source": [
    "### Rebanado(slicing) de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exclusive-assist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54340494, 0.27836939, 0.42451759, 0.84477613, 0.00471886,\n",
       "       0.12156912, 0.67074908, 0.82585276, 0.13670659, 0.57509333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stupid-start",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "natural-protest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4948712717532915"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "confidential-casino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-exhibit",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Primeros pasos con archivos HDF5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-verse",
   "metadata": {},
   "source": [
    "HDF5 utiliza un sistema de tipos muy similar a Numpy. Cada `array` (dataset) o conjunto de datos en un archivo HDF5 tiene un tipo fijo representado por un objeto de tipo. El paquete `h5py` mapea automáticamente el HDF5\n",
    "type system en NumPy dtypes, lo que, entre otras cosas, facilita el intercambio\n",
    "datos con NumPy.\n",
    "\n",
    "Por ejemplo, HDF5, toma prestada esta sintaxis de \"corte\" para permitir cargar solo porciones de un conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-keeping",
   "metadata": {},
   "source": [
    "### Mi primer archivo hdf5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "referenced-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File(\"../Datos/nombre.hdf5\",\"a\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-influence",
   "metadata": {},
   "source": [
    "### Modos de abrir una archivo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "confirmed-consumer",
   "metadata": {},
   "source": [
    "f = h5py.File(\"../data/nombre.hdf5\",\"w\") # Nuevo archivo. Si existe lo reescribe\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"w-\") # Nuevo archivo, solo si no existe\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"r\") # Abre solo lectura. Debe existir\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"r+\") # Abre lectura y escritura. Debe existir\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"a\") # Abre lectura y escritura. Si no existe, lo crea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-gibson",
   "metadata": {},
   "source": [
    "### Usando un manejador de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-costa",
   "metadata": {},
   "source": [
    "El  ejemplo típico de manejo de contexto con archivos es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/garbage.txt\",\"w\") as f:\n",
    "    f.write(\"Hello!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-witch",
   "metadata": {},
   "source": [
    "Cuando se sale del contexto, el archvo se cierra automáticamente.\n",
    "\n",
    "Podemos hacer lo mismo con archivos hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/nombre.hdf5\",\"w\") as f:\n",
    "    print(f[\"dataset_perdido\"]) # error dataset no existe en el archivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys()) # error. Archivo cerrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-unemployment",
   "metadata": {},
   "source": [
    "### Controladores de archivos (file drivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-reading",
   "metadata": {},
   "source": [
    "Los controladores de archivos se encuentran entre el sistema de archivos y el mundo de alto nivel de los grupos HDF5, los datasets y atributos. Los controladores se ocupan de la mecánica del *manejo de espacio en memoria* de los   archivos HDF5 disponibles en el disco.\n",
    "\n",
    "Por lo general, no tendrá que preocuparse por cúal controlador está en uso, ya que el controlador predeterminado funciona bien para la mayoría de las aplicaciones.\n",
    "\n",
    "Lo mejor de los controladores es que una vez que se abre el archivo, son totalmente transparentes.\n",
    "Simplemente use la biblioteca HDF5 como de costumbre, y el controlador se encarga de la mecánica de almacenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-mauritius",
   "metadata": {},
   "source": [
    "#### Controlador (driver) core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-yugoslavia",
   "metadata": {},
   "source": [
    "El controlador `core` almacena su archivo completamente en la memoria. Obviamente, hay un límite en cuanto a cómo\n",
    "gran cantidad de datos que puede almacenar, pero la compensación es lecturas y escrituras increíblemente rápidas. Es una gran elección cuando desea la velocidad de acceso a la memoria, pero también desea utilizar el HDF5\n",
    "estructuras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "physical-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../Datos/nombre.hdf5', driver='core')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-bryan",
   "metadata": {},
   "source": [
    "Para decirle a HDF5 que cree un archivo  y que guarde la imagen actual del archivo cuando se cierre, puede usar `backing store`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/nombre.hdf5', driver='core', backing_store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-webmaster",
   "metadata": {},
   "source": [
    "#### Controlador family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-candle",
   "metadata": {},
   "source": [
    "Permite separar un archivo en múltiple imágenes, cada de la cuales comparte cierto tamaño máximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide el archivo en trozos de 1Gb de memoria\n",
    "f = h5py.File('../data/family.hdf5', memb_size=1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-warner",
   "metadata": {},
   "source": [
    "El tamaño por defecto es memb_size = $2^{31}-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-giant",
   "metadata": {},
   "source": [
    "#### Controlador mpio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-processor",
   "metadata": {},
   "source": [
    "Este controlador es el corazón de Parallel HDF5. Le permite acceder al mismo archivo desde múltiples\n",
    "procesos al mismo tiempo. Puede tener docenas o incluso cientos de procesos de computación paralela, todos los cuales comparten una vista coherente de un solo archivo en el disco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-portsmouth",
   "metadata": {},
   "source": [
    "### El bloque del usuario (user block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-beijing",
   "metadata": {},
   "source": [
    "Una característica interesante de HDF5 es que los archivos pueden estar precedidos por datos arbitrarios de usuario.\n",
    "\n",
    "Cuando se abre un archivo, la biblioteca busca el encabezado HDF5 al comienzo del\n",
    "archivo, luego 512 bytes en, luego 1024, y así sucesivamente en potencias de 2. \n",
    "\n",
    "Dicho espacio al principio del archivo se denomina `bloque de usuario` y allí el usuario puede almacenar los datos que desee.\n",
    "Las únicas restricciones están en el tamaño del bloque (potencias de 2 y al menos 512), y que\n",
    "no debería tener el archivo abierto en HDF5 al escribir en el bloque de usuario. Aquí hay un\n",
    "ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "illegal-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('../Datos/userblock_example.hdf5', 'w', userblock_size=512)\n",
    "print(f.userblock_size)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "infectious-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra = 'a'*512\n",
    "with open('../Datos/userblock_example.hdf5', 'r+') as f:\n",
    "    f.write(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "canadian-carolina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../Datos/userblock_example.hdf5', 'rb+') as f: # abre como binario porque así está almacenado\n",
    "    pal = f.read(512)\n",
    "pal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-machinery",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Datasets HDF5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-iceland",
   "metadata": {},
   "source": [
    "Los conjuntos de datos (`datasets`) son la característica central de HDF5. Puede pensar en ellos como arreglos (arrays) NumPy que viven en disco. Cada conjunto de datos en HDF5 tiene un nombre, un tipo y una forma, y admite datos aleatorios.\n",
    "acceso. \n",
    "\n",
    "A diferencia de np.save y friends integrados en Numpy, no es necesario leer ni escribir el arreglo  completo como un bloque; puede usar la sintaxis estándar de NumPy para cortar, leer o\n",
    "escribir solo las partes que desee del array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-direction",
   "metadata": {},
   "source": [
    "### Elemento escenciales sobre datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-brighton",
   "metadata": {},
   "source": [
    "Primero, creemos un archivo para que tengamos un lugar donde almacenar nuestros conjuntos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "developed-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../Datos/testfile.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-treasure",
   "metadata": {},
   "source": [
    "Cada conjunto de datos de un archivo HDF5 tiene un nombre. Veamos qué sucede si asignamos un nuevo\n",
    "array NumPy a un nombre en el archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "alternative-labor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"my_dataset\": shape (5, 2), type \"<f8\">"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.ones((5,2))\n",
    "f['my_dataset'] = arr\n",
    "dset = f['my_dataset'] \n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "controlling-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['my_dataset']>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-assistant",
   "metadata": {},
   "source": [
    "Observe que *dset* es una instancia de la clase *h5py-Dataset*. Este objeto se accesa como cualquier archivo Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "egyptian-coffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(dset[:])\n",
    "print(dset[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-disposition",
   "metadata": {},
   "source": [
    "### Type y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "invisible-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(5, 2)\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dset.dtype)\n",
    "print(dset.shape)\n",
    "print(dset[:, -1])\n",
    "out = dset[...]\n",
    "print(type(out))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-sacrifice",
   "metadata": {},
   "source": [
    "### Creación de datasets vacíos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-quarter",
   "metadata": {},
   "source": [
    "No es necesario tener una matriz NumPy lista para crear un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "musical-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('test1', shape=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cardiovascular-twelve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['my_dataset', 'test1']>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ignored-manchester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-monster",
   "metadata": {},
   "source": [
    "HDF5 es lo suficientemente inteligente como para asignar solo la cantidad de espacio en el disco que realmente necesita\n",
    "almacenar los datos que escribe. A continuación, se muestra un ejemplo: suponga que desea crear un conjunto de datos 1D\n",
    "que puede contener muestras de datos de 4 gigabytes de un experimento de larga duración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "provincial-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('big_dataset', (1024**3), dtype= np.float32)\n",
    "dset[0:1024] = np.arange(1024)\n",
    "f.flush() # envia los datos al archivo\n",
    "#f.close()\n",
    "\n",
    "# Por favor revise el tamaño del archivo en el sistema operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -lh testfile.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-island",
   "metadata": {},
   "source": [
    "### Conversión de tipos al enviar al archivo hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "golden-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "bigdata = np.ones((100,1000)) # np.float64\n",
    "print(bigdata.dtype)\n",
    "print(bigdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-craps",
   "metadata": {},
   "source": [
    "Guarda con el tipo de dato original: np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "electronic-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../Datos/big1.hdf5','w') as f1:\n",
    "    f1['big'] = bigdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-samuel",
   "metadata": {},
   "source": [
    "Cambia el tipo de dato al enviar al archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cardiac-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../Datos/big2.hdf5', 'w') as f2:\n",
    "    f2.create_dataset('big', data=bigdata, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recupera los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "exact-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "f1 = h5py.File('../Datos/big1.hdf5')\n",
    "f2 = h5py.File('../Datos/big2.hdf5')\n",
    "print(f1['big'].dtype)\n",
    "print(f2['big'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-tension",
   "metadata": {},
   "source": [
    "Conversión automática de tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-haven",
   "metadata": {},
   "source": [
    "La propia biblioteca HDF5 maneja la conversión de tipos y lo hace sobre la marcha (*on the fly*) cuando se guarda en o se lee de un archivo.\n",
    "\n",
    "Vemos como sucede esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f2['big']\n",
    "print(dset.dtype)\n",
    "print(dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-tender",
   "metadata": {},
   "source": [
    "En este momento *dset* apunta a los datos en el archivo, pero no han sido cargados en memoria. Para cargarlos con 'float64' lo que debe hacer asignar primero un arreglo vacío de doble precisión en la memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_out = np.empty((100, 1000), dtype= np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-management",
   "metadata": {},
   "source": [
    "Ahora cargamos los datos a la memoria en ese espacio asignao en memoria. La API de HDF5 hace la conversón por el camino (*on the fly*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.read_direct(big_out)\n",
    "print(dset.dtype)\n",
    "print(big_out.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-converter",
   "metadata": {},
   "source": [
    "*dset* sigue apuntando al archivo, mientras que big_out apunta a los datos (convertidos) en memoria.\n",
    "\n",
    "En realidad con *read_direct* no necesariamente tiene que leer todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-calcium",
   "metadata": {},
   "source": [
    "#### Lectura con astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dset.astype('float64'):\n",
    "    out = dset[0,:]\n",
    "\n",
    "print(out.dtype)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-cleveland",
   "metadata": {},
   "source": [
    "### Lectura y escritura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-lightweight",
   "metadata": {},
   "source": [
    "En esta seccion estudiamos inportantes asuntos de implementación  que mejoran el desempeño de los programas con hdf5. Primero estudiamos algunas diferencias entre los arreglos de Numpy y los datasets de hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-citizenship",
   "metadata": {},
   "source": [
    "#### Rebanado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-forestry",
   "metadata": {},
   "source": [
    "Empezamos leyendo de un dataset existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "f2 = h5py.File('../Datos/big1.hdf5','r+')\n",
    "print(f2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f2['big']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dset[0:10, 20:70]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-novel",
   "metadata": {},
   "source": [
    "#### ¿Que ocurrió?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-summit",
   "metadata": {},
   "source": [
    "1. h5py calcula la forma (10, 50) del array resultante.\n",
    "2. A un array  NumPy vacío se le asigna la forma (10, 50).\n",
    "3. HDF5 selecciona la parte apropiada del conjunto de datos.\n",
    "4. HDF5 copia datos del conjunto de datos en el arreglo NumPy vacía.\n",
    "5. Se devuelve el arreglo NumPy recién llenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-blackberry",
   "metadata": {},
   "source": [
    "#### Tip 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-sailing",
   "metadata": {},
   "source": [
    "Tomar rebabadas de tamaño razonable. Revise los dos siguientes snippets de código. ¿Cuál es más eficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequear valiores negativos y reemplazarlos por cero (0).\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "f2 = h5py.File('../Datos/big1.hdf5','r+')\n",
    "\n",
    "for ix in range(100):\n",
    "    for iy in range(1000):\n",
    "        val = dset[ix,iy]     # lee elemento\n",
    "        if val < 0: \n",
    "            dset[ix,iy] = 0   # recorta a 0 si es negativo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequear valiores negativos y reemplazarlos por cero (0).\n",
    "\n",
    "%time\n",
    "for ix in range(100):\n",
    "    val = dset[ix,:] # lee una fila\n",
    "    val[val<0] = 0    # recorta si es negativo\n",
    "    dset[ix,:] = val  # escribe de regreso en el dataset hdf5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-offset",
   "metadata": {},
   "source": [
    "#### Comparación con dask- array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.from_array(dset, chunks=(100))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "x[x<0] = 0\n",
    "dset[:] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-spokesman",
   "metadata": {},
   "source": [
    "#### Indexación de datasets hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "f = h5py.File('../Datos/example1.hdf5','w')\n",
    "dset = f.create_dataset('range', data=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset[4])   # un elemento\n",
    "print(dset[4:8]) # rebanado\n",
    "print(dset[...]) # notación puntos suspensivos (ellipsis) todo\n",
    "print(dset[:])   # todo en una dimensión\n",
    "print(dset[4:-1]) # notación del último elemento\n",
    "print(dset[::-1])   # Fallo. Recorrer en modo reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-provincial",
   "metadata": {},
   "source": [
    "#### Rebanado multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('4d', shape=(100,80,50,20))\n",
    "print(dset.shape)\n",
    "print(dset[...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('1d', shape=(1,), data = 42)\n",
    "\n",
    "print(dset[0])  \n",
    "print(dset.shape)\n",
    "print(dset[...].shape) \n",
    "print(dset[:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-stomach",
   "metadata": {},
   "source": [
    "#### Indexación boolena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-limitation",
   "metadata": {},
   "source": [
    "Como en Numpy. se pueden usar máscara con valores boolenos para extraer  elementos de un dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random(10)*2 -1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('random', data=data)\n",
    "dset[data<0] = 0\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[data<0] = -1*data[data<0]\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-jaguar",
   "metadata": {},
   "source": [
    "#### Coordenadas con listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[[1,2,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-diesel",
   "metadata": {},
   "source": [
    "#### Leyendo directamente en un arreglo existente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-refrigerator",
   "metadata": {},
   "source": [
    "Al leer directamente en un array existente se llena el arrelgo y se hace el recast (conversión de tipo) de manera automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('100_1000_array', shape=(100, 1000), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.empty((100,1000), dtype=np.float64)\n",
    "dset.read_direct(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-transcript",
   "metadata": {},
   "source": [
    "Supongamos que queremos leer la primera fila, en dset [0 ,:], y depositarlo\n",
    "en el array de salida en fila 50: out[50 ,:]. Podemos utilizar las palabras clave *source_sel* y *dest_sel*, para la selección de la fuente y la selección del destino respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.read_direct(out, source_sel = np.s_[0,:], dest_sel= np.s_[50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-dietary",
   "metadata": {},
   "source": [
    "Observe los dos siguientes snippets de código. ¿Cuál es más eficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dset[:, 0:50]\n",
    "print(out.shape)\n",
    "means = out.mean(axis=1)\n",
    "print(means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.empty((100,50),dtype = np.float32)\n",
    "dset.read_direct(out, np.s_[:,0:50])\n",
    "mean = out.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-trade",
   "metadata": {},
   "source": [
    "Este puede parecer un caso trivial, pero hay una diferencia importante entre los dos  enfoques. \n",
    "\n",
    "* En el primer ejemplo, h5py crea internamente la matriz out, que se utiliza para almacenar la rebanada y luego desecharla. \n",
    "* En el segundo ejemplo, out es asignado por el usuario y se puede reutilizar para futuras llamadas a read_direct.\n",
    "\n",
    "No hay dierencia en el desempeño, pero ahora evaluamos un arreglo grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('test', shape=(10000, 10000), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[:] = np.random.random(10000) # uso de broadcasting para completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4)\n",
    "import dask.array as da\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "dset[:,0:500].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy\n",
    "out = np.empty((10000, 500), dtype=np.float32)\n",
    "%time\n",
    "dset.read_direct(out, np.s_[:,0:500])\n",
    "out.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask\n",
    "%time\n",
    "x = da.from_array(dset[:,0:500], chunks=(4))\n",
    "y= x.mean(axis=1)\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dataset():\n",
    "    dset[:,0:500].mean(axis=1)\n",
    "\n",
    "def time_numpy():\n",
    "    dset.read_direct(out, np.s_[:,0:500])\n",
    "    out.mean(axis=1)\n",
    "\n",
    "#def time_dask():\n",
    "#    x = da.from_array(dset[:,0:500], chunks=(4))\n",
    "#    y = x.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(time_dataset, number=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(time_numpy, number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit(time_dask, number=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-toilet",
   "metadata": {},
   "source": [
    "### Cambio de tamaño de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-reality",
   "metadata": {},
   "source": [
    "Así como en Numpy se puede cambiar el tamaño de los arreglos, mientras se mantenga la coherencia en el tamaño global, dejando al usuario la responsabilidad de la interpretabilidad de los datos, con los datasets de hdf5 podemos hacer lo mismo, pero es posible perder información. Veámos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('fixed', (2,2))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-question",
   "metadata": {},
   "source": [
    "La propiedad maxshape sugiere que el tamaño podría variar hasta esos extremos. En efecto, asi es. Veámos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('resizable', (2,2), maxshape=(2,2))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.resize((1,1))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.resize((1,3)) # falla porque 3>2\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-daughter",
   "metadata": {},
   "source": [
    "No esposible cambiar las dimensiones, es decir, el número total de ejes, de una dataset. Pero si se pueden dejar dimensiones sin un tamaño definido para que crezcan idefinidamente. Basta colocar en la posición respectiva *None* cuando crea el dataset. Veámos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('unlimited', (2,2), maxshape =(2,None))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.resize((2,2*30))\n",
    "print(dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-trade",
   "metadata": {},
   "source": [
    "#### Cuidados con el cambio de tamaños en datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-closing",
   "metadata": {},
   "source": [
    "Las reglas de Numpy no aplican exactamento a los dataset. Observe el siguiente ejemplo Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a.resize((1,4))\n",
    "print(a)\n",
    "\n",
    "a.resize((1,10))\n",
    "print(a)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gibraltar",
   "metadata": {},
   "source": [
    "Ahora veámos el mismo código con dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('sizetest', (2,2), dtype=np.int32, \n",
    "                        maxshape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[...] = [[1,2],[3,4]]\n",
    "print(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.resize((1,4))\n",
    "dset[...] # se perdió la fila 1, hdf5 no pudo hacer el reordenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.resize((1,10))\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-failing",
   "metadata": {},
   "source": [
    "#### Cuando y como hacer el cambio de tamaño del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-record",
   "metadata": {},
   "source": [
    "Mostramos dos vías. La segunda es más recomendada.\n",
    "\n",
    "1. Se van agregando elementos a la medida que el datset crece\n",
    "2. Se crea un dataset grande y luego se poda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregado al paso\n",
    "\n",
    "dset1= f.create_dataset('time_traces', (1,1000), maxshape=(None,1000))\n",
    "\n",
    "def add_trace(arr):\n",
    "    dset.resize(dset1.shape[0]+1, 1000)\n",
    "    dset[-1,:] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando dataset grande y al final podando\n",
    "\n",
    "dset2 = f.create_dataset('time_traces_2', (5000,1000), maxshape=(None,1000))\n",
    "\n",
    "ntraces= 0\n",
    "def add_trace_2(arr):\n",
    "    global ntraces\n",
    "    dset2[ntraces,:] = arr\n",
    "    ntraces += 1\n",
    "\n",
    "def done():\n",
    "    dset.resize((ntraces,1000))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-collar",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Fragmentación (chunking) y compresión</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-wallace",
   "metadata": {},
   "source": [
    "Consideremos el siguiente arreglo 2-dimensional de Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([['A','B'],['C','D']])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-convenience",
   "metadata": {},
   "source": [
    "En realidad el arreglo esta organizado en la memoria linealmente, en el siguiente orden\n",
    "\n",
    "+ 'A', 'B', 'C', 'D'\n",
    "\n",
    "Lo mismo ocurre con los datasets de hdf5. Esto implica que algunas forma de recuperacion de información son mpás eficientes que otras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-netherlands",
   "metadata": {},
   "source": [
    "Suponga que va a almacenar 10 imágenes en b/n (escala de grises) de tamaño 480*640. Entonces haría algo como lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('imagetest.hdf5','w')\n",
    "dset = f.create_dataset('Imagenes', (100,480,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-melbourne",
   "metadata": {},
   "source": [
    "La primera imagen se recupera como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen = dset[0,:,:]\n",
    "image.shape  # (480, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-shock",
   "metadata": {},
   "source": [
    "Esta imagen  muesra como es almacenado el dataset\n",
    "\n",
    "![dataset_almacenamiento](../Imagenes/dataset_almacenamiento.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-bikini",
   "metadata": {},
   "source": [
    "## Almacenamiento fragmentado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-positive",
   "metadata": {},
   "source": [
    "Pero, ¿qué pasa si, en lugar de procesar imágenes completas una tras otra, nuestra aplicación trata con mosaicos de imagen? \n",
    "\n",
    "Supongamos que queremos leer y procesar los datos en un segmento de 64 × 64 píxeles en la esquina de la primera imagen; por ejemplo, digamos que queremos agregar un logotipo.\n",
    "Nuestra selección de rebanado sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = dset[0,0:64, 0.64]\n",
    "tile.shape # (64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-anaheim",
   "metadata": {},
   "source": [
    "¿Y si hubiera alguna forma de expresar esto de antemano? ¿No hay forma de preservar la forma del conjunto de datos, que es semánticamente importante, pero dígale a HDF5 que optimice la\n",
    "conjunto de datos para el acceso en bloques de 64 × 64 píxeles?\n",
    "\n",
    "\n",
    "Eso es lo que hace la fragmentación en HDF5. Le permite especificar la \"forma\" N-dimensional que\n",
    "se adapta mejor a su patrón de acceso. Cuando llega el momento de escribir datos en el disco, HDF5 se divide\n",
    "los datos en \"trozos\" de la forma especificada, los aplana y los escribe en el disco. Los fragmentos se almacenan en varios lugares del archivo y sus coordenadas están indexadas por un\n",
    "Árbol B (binario).\n",
    "\n",
    "Aquí tenemos un ejemplo. Tomemos el conjunto de datos de forma (100, 480, 640) que se acaba de mostrar y digamos a HDF5\n",
    "que lo almacene en formato fragmentado. \n",
    "\n",
    "Hacemos esto proporcionando una nueva palabra clave, fragmentos, al método *create_dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('fragmentado', (100,480,640), dtype='i1',\n",
    "                       chunks=(1,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-furniture",
   "metadata": {},
   "source": [
    "Así son almacenados los datos en este caso.\n",
    "\n",
    "![fragmentado](../Imagenes/chunckshdf5.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Así es como ocurre la compresión en hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-phenomenon",
   "metadata": {},
   "source": [
    "#### Auto fragmentado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-bibliography",
   "metadata": {},
   "source": [
    "Si no esta seguro sobre como gestionará el dataset, puede dejar que hdf5 decida como fragmentarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('imagenes2', (100,480,640), chunks=True)\n",
    "dset.chunks # (13, 60,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-depth",
   "metadata": {},
   "source": [
    "### Elegir una forma manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-nashville",
   "metadata": {},
   "source": [
    "\n",
    "A continuación, se incluyen algunas cosas que debe tener en cuenta al trabajar con fragmentos. El proceso de elegir\n",
    "la forma de los fragmentos es una compensación entre las siguientes tres restricciones:\n",
    "\n",
    "1. Los fragmentos más grandes para un tamaño de conjunto de datos determinado reducen el tamaño del fragmento del árbol B, lo que hace es más rápido para buscar y cargar fragmentos.\n",
    "2. Dado que los fragmentos son todo o nada (leer una parte carga todo el fragmento), los fragmentos también aumentan la posibilidad de que lea datos en la memoria que no usará.\n",
    "3. La caché de fragmentos HDF5 solo puede contener un número finito de fragmentos. Trozos más grandes\n",
    "de 1 MB ni siquiera compartiran el caché.\n",
    "\n",
    "Entonces, estos son los puntos principales a tener en cuenta:\n",
    "\n",
    "- *¿Necesita realmente especificar un tamaño de fragmento?* Es mejor restringir el tamaño manual de los fragmentos a los casos en los que esté seguro de que su conjunto de datos\n",
    "se accederá de una manera que probablemente sea ineficaz con el almacenamiento contiguo o una forma de trozo adivinada automáticamente. Y como todas las optimizaciones, ¡debería compararlas!\n",
    "\n",
    "- *Intente expresar el patrón de acceso \"natural\" que tendrá su conjunto de datos*. Como en nuestro ejemplo, si está almacenando un montón de imágenes en un conjunto de datos y sabe que\n",
    "su aplicación leerá \"mosaicos\" particulares de 64 × 64, podría usar N × 64 × 64 trozos (o N × 128 × 128) a lo largo de los ejes de la imagen.\n",
    "\n",
    "- *No los hagas demasiado pequeños*. Tenga en cuenta que HDF5 tiene que usar datos de indexación para realizar un seguimiento de las cosas; si utiliza algo patológico como un tamaño de fragmento de 1 byte, la mayor parte de su espacio en disco será\n",
    "tomado por metadatos. Una buena regla general para la mayoría de los conjuntos de datos es mantener fragmentos\n",
    "por encima de 10 KB más o menos.\n",
    "\n",
    "- *No los hagas demasiado grandes*. La clave para recordar es que cuando lee cualquier dato en un fragmento, todo el\n",
    "se lee el fragmento. Si solo usa un subconjunto de los datos, el tiempo extra dedicado a la lectura de el disco está desperdiciado. Tenga en cuenta que los trozos de más de 1 MiB de forma predeterminada no participar en el rápido \"caché de fragmentos\" en memoria y, en su lugar, se leerá desde el disco\n",
    "cada vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-abuse",
   "metadata": {},
   "source": [
    "### Filtros y Compresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-pepper",
   "metadata": {},
   "source": [
    "Con la fragmentación, es posible realizar la compresión de forma transparente en un conjunto de datos.\n",
    "\n",
    "Se conoce el tamaño inicial de cada fragmento y, dado que están indexados por un árbol B, pueden\n",
    "almacenarse en cualquier lugar del archivo, no solo uno tras otro. En otras palabras, cada trozo es\n",
    "libre para crecer o encogerse sin golpear a los demás"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-airplane",
   "metadata": {},
   "source": [
    "####  Tubería de filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-allah",
   "metadata": {},
   "source": [
    "HDF5 tiene el concepto de una tubería de filtro, que es solo una serie de operaciones realizadas\n",
    "en cada fragmento cuando está escrito. Cada filtro es libre de hacer lo que quiera con los datos en\n",
    "el fragmento: comprimirlo, sumarlo, agregar metadatos, cualquier cosa. Cuando se lee el archivo, \n",
    "el filtro se ejecuta en modo \"inverso\" para reconstruir los datos originales.\n",
    "\n",
    "La imagen muestra como sucede la acción\n",
    "\n",
    "\n",
    "![Tuberia_filtro](../Imagenes/tuberia_filtros.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-calculator",
   "metadata": {},
   "source": [
    "#### Filtros de compresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-marriage",
   "metadata": {},
   "source": [
    "Hay varios filtros de compresión disponibles en HDF5. Con mucho, el más utilizado es el filtro GZIP. (También escuchará que se hace referencia a esto como el filtro \"DEFLATED\"; en el mundo HDF5, ambos nombres se utilizan para el mismo filtro).\n",
    "\n",
    "A continuación, se muestra un ejemplo de compresión GZIP utilizada en un conjunto de datos de punto flotante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset =  f.create_dataset('BigDataset', (1000,1000),dtype='f', \n",
    "                         compression='gzip' )\n",
    "dset.compression # 'gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-wilderness",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Grupos, enlaces e interación</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-roads",
   "metadata": {},
   "source": [
    "Los grupos son el objeto contenedor HDF5, análogo a las carpetas de un sistema de archivos. Ellos pueden\n",
    "contener conjuntos de datos y otros grupos, lo que le permite construir una estructura jerárquica con\n",
    "objetos perfectamente organizados en grupos y subgrupos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-appreciation",
   "metadata": {},
   "source": [
    "#### El grupo raiz y subgrupos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-melissa",
   "metadata": {},
   "source": [
    "El objeto de grupo más general es h5py.Group, del cual h5py.File es una subclase. Otro Los grupos se crean fácilmente con el método *create_group*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baking-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/SubGroup\" (0 members)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import h5py\n",
    "f = h5py.File('../Datos/Groups.hdf5','w')\n",
    "subgroup = f.create_group('SubGroup')\n",
    "subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "utility-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SubGroup'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroup.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-issue",
   "metadata": {},
   "source": [
    "Por supuesto, los grupos también se pueden anidar. El método *create_group* existe en todos los grupos objetos, no solo File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SubGroup/AnotherGroup'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsubgroup = subgroup.create_group('AnotherGroup')\n",
    "subsubgroup.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-couple",
   "metadata": {},
   "source": [
    "No es necesario crear manualmente todos los subgrupos anidados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ahead-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/some/big/path\" (0 members)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = f.create_group('/some/big/path')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increased-construction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['SubGroup', 'some']>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-liberal",
   "metadata": {},
   "source": [
    "### Elementos escenciales de Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-obligation",
   "metadata": {},
   "source": [
    "Si no recuerda nada más de esta sección, recuerde esto: los grupos funcionan principalmente como diccionarios. \n",
    "\n",
    "Hay un par de agujeros en esta abstracción, pero en general funciona sorprendentemente bien. Los grupos son iterables y tienen un subconjunto del diccionario Python normal\n",
    "API.\n",
    "\n",
    "Agregemos unos pocos objetos al archivo de este ejemplo para lo que sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imposed-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Dataset1'] = 1.0\n",
    "f['Dataset2'] = 2.0\n",
    "f['Dataset3'] = 3.0\n",
    "subgroup['Dataset4'] = 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-workshop",
   "metadata": {},
   "source": [
    "#### Acceso al estilo de diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "answering-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accesa el dataset asociado a la 'clave' dataset1\n",
    "dset1 = f['Dataset1']\n",
    "\n",
    "dset4 = f['SubGroup/Dataset4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reasonable-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n",
      "<KeysViewHDF5 ['Dataset1', 'Dataset2', 'Dataset3', 'SubGroup', 'some']>\n"
     ]
    }
   ],
   "source": [
    "print(len(f))\n",
    "print(len(f['SubGroup']))\n",
    "print(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-guidance",
   "metadata": {},
   "source": [
    "#### Propiedades especiales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-victory",
   "metadata": {},
   "source": [
    "\n",
    "- Propiedad *.file*. Regresa un objeto de tipo File que apunta al archivo en el que reside su objeto.\n",
    "- Propiedad *.parent*. Retorna el objeto Group que contiene su objeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extreme-seattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<HDF5 group \"/\" (1 members)>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('../Datos/propdemo.hdf5', 'w')\n",
    "grp = f.create_group('hola')\n",
    "print(grp.file == f)\n",
    "print(grp.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-airfare",
   "metadata": {},
   "source": [
    "### Trabajando con enlaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-tribute",
   "metadata": {},
   "source": [
    "Existe una capa entre el objeto  Group y sus objetos miembros. Los dos están relacionados por el concepto de enlaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-estate",
   "metadata": {},
   "source": [
    "#### Enlaces duros (hard links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-asian",
   "metadata": {},
   "source": [
    "Los enlaces en HDF5 se manejan de la misma manera que en los sistemas de archivos modernos. Objetos\n",
    "como los datasets y los grupos no tienen un nombre intrínseco; más bien, tienen una dirección (byte\n",
    "offset) en el archivo que HDF5 tiene que buscar. Cuando asigna un objeto a un nombre en un grupo esa dirección se registra en el grupo y se asocia con el nombre que proporcionó\n",
    "para formar un enlace.\n",
    "\n",
    "Entre otras cosas, esto significa que los objetos de un archivo HDF5 pueden tener más de un nombre; de hecho, tienen tantos nombres como enlaces existan apuntando a ellos. El número\n",
    "de enlaces que apuntan a un objeto se registra, y cuando no existen más enlaces, el espacio utilizado es liberado porque el objeto es liberado.\n",
    "\n",
    "Este tipo de vínculo, el predeterminado en HDF5, se denomina vínculo duro para diferenciarlo de otros tipos de enlaces.\n",
    "\n",
    "Veámos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "senior-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/x\n",
      "True\n",
      "/y\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('linksdemo.hdf5', 'w')\n",
    "grpx = f.create_group('x')\n",
    "print(grpx.name)\n",
    "\n",
    "f['y'] = grpx\n",
    "\n",
    "grpy = f['y']\n",
    "print(grpy == grpx)\n",
    "print(grpy.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-emphasis",
   "metadata": {},
   "source": [
    "Para remover enlaces se usa la sintaxis se diccionarios *del*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "swiss-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['x', 'y']>\n",
      "<KeysViewHDF5 ['x']>\n",
      "<KeysViewHDF5 []>\n"
     ]
    }
   ],
   "source": [
    "print(f.keys())\n",
    "del f['y'] # elimina el enlace 'y'\n",
    "print(f.keys())\n",
    "del f['x'] # elimina el enlace 'x'\n",
    "# como no hay mas enlces el grupe es eliminado\n",
    "print(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5571d6-e053-4790-acbc-1a7061d22399",
   "metadata": {},
   "source": [
    "### Espacio libre y reempacado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc4c30-e2f8-4798-a574-fab0a293870b",
   "metadata": {},
   "source": [
    "Cuando se elimina un objeto (por ejemplo, un conjunto de datos grande), el espacio que ocupaba en el disco\n",
    "se reutiliza para nuevos objetos como grupos y conjuntos de datos. Sin embargo, en el momento de escribir, HDF5\n",
    "no realiza un seguimiento de ese \"espacio libre\" en los ciclos de apertura/cierre de archivos. \n",
    "\n",
    "Entonces, si no terminas reutilizando el espacio en el momento en que cierre el archivo, puede terminar con un \"agujero\" de inutilizable espacio en el archivo que no se puede recuperar.\n",
    "\n",
    "\n",
    "Este tema es una alta prioridad de desarrollo para el Grupo HDF. Mientras tanto, si los archivos parecen inusualmente grandes, puede \"volver a empaquetarlos\" con la herramienta `h5repack`, de HDF5. En el sistema operativo vaya a la carpeta en donde está el archivo  y ejecute por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea81c8-2b46-4e2e-97d7-bae9617154f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5repack bigfile.hdf5 out.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38726b-72b9-4410-9b18-e351134b0f13",
   "metadata": {},
   "source": [
    "### Enlaces blandos (soft links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab917048-3ae8-40bb-9903-b1e9a3d5cf08",
   "metadata": {},
   "source": [
    "A diferencia de los enlaces *duros*, que asocian un nombre de enlace con un objeto particular en el archivo, los enlaces blandos en su lugar, almacena la ruta a un objeto.\n",
    "\n",
    "Si tuviéramos que crear un enlace duro en el grupo raíz al conjunto de datos, siempre apuntaría\n",
    "a ese objeto en particular, incluso si el conjunto de datos es movido o desvinculado del grupo. Aquí hay un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "893fd509-078a-4e92-b781-8dbd80ea6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f = h5py.File('../Datos/test.hdf5', 'w')\n",
    "grp = f.create_group('mygroup')\n",
    "dset = grp.create_dataset('dataset', (100,))\n",
    "\n",
    "f['hardlink'] = dset\n",
    "print(f['hardlink'] == grp['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18b5f3-b7ef-4094-9189-199706d58f3e",
   "metadata": {},
   "source": [
    "Movamos hacia atrás el conjunto de datos y luego creemos un vínculo blando que apunte a la ruta */mygroup/dataset*. Para decirle a HDF5 que queremos crear un enlace suave, asigne un\n",
    "instancia de la clase *h5py.SoftLink* a un nombre en el archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2241f2e-b338-467f-9bef-3fcc967ad940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "grp.move('dataset', 'new_dataset_name')\n",
    "print(f['hardlink'] == grp['new_dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6546bd-da48-4970-99f8-b01e228d5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "grp.move('new_dataset_name', 'dataset')\n",
    "f['softlink'] = h5py.SoftLink('/mygroup/dataset')\n",
    "print(f['softlink'] == grp['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7782403-18a7-4572-b09a-eae9ed4053e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c16e2f-0805-4de1-8c52-65440b1c13c7",
   "metadata": {},
   "source": [
    "Los objetos *SoftLink* son muy simples; solo tienen una propiedad, .path, que contiene la ruta\n",
    "siempre que se creen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01a3247-8684-4143-aa0f-0da53b83ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SoftLink to \"../Datos/path\">\n",
      "../Datos/path\n"
     ]
    }
   ],
   "source": [
    "softlink = h5py.SoftLink('../Datos/path')\n",
    "print(softlink)\n",
    "print(softlink.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc81836-53c6-4e3d-b0ed-7e72d5104bc1",
   "metadata": {},
   "source": [
    "### Usar get para determinar tipos de objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-shooting",
   "metadata": {},
   "source": [
    "Esta versión de *get* es un poco más capaz que la de Python.\n",
    "Hay dos palabras clave adicionales además del estilo de diccionario predeterminado: `getclass` y getlink`. \n",
    "\n",
    "La palabra clave `getclass` le permite recuperar el tipo de un objeto sin\n",
    "realmente tener que abrirlo. \n",
    "\n",
    "En el nivel HDF5, esto solo requiere leer algunos metadatos y por lo tanto es muy rápido.\n",
    "Aquí hay un ejemplo: primero crearemos un archivo que contenga un solo grupo y un solo conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "discrete-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset\": shape (100,), type \"<f4\">"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('../Datos/get_demo.hdf5', 'w')\n",
    "f.create_group('subgroup')\n",
    "f.create_dataset('dataset', (100,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stuck-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset <class 'h5py._hl.dataset.Dataset'>\n",
      "subgroup <class 'h5py._hl.group.Group'>\n"
     ]
    }
   ],
   "source": [
    "for name in f:\n",
    "    print(name, f.get(name, getclass=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0aa6a-be1b-49b2-8ea9-a6e8252f8280",
   "metadata": {},
   "source": [
    "### Uso de require para simplificar su aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd0d99-3a64-4d58-b570-b7d9d88bdc30",
   "metadata": {},
   "source": [
    "A diferencia de los diccionarios de Python, no puede sobrescribir directamente a los miembros de un grupo:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975aa9c-07ad-4f1d-af02-be5373ae2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../Datos/require_demo.hdf5','w')\n",
    "f.create_group('x')\n",
    "f.create_group('y')\n",
    "f.create_group('y')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3934358-0bb6-40b7-9470-237fff27f6e0",
   "metadata": {},
   "source": [
    "ValueError: unable to create group (Symbol table: Unable to initialize object)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6bee98c-d861-4ab1-8ef0-0d854a0465c6",
   "metadata": {},
   "source": [
    "Esto también es válido para los objetos de vinculación rígida manual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a6b45-aad1-424f-9f2f-70bccf9be7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['y'] = f['x']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19e161e7-c655-4077-99b6-70c3494973e6",
   "metadata": {},
   "source": [
    "OSError: Unable to create link (name already exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f4a51-7d6a-483b-ab64-bec8bdfe4968",
   "metadata": {},
   "source": [
    "Esta es una función intencional diseñada para evitar la pérdida de datos. Dado que los objetos son inmediatamente\n",
    "eliminado cuando los desvincula de un grupo, debe eliminar explícitamente el enlace en lugar de\n",
    "que tener HDF5 lo haga por usted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7641e0-4824-489a-b896-911776768f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "del f['y']\n",
    "f['y'] = f['x']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8514b9-bb68-4ea9-b2b8-1b0ae6a37fd0",
   "metadata": {},
   "source": [
    "Esto conduce a algunos dolores de cabeza en el código del mundo real. Por ejemplo, un fragmento de análisis\n",
    "El código puede crear un archivo y escribir los resultados en un conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c4d17-07b1-49d6-a7e0-a5b3fbc747dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = do_large_calculation()\n",
    "with h5py.File('output.hdf5') as f:\n",
    "...\n",
    "f.create_dataset('results', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19214ae3-fb4a-4414-9f60-42925765a29d",
   "metadata": {},
   "source": [
    "Si hay muchos conjuntos de datos y grupos en el archivo, es posible que no sea apropiado sobrescribir\n",
    "el archivo completo cada vez que se ejecuta el código. Pero si no abrimos en modo *w*, nuestro programa\n",
    "sólo funcionará la primera vez, a menos que eliminemos manualmente el archivo de salida cada vez que corre el programa\n",
    "\n",
    "Para lidiar con esto, create_group y create_dataset tienen métodos complementarios llamados\n",
    "*require_group* y *require_dataset*. Hacen exactamente lo mismo, solo que primero\n",
    "busque un grupo o conjunto de datos existente y lo  devuleven en su lugar.\n",
    "\n",
    "Ambas versiones toman exactamente los mismos argumentos y palabras clave. En el caso de *require_dataset*, h5py también verifica la forma solicitada y el tipo dtype con cualquier conjunto de datos existente y\n",
    "falla si no coinciden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ac6d1-8f18-4502-8861-07e8643aeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_dataset('dataset', (100,), dtype='i')\n",
    "f.require_dataset('dataset', (100,), dtype='f')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25fd549d-25d6-447f-9bb6-37818085ae56",
   "metadata": {},
   "source": [
    "TypeError: Datatypes cannot be safely cast (existing int32 vs new f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6963778-b6a5-4c5b-8eff-62b9e9777b47",
   "metadata": {},
   "source": [
    "Hay un detalle menor aquí, en el sentido de que un conflicto solo se considera que ocurre si las formas no\n",
    "coincidencia, o la precisión solicitada del tipo de datos es mayor que la precisión existente.\n",
    "Entonces, si hay un conjunto de datos int64 preexistente, *require_dataset* tendrá éxito si int32 es\n",
    "solicitado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06510441-35a0-462a-937b-d87db7f1fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_dataset('int_dataset', (100,), dtype='int64')\n",
    "f.require_dataset('int_dataset', (100,), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e2c46-523e-409e-bbf9-e851cf88fbe7",
   "metadata": {},
   "source": [
    "Las reglas de conversión de NumPy se utilizan para comprobar si hay conflictos; puedes probar los tipos tú mismo\n",
    "utilizando *np.can_cast*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207e7aa-0730-465a-b701-7064ef97c495",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Iteración y contenedor</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603daf0-9a6c-4ae4-8052-0dc69541a3c7",
   "metadata": {},
   "source": [
    "### Cómo se almacenan realmente los grupos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4d464-4a6e-46e4-83f9-76a421c088b4",
   "metadata": {},
   "source": [
    "En el archivo HDF5, los miembros del grupo se indexan mediante una estructura denominada \"árbol B\". \n",
    "\n",
    "Los \"árboles B\" son estructuras de datos excelentes para realizar un seguimiento de una gran cantidad de elementos,\n",
    "al mismo tiempo que agiliza la recuperación (y la adición) de elementos. Funcionan tomando una colección de elementos, cada uno de los cuales se puede ordenar de acuerdo con algún esquema como un nombre de cadena o\n",
    "identificador numérico y la creación de un \"índice\" en forma de árbol para recuperar rápidamente un elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c195a16f-9600-4f49-a768-c0e243cf5a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['1', '10', '2', 'data']>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('iterationdemo.hdf5', 'w')\n",
    "f.create_group('1')\n",
    "f.create_group('2')\n",
    "f.create_group('10')\n",
    "f.create_dataset('data', (100,))\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962dccd-ae0e-4aac-a5e8-eac6785e994b",
   "metadata": {},
   "source": [
    "### Iteración estilo diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "681fbf54-7c45-423d-9a2d-a00b6f06dab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '10', '2', 'data']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd810f4d-ad63-4121-ac78-ae0fc7ad3c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', <HDF5 group \"/1\" (0 members)>),\n",
       " ('10', <HDF5 group \"/10\" (0 members)>),\n",
       " ('2', <HDF5 group \"/2\" (0 members)>),\n",
       " ('data', <HDF5 dataset \"data\": shape (100,), type \"<f4\">)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,y) for (x,y) in f.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c197cc2e-655b-40ff-a664-f0512c9dbf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<HDF5 group \"/1\" (0 members)>,\n",
       " <HDF5 group \"/10\" (0 members)>,\n",
       " <HDF5 group \"/2\" (0 members)>,\n",
       " <HDF5 dataset \"data\": shape (100,), type \"<f4\">]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y for y in f.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7db7c0-e94f-4002-95cb-d72aeaf58393",
   "metadata": {},
   "source": [
    "### Tests de contenecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86d2e66c-4f97-45a9-b64d-145dc6d5ba5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1' in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93014b67-a81d-4c94-8703-5fcaf8230e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp = f['1']\n",
    "path = '/1'\n",
    "path in grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9e6c16e-84af-4b35-a4f9-7f4368807b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890cc9c-9b90-4e66-8791-c5637e10e719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
