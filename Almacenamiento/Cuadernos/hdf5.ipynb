{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "useful-trading",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Big Data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-district",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>HDF5<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-croatia",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-maryland",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "1. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-indie",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-baptist",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-humanitarian",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-porcelain",
   "metadata": {},
   "source": [
    "1. Oleg Jarma, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-travel",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-webster",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-panama",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-ocean",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-arena",
   "metadata": {},
   "source": [
    "### Importar modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "charged-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define carpeta de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stopped-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../Datos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-fashion",
   "metadata": {},
   "source": [
    "### Organización de datos y metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-specialist",
   "metadata": {},
   "source": [
    "### Genera datos simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "institutional-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibilidad\n",
    "np.random.seed(100)\n",
    "\n",
    "# Temperatura\n",
    "temperatura_15 = np.random.random(1024) # tempraturas est. 10\n",
    "temperatura_10 = np.random.random(1024)\n",
    "\n",
    "# Viento\n",
    "viento_15 = np.random.random(2048)\n",
    "viento_10 = np.random.random(2048)\n",
    "\n",
    "estacion_15 = 15  # estación 15\n",
    "estacion_10 = 10\n",
    "\n",
    "dt_temp = 10 # delta-T temperatura\n",
    "dt_viento = 20\n",
    "\n",
    "start_time_temp_15 = 1375204299 # en Unix time\n",
    "start_time_temp_10 = 1375204500 # en Unix time\n",
    "start_time_viento_15 = 137550000\n",
    "start_time_viento_10 = 137550000\n",
    "\n",
    "file_name = \"clima.hdf5\" # archivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "compound-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54340494 0.27836939 0.42451759 ... 0.24908888 0.27119505 0.05880634]\n"
     ]
    }
   ],
   "source": [
    "print(temperatura_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-gender",
   "metadata": {},
   "source": [
    "### Vamos a crear la siguiente estructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-marine",
   "metadata": {},
   "source": [
    "* Archivo : clima.hdf5\n",
    "* Estación (15 para el ejemplo): /15/\n",
    "* Temperatura: /15/temperatura\n",
    "* Viento: /15/viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cleared-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datos/clima.hdf5\n",
      "<HDF5 file \"clima.hdf5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(folder, file_name)\n",
    "\n",
    "f = h5py.File(file, mode='w') \n",
    "print(file)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "literary-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "f['/15/temperatura'] = temperatura_15\n",
    "f['/15/temperatura'].attrs[\"dt\"] = dt_temp\n",
    "f['/15/temperatura'].attrs['star_time'] = start_time_temp_15\n",
    "\n",
    "f['/15/viento'] = viento_15\n",
    "f['/15/viento'].attrs[\"dt\"] = dt_viento\n",
    "f['/15/viento'].attrs['star_time'] = start_time_viento_15\n",
    "\n",
    "\n",
    "\n",
    "f['/10/temperatura'] = temperatura_10\n",
    "f['/10/temperatura'].attrs[\"dt\"] = dt_temp\n",
    "f['/10/temperatura'].attrs['star_time'] = start_time_temp_10\n",
    "\n",
    "f['/10/viento'] = viento_10\n",
    "f['/10/viento'].attrs[\"dt\"] = dt_viento\n",
    "f['/10/viento'].attrs['star_time'] = start_time_viento_10\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-pointer",
   "metadata": {},
   "source": [
    "### Recupera datos del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "olive-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(folder, file_name)\n",
    "f = h5py.File(file, mode='r') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dated-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['10', '15']>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "secure-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_15 = f['/15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "closed-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/15\" (2 members)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "annual-prague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['temperatura', 'viento']>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_15.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "catholic-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: 10\n",
      "star_time: 1375204299\n"
     ]
    }
   ],
   "source": [
    "dataset = f['/15/temperatura']\n",
    "\n",
    "for key, value in dataset.attrs.items():\n",
    "    print('%s: %s' %  (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-intention",
   "metadata": {},
   "source": [
    "### Recorte (slicing) de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "subsequent-collectible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54340494, 0.27836939, 0.42451759, 0.84477613, 0.00471886,\n",
       "       0.12156912, 0.67074908, 0.82585276, 0.13670659, 0.57509333])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "signed-radical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "authorized-heath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4948712717532915"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cellular-system",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-festival",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Primeros pasos con archivos HDF5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-prevention",
   "metadata": {},
   "source": [
    "HDF5 utiliza un sistema de tipos muy similar a Numpy. Cada `matriz` o conjunto de datos en un archivo HDF5 tiene un\n",
    "tipo fijo representado por un objeto de tipo. El paquete `h5py` mapea automáticamente el HDF5\n",
    "type system en NumPy dtypes, lo que, entre otras cosas, facilita el intercambio\n",
    "datos con NumPy.\n",
    "\n",
    "Por ejemplo, HDF5, tomar prestada esta sintaxis de \"corte\" para permitir cargar solo porciones de un conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-client",
   "metadata": {},
   "source": [
    "### Mi primer archivo hdf5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"a\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-spank",
   "metadata": {},
   "source": [
    "### Modos de abrir una archivo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mechanical-wheat",
   "metadata": {},
   "source": [
    "f = h5py.File(\"../data/nombre.hdf5\",\"w\") # Nuevo archivo. Si existe lo reescribe\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"r\") # Abre solo lectura. Debe existir\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"r+\") # Abre lectura y escritura. Debe existir\n",
    "f = h5py.File(\"../data/nombre.hdf5\",\"a\") # Abre lectura y escritura. Si no existe, lo crea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../data/nombre.hdf5\",\"w-\") # Nuevo archivo, solo si no existe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-letter",
   "metadata": {},
   "source": [
    "### Usando un manejador de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-reconstruction",
   "metadata": {},
   "source": [
    "El  ejemplo típico de manejo de contexto con archivos es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "postal-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/garbage.txt\",\"w\") as f:\n",
    "    f.write(\"Hello!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-friend",
   "metadata": {},
   "source": [
    "Cuando se sale del contexto, el archvo se cierra automáticamente.\n",
    "\n",
    "Podemos hacer lo mismo con archivos hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/nombre.hdf5\",\"w\") as f:\n",
    "    print(f[\"dataset_perdido\"]) # error dataset no existe en el archivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys()) # error. Archivo cerrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upset-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Closed HDF5 file>\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressive-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.files.File"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-denver",
   "metadata": {},
   "source": [
    "### Manejadores de archivos (file drivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-maryland",
   "metadata": {},
   "source": [
    "Los controladores de archivos se encuentran entre el sistema de archivos y el mundo de alto nivel de los grupos HDF5, datasets y atributos. Se ocupan de la mecánica del *manejo de espacio en memoria* de los   archivos HDF5 disponibles en el disco.\n",
    "\n",
    "Por lo general, no tendrá que preocuparse por qué controlador está en uso, ya que el controlador predeterminado funciona bien para la mayoría de las aplicaciones.\n",
    "\n",
    "Lo mejor de los controladores es que una vez que se abre el archivo, son totalmente transparentes.\n",
    "Simplemente use la biblioteca HDF5 como de costumbre, y el controlador se encarga de la mecánica de almacenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-ideal",
   "metadata": {},
   "source": [
    "#### Manejador (driver) core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-basement",
   "metadata": {},
   "source": [
    "El controlador `core` almacena su archivo completamente en la memoria. Obviamente, hay un límite en cuanto a cómo\n",
    "gran cantidad de datos que puede almacenar, pero la compensación es lecturas y escrituras increíblemente rápidas. Es un gran elección cuando desea la velocidad de acceso a la memoria, pero también desea utilizar el HDF5\n",
    "estructuras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/nombre.hdf5', driver='core')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-adapter",
   "metadata": {},
   "source": [
    "Para decirle a HDGF5 que cree un archivo  que guarde la imagen actual del archivo cundo se cierre, puede usar `backing store`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/nombre.hdf5', driver='core', backing_store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-proposal",
   "metadata": {},
   "source": [
    "#### Manejador family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-switzerland",
   "metadata": {},
   "source": [
    "Permite separar un archivo en múltiple imágenes, cada de la cuales comparte cierto tamaño máximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide el archivo en trozos de 1Gb de memoria\n",
    "f = h5py.File('../data/family.hdf5', memb_size=1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-prince",
   "metadata": {},
   "source": [
    "El tamaño por defecto es memb_size = $2^{31}-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-freeze",
   "metadata": {},
   "source": [
    "#### Manejador mpio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-registration",
   "metadata": {},
   "source": [
    "Este controlador es el corazón de Parallel HDF5. Le permite acceder al mismo archivo desde múltiples\n",
    "procesos al mismo tiempo. Puede tener docenas o incluso cientos de procesos de computación paralela, todos los cuales comparten una vista coherente de un solo archivo en el disco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-institution",
   "metadata": {},
   "source": [
    "### El bloque del usuario (user block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-asthma",
   "metadata": {},
   "source": [
    "Una característica interesante de HDF5 es que los archivos pueden estar precedidos por datos de usuario arbitrarios.\n",
    "Cuando se abre un archivo, la biblioteca busca el encabezado HDF5 al comienzo del\n",
    "archivo, luego 512 bytes en, luego 1024, y así sucesivamente en potencias de 2. \n",
    "\n",
    "Dicho espacio al principio del archivo se denomina `bloque de usuario` y allí el usuario puede almacenar los datos que desee.\n",
    "Las únicas restricciones están en el tamaño del bloque (potencias de 2 y al menos 512), y que\n",
    "no debería tener el archivo abierto en HDF5 al escribir en el bloque de usuario. Aquí hay un\n",
    "ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adolescent-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('../data/userblock_example.hdf5', 'w', userblock_size=512)\n",
    "print(f.userblock_size)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "extensive-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/userblock_example.hdf5', 'rb+') as f:\n",
    "#    f.write('a'*512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-serial",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Datasets HDF5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-incidence",
   "metadata": {},
   "source": [
    "Los conjuntos de datos (`datasets`) son la característica central de HDF5. Puede pensar en ellos como arreglos (arrays) NumPy que viven en disco. Cada conjunto de datos en HDF5 tiene un nombre, un tipo y una forma, y admite datos aleatorios.\n",
    "acceso. \n",
    "\n",
    "A diferencia de np.save y friends integrados, no es necesario leer ni escribir el arreglo  completo como un bloque; puede usar la sintaxis estándar de NumPy para cortar, leer o\n",
    "escribir solo las partes que desee del array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-linux",
   "metadata": {},
   "source": [
    "### Elemento escenciales sobre datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-amateur",
   "metadata": {},
   "source": [
    "Primero, creemos un archivo para que tengamos un lugar donde almacenar nuestros conjuntos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secondary-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/testfile.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-admission",
   "metadata": {},
   "source": [
    "Cada conjunto de datos de un archivo HDF5 tiene un nombre. Veamos qué sucede si asignamos un nuevo\n",
    "array NumPy a un nombre en el archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "national-guide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"my_dataset\": shape (5, 2), type \"<f8\">"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.ones((5,2))\n",
    "f['my_dataset'] = arr\n",
    "dset = f['my_dataset'] \n",
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-species",
   "metadata": {},
   "source": [
    "Observe que *dset* es una instancia de la clase *h5py-Dataset*. Este objeto se accesa como cualquier archivo Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "basic-crash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-corner",
   "metadata": {},
   "source": [
    "### Type y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "provincial-skiing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(5, 2)\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dset.dtype)\n",
    "print(dset.shape)\n",
    "print(dset[:, -1])\n",
    "out = dset[...]\n",
    "print(type(out))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-northwest",
   "metadata": {},
   "source": [
    "### Creación de datasets vacíos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-diamond",
   "metadata": {},
   "source": [
    "No es necesario tener una matriz NumPy lista para crear un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "turkish-youth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"test1\": shape (10, 10), type \"<f4\">"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = f.create_dataset('../data/test1', (10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-vocabulary",
   "metadata": {},
   "source": [
    "HDF5 es lo suficientemente inteligente como para asignar solo la cantidad de espacio en el disco que realmente necesita\n",
    "almacenar los datos que escribe. A continuación, se muestra un ejemplo: suponga que desea crear un conjunto de datos 1D\n",
    "que puede contener muestras de datos de 4 gigabytes de un experimento de larga duración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('../data/big_dataset', (1024**3), dtype= np.float32)\n",
    "dset[0:1024] = np.arange(1024)\n",
    "f.flush()\n",
    "#f.close()\n",
    "\n",
    "# Por favor revise el tamaño del archivo en el sistema operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -lh testfile.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-republic",
   "metadata": {},
   "source": [
    "Crea array Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "square-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "bigdata = np.ones((100,1000))\n",
    "print(bigdata.dtype)\n",
    "print(bigdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-lyric",
   "metadata": {},
   "source": [
    "Guarda con el tipo de dato original: np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patient-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../Datos/big1.hdf5','w') as f1:\n",
    "    f1['big'] = bigdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-albuquerque",
   "metadata": {},
   "source": [
    "Cambia tipos de dato al enviar al archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olympic-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../Datos/big2.hdf5', 'w') as f2:\n",
    "    f2.create_dataset('big', data=bigdata, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recupera los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "friendly-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "f1 = h5py.File('../Datos/big1.hdf5')\n",
    "f2 = h5py.File('../Datos/big2.hdf5')\n",
    "print(f1['big'].dtype)\n",
    "print(f2['big'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-maple",
   "metadata": {},
   "source": [
    "Conversión automática de tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-response",
   "metadata": {},
   "source": [
    "La propia biblioteca HDF5 maneja la conversión de tipos y lo hace sobre la marcha cuando se guarda en o se lee de un archivo.\n",
    "\n",
    "Vemos como sucede esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "russian-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "dset = f2['big']\n",
    "print(dset.dtype)\n",
    "print(dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-intervention",
   "metadata": {},
   "source": [
    "En este momento *dset* apunta a os datos en el archivo, pero no han sido cargados en memoria. Para cargarlos con 'float64' lo que debe hacer asignar primero un arreglo vacío de doble precisión en la memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "economic-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_out = np.empty((100, 1000), dtype= np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-topic",
   "metadata": {},
   "source": [
    "Ahora cargamos los adots a la meoria en ese espacio asignao en memoria. La API de HDF5 hace la conversón por el camino (*on the fly*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "national-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "dset.read_direct(big_out)\n",
    "print(dset.dtype)\n",
    "print(big_out.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-chemical",
   "metadata": {},
   "source": [
    "*dset* sigue apuntando al archivo, mientras que big_out apunta a los datos (convertidos) en memoria.\n",
    "\n",
    "En realidad con *read_direct* no necesariamente tiene que leer todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-wilderness",
   "metadata": {},
   "source": [
    "#### Lectura con astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "burning-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "with dset.astype('float64'):\n",
    "    out = dset[0,:]\n",
    "\n",
    "print(out.dtype)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-renewal",
   "metadata": {},
   "source": [
    "### Lectura y escritura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-rotation",
   "metadata": {},
   "source": [
    "En esta seccion estudiamos inportantes asuntos de implementación  que mejoran el desempeño de los programas con hdf5. Pruiemreo estudiamos algunoas diferencias entre lis arreglos de Numpy y los datasets de hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-professor",
   "metadata": {},
   "source": [
    "#### Rebanado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-murder",
   "metadata": {},
   "source": [
    "Empzamos leyendo de un dataset existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "armed-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['big']>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "f2 = h5py.File('../Datos/big1.hdf5','r+')\n",
    "print(f2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-chorus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"big\": shape (100, 1000), type \"<f8\">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = f2['big']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moving-assembly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = dset[0:10, 20:70]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-dream",
   "metadata": {},
   "source": [
    "#### ¿Que ocurrió?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-coverage",
   "metadata": {},
   "source": [
    "1. h5py calcula la forma (10, 50) del array resultante.\n",
    "2. A un array  NumPy vacío se le asigna la forma (10, 50).\n",
    "3. HDF5 selecciona la parte apropiada del conjunto de datos.\n",
    "4. HDF5 copia datos del conjunto de datos en el arreglo NumPy vacía.\n",
    "5. Se devuelve el arreglo NumPy recién llenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-accountability",
   "metadata": {},
   "source": [
    "#### Tip 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-barcelona",
   "metadata": {},
   "source": [
    "Tomar rebabadas de tamaño razonable. Revise los dos siguientes snippets de código. ¿Cuál es más eficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acquired-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequear valiores negativos y reemplazarlos por cero (0).\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "grateful-banana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "f2 = h5py.File('../Datos/big1.hdf5','r+')\n",
    "\n",
    "for ix in range(100):\n",
    "    for iy in range(1000):\n",
    "        val = dset[ix,iy]     # lee elemento\n",
    "        if val < 0: \n",
    "            dset[ix,iy] = 0   # recorta a 0 si es negativo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequear valiores negativos y reemplazarlos por cero (0).\n",
    "\n",
    "%time\n",
    "for ix in range(100):\n",
    "    val = dset[ix,:] # lee una fila\n",
    "    val[val<0] = 0    # recorta si es negativo\n",
    "    dset[ix,:] = val  # escribe de regreso en el dataset hdf5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-religious",
   "metadata": {},
   "source": [
    "#### Comparación codn dask- array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "durable-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "massive-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 400.00 kB </td> <td> 400.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (100, 1000) </td> <td> (100, 1000) </td></tr>\n",
       "    <tr><th> Count </th><td> 2 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"88\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"120\" y2=\"38\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"38\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"38\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,38.596863 0.000000,38.596863\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"58.596863\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1000</text>\n",
       "  <text x=\"140.000000\" y=\"19.298432\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,19.298432)\">100</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(100, 1000), dtype=float32, chunksize=(100, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.from_array(dset, chunks=(1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 12.4 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "x[x<0] = 0\n",
    "dset[:] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-vanilla",
   "metadata": {},
   "source": [
    "#### Indexación de datasets hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "previous-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "f = h5py.File('../Datos/example1.hdf5','w')\n",
    "dset = f.create_dataset('range', data=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "buried-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[4 5 6 7]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[4 5 6 7 8]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Step must be >= 1 (got -1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-edba22ca2b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# todo en una dimensión\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# notación del último elemento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Fallo. Recorrer en modo reverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dask-tutorial/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;31m# Perform the dataspace selection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnselect\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dask-tutorial/lib/python3.8/site-packages/h5py/_hl/selections.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(shape, args, dataset)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_selector.pyx\u001b[0m in \u001b[0;36mh5py._selector.Selector.make_selection\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_selector.pyx\u001b[0m in \u001b[0;36mh5py._selector.Selector.apply_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Step must be >= 1 (got -1)"
     ]
    }
   ],
   "source": [
    "print(dset[4])   # un elemento\n",
    "print(dset[4:8]) # rebanado\n",
    "print(dset[...]) # notación puntos suspensivos (ellipsis) todo\n",
    "print(dset[:])   # todo en una dimensión\n",
    "print(dset[4:-1]) # notación del último elemento\n",
    "print(dset[::-1])   # Fallo. Recorrer en modo reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-current",
   "metadata": {},
   "source": [
    "#### Rebanado multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nominated-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 80, 50, 20)\n",
      "(100, 80, 50, 20)\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset('4d', shape=(100,80,50,20))\n",
    "print(dset.shape)\n",
    "print(dset[...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "employed-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "(1,)\n",
      "(1,)\n",
      "[42]\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset('1d', shape=(1,), data = 42)\n",
    "\n",
    "print(dset[0])  \n",
    "print(dset.shape)\n",
    "print(dset[...].shape) \n",
    "print(dset[:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-korea",
   "metadata": {},
   "source": [
    "#### Indexación boolena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-allen",
   "metadata": {},
   "source": [
    "Como en Numpy. se pueden usar máscara con valores boolenos para extraer  elementos de un dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "familiar-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62873253,  0.26622238, -0.80604774, -0.50047776, -0.63580955,\n",
       "        0.31386122,  0.70394949, -0.91514766,  0.30259824,  0.17020042])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.random(10)*2 -1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "productive-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62873253, 0.26622238, 0.        , 0.        , 0.        ,\n",
       "       0.31386122, 0.70394949, 0.        , 0.30259824, 0.17020042])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = f.create_dataset('random', data=data)\n",
    "dset[data<0] = 0\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coral-chemical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62873253, 0.26622238, 0.80604774, 0.50047776, 0.63580955,\n",
       "       0.31386122, 0.70394949, 0.91514766, 0.30259824, 0.17020042])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[data<0] = -1*data[data<0]\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-gathering",
   "metadata": {},
   "source": [
    "#### Coordenadas con listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "final-monitoring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26622238, 0.80604774, 0.91514766])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[[1,2,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-jimmy",
   "metadata": {},
   "source": [
    "#### Leyendo directamente en un arreglo existente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-window",
   "metadata": {},
   "source": [
    "Al leer directamente en un array existente se llena el arrelgo y se hace el recast (conversión de tipo) de manera automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ancient-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('100_1000_array', shape=(100, 1000), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alike-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"100_1000_array\": shape (100, 1000), type \"<f4\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compliant-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.empty((100,1000), dtype=np.float64)\n",
    "dset.read_direct(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-examination",
   "metadata": {},
   "source": [
    "Supongamos que queremos leer la primera fila, en dset [0 ,:], y depositarlo\n",
    "en el array de salida en fila 50: out[50 ,:]. Podemos utilizar las palabras clave *source_sel* y *dest_sel*, para la selección de la fuente y la selección del destino respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "noted-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.read_direct(out, source_sel = np.s_[0,:], dest_sel= np.s_[50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-birth",
   "metadata": {},
   "source": [
    "Observe los dos siguientes snippets de código. ¿Cuál es más eficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bearing-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "out = dset[:, 0:50]\n",
    "print(out.shape)\n",
    "means = out.mean(axis=1)\n",
    "print(means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aware-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.empty((100,50),dtype = np.float32)\n",
    "dset.read_direct(out, np.s_[:,0:50])\n",
    "mean = out.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-stranger",
   "metadata": {},
   "source": [
    "Este puede parecer un caso trivial, pero hay una diferencia importante entre los dos  enfoques. \n",
    "\n",
    "* En el primer ejemplo, h5py crea internamente la matriz out, que se utiliza para almacenar la rebanada y luego desecharla. \n",
    "* En el segundo ejemplo, out es asignado por el usuario y se puede reutilizar para futuras llamadas a read_direct.\n",
    "\n",
    "No hay dierencia en el desmepeño, pero ahora evaluamos un arreglo grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "charged-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('test', shape=(10000, 10000), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "saving-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[:] = np.random.random(10000) # uso de broadcasting para completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "viral-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaro/anaconda3/envs/dask-tutorial/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36437 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4)\n",
    "import dask.array as da\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fleet-comedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50052536, 0.50052536, 0.50052536, ..., 0.50052536, 0.50052536,\n",
       "       0.50052536], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "dset[:,0:500].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interim-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 14.3 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50052536, 0.50052536, 0.50052536, ..., 0.50052536, 0.50052536,\n",
       "       0.50052536], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy\n",
    "out = np.empty((10000, 500), dtype=np.float32)\n",
    "%time\n",
    "dset.read_direct(out, np.s_[:,0:500])\n",
    "out.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "geological-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask\n",
    "%time\n",
    "x = da.from_array(dset[:,0:500], chunks=(4))\n",
    "y= x.mean(axis=1)\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "excited-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dataset():\n",
    "    dset[:,0:500].mean(axis=1)\n",
    "\n",
    "def time_numpy():\n",
    "    dset.read_direct(out, np.s_[:,0:500])\n",
    "    out.mean(axis=1)\n",
    "\n",
    "def time_dask():\n",
    "    x = da.from_array(dset[:,0:500], chunks=(4))\n",
    "    y = x.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "appointed-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "secret-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.053280004000044"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(time_dataset, number=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "received-fantasy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.872780079999984"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(time_numpy, number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(time_dask, number=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-sample",
   "metadata": {},
   "source": [
    "### Cambio de tamaño de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-spray",
   "metadata": {},
   "source": [
    "Así como en Numpy se puede cambira el tamaño de los arreglos, mientras se mantenga la coherencia en el tamño global, dejando al usuario la responsabilidad de la interpretabilidad de los datos, con los datasets de hdf5 podemos hacer lo mismo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "corresponding-small",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset('fixed', (2,2))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-bulgaria",
   "metadata": {},
   "source": [
    "La propiedad maxshape sugiere que el tamaño podría variar hasta esos extremos. En efecto, asi es. Veámos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "spiritual-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset('resizable', (2,2), maxshape=(2,2))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "czech-alliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "dset.resize((1,1))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "racial-anaheim",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to set extend dataset (dimension cannot exceed the existing maximal size (new: 3 max: 2))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7498a6e728a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dask-tutorial/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, axis)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;31m#h5f.flush(self.id)  # THG recommends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.set_extent\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to set extend dataset (dimension cannot exceed the existing maximal size (new: 3 max: 2))"
     ]
    }
   ],
   "source": [
    "dset.resize((1,3)) # falla porque 3>2\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-thomas",
   "metadata": {},
   "source": [
    "No esposible cambiar las dimensiones, es decir, el número total de ejes, de una dataset. Pero si se pueden dejar dimensiones sin un tamaño definido para que crezcan idefinidamente. Basta colocar en la posición respectiva *None* cuando crea el dataset. Veámos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "complex-innocent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, None)\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset('unlimited', (2,2), maxshape =(2,None))\n",
    "print(dset.shape)\n",
    "print(dset.maxshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "empty-jewel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 60)\n"
     ]
    }
   ],
   "source": [
    "dset.resize((2,2*30))\n",
    "print(dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-duncan",
   "metadata": {},
   "source": [
    "#### Cuidados con el cambio de tamaños en datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-emerald",
   "metadata": {},
   "source": [
    "Las reglas de Numpy no aplican exactamento a los dataset. Observe el siguiente ejemplo Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "confidential-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2 3 4]]\n",
      "[[1 2 3 4 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a.resize((1,4))\n",
    "print(a)\n",
    "\n",
    "a.resize((1,10))\n",
    "print(a)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-diabetes",
   "metadata": {},
   "source": [
    "Ahora veámos el mismo código con dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('sizetest', (2,2), dtype=np.int32, \n",
    "                        maxshape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "false-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"sizetest\": shape (2, 2), type \"<i4\">\n"
     ]
    }
   ],
   "source": [
    "dset[...] = [[1,2],[3,4]]\n",
    "print(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "major-farmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "entire-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.resize((1,4))\n",
    "dset[...] # se perdió la fila 1, hdf5 no pudo hacer el reordenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "political-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.resize((1,10))\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-discount",
   "metadata": {},
   "source": [
    "#### Cuando y como hacer el cambio de tamaño del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-airport",
   "metadata": {},
   "source": [
    "Mostramos dos vías. La segunda es más recomendada.\n",
    "\n",
    "1. Se van agregando elelemtos a la medida que el datset crece\n",
    "2. Se crea un dataset grande y luego se poda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregado al paso\n",
    "\n",
    "dset1= f.create_dataset('time_traces', (1,1000), maxshape=(None,1000))\n",
    "\n",
    "def add_trace(arr):\n",
    "    dset.resize(dset1.shape[0]+1, 1000)\n",
    "    dset[-1,:] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando dataset grande y al final podando\n",
    "\n",
    "dset2 = f.create_dataset('time_traces_2', (5000,1000), maxshape=(None,1000))\n",
    "\n",
    "ntraces= 0\n",
    "def add_trace_2(arr):\n",
    "    global ntraces\n",
    "    dset2[ntraces,:] = arr\n",
    "    ntraces += 1\n",
    "\n",
    "def done():\n",
    "    dset.resize((ntraces,1000))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-class",
   "metadata": {},
   "source": [
    "### Fragmentación (chunking) y compresión "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-colony",
   "metadata": {},
   "source": [
    "Consideremos el siguiente arreglo 2-dimensional de Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smoking-brick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' 'B']\n",
      " ['C' 'D']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([['A','B'],['C','D']])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-greene",
   "metadata": {},
   "source": [
    "En realidad el arreglo esta organizado en la memoria linealmente, en el siguiente orden\n",
    "\n",
    "+ 'A', 'B', 'C', 'D'\n",
    "\n",
    "Lo mismo ocurre con los datasets de hdf5. Esto implica que algunas forma de recuperacion de información son mpás eficientes que otras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-alberta",
   "metadata": {},
   "source": [
    "Suponga que va a almacenar 10 imágenes en b/n (escala de grises) de tamaño 480*640. Entonces haría algo como lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "express-miami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('imagetest.hdf5','w')\n",
    "dset = f.create_dataset('Imagenes', (100,480,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-opening",
   "metadata": {},
   "source": [
    "La primera imagen se recupera como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen = dset[0,:,:]\n",
    "image.shape  # (480, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-arrangement",
   "metadata": {},
   "source": [
    "Esta imagen  muesra como es almacenado el dataset\n",
    "\n",
    "![dataset_almacenamiento](../Imagenes/dataset_almacenamiento.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-universe",
   "metadata": {},
   "source": [
    "#### Almacenamiento fragmentado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-embassy",
   "metadata": {},
   "source": [
    "Pero, ¿qué pasa si, en lugar de procesar imágenes completas una tras otra, nuestra aplicación trata con mosaicos de imagen? \n",
    "\n",
    "Supongamos que queremos leer y procesar los datos en un segmento de 64 × 64 píxeles en la esquina de la primera imagen; por ejemplo, digamos que queremos agregar un logotipo.\n",
    "Nuestra selección de rebanado sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = dset[0,0:64, 0.64]\n",
    "tile.shape # (64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-button",
   "metadata": {},
   "source": [
    "¿Y si hubiera alguna forma de expresar esto de antemano? ¿No hay forma de preservar la forma del conjunto de datos, que es semánticamente importante, pero dígale a HDF5 que optimice la\n",
    "conjunto de datos para el acceso en bloques de 64 × 64 píxeles?\n",
    "\n",
    "\n",
    "Eso es lo que hace la fragmentación en HDF5. Le permite especificar la \"forma\" N-dimensional que\n",
    "se adapta mejor a su patrón de acceso. Cuando llega el momento de escribir datos en el disco, HDF5 se divide\n",
    "los datos en \"trozos\" de la forma especificada, los aplana y los escribe en el disco. Los fragmentos se almacenan en varios lugares del archivo y sus coordenadas están indexadas por un\n",
    "Árbol B (binario).\n",
    "\n",
    "Aquí tenemos un ejemplo. Tomemos el conjunto de datos de forma (100, 480, 640) que se acaba de mostrar y digamos a HDF5\n",
    "que lo almacene en formato fragmentado. \n",
    "\n",
    "Hacemos esto proporcionando una nueva palabra clave, fragmentos, al método *create_dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "judicial-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('fragmentado', (100,480,640), dtype='i1',\n",
    "                       chunks=(1,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-partnership",
   "metadata": {},
   "source": [
    "Así son almacenados los datos en este caso.\n",
    "\n",
    "![fragmentado](../Imagenes/chunckshdf5.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "Así es como ocurre la compresión en hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-phone",
   "metadata": {},
   "source": [
    "#### Auto fragmentado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-persian",
   "metadata": {},
   "source": [
    "Si nos esta seguro sobre como gestionará el dataset, puede dejar que hdf5 decida como fragmentarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('imagenes2', (100,480,640), chunks=True)\n",
    "dset.chunks # (13, 60,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-emergency",
   "metadata": {},
   "source": [
    "### Elegir una forma manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-tooth",
   "metadata": {},
   "source": [
    "\n",
    "A continuación, se incluyen algunas cosas que debe tener en cuenta al trabajar con fragmentos. El proceso de elegir\n",
    "la forma de los fragmentos es una compensación entre las siguientes tres restricciones:\n",
    "\n",
    "1. Los fragmentos más grandes para un tamaño de conjunto de datos determinado reducen el tamaño del fragmento del árbol B, lo que hace es más rápido para buscar y cargar fragmentos.\n",
    "2. Dado que los fragmentos son todo o nada (leer una parte carga todo el fragmento), los fragmentos también aumentan la posibilidad de que lea datos en la memoria que no usará.\n",
    "3. La caché de fragmentos HDF5 solo puede contener un número finito de fragmentos. Trozos más grandes\n",
    "de 1 MB ni siquiera compartiran en el caché.\n",
    "\n",
    "Entonces, estos son los puntos principales a tener en cuenta:\n",
    "\n",
    "- *¿Necesita realmente especificar un tamaño de fragmento?* Es mejor restringir el tamaño manual de los fragmentos a los casos en los que esté seguro de que su conjunto de datos\n",
    "se accederá de una manera que probablemente sea ineficaz con el almacenamiento contiguo o una forma de trozo adivinada automáticamente. Y como todas las optimizaciones, ¡debería compararlas!\n",
    "\n",
    "- *Intente expresar el patrón de acceso \"natural\" que tendrá su conjunto de datos*. Como en nuestro ejemplo, si está almacenando un montón de imágenes en un conjunto de datos y sabe que\n",
    "su aplicación leerá \"mosaicos\" particulares de 64 × 64, podría usar N × 64 × 64 trozos (o N × 128 × 128) a lo largo de los ejes de la imagen.\n",
    "\n",
    "- *No los hagas demasiado pequeños*. Tenga en cuenta que HDF5 tiene que usar datos de indexación para realizar un seguimiento de las cosas; si utiliza algo patológico como un tamaño de fragmento de 1 byte, la mayor parte de su espacio en disco será\n",
    "tomado por metadatos. Una buena regla general para la mayoría de los conjuntos de datos es mantener fragmentos\n",
    "por encima de 10 KB más o menos.\n",
    "\n",
    "- *No los hagas demasiado grandes*. La clave para recordar es que cuando lee cualquier dato en un fragmento, todo el\n",
    "se lee el fragmento. Si solo usa un subconjunto de los datos, el tiempo extra dedicado a la lectura de el disco está desperdiciado. Tenga en cuenta que los trozos de más de 1 MiB de forma predeterminada no participar en el rápido \"caché de fragmentos\" en memoria y, en su lugar, se leerá desde el disco\n",
    "cada vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-heaven",
   "metadata": {},
   "source": [
    "### Filtros y Compresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-arena",
   "metadata": {},
   "source": [
    "Con la fragmentación, es posible realizar la compresión de forma transparente en un conjunto de datos.\n",
    "\n",
    "Se conoce el tamaño inicial de cada fragmento y, dado que están indexados por un árbol B, pueden\n",
    "almacenarse en cualquier lugar del archivo, no solo uno tras otro. En otras palabras, cada trozo es\n",
    "libre para crecer o encogerse sin golpear a los demás"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-gentleman",
   "metadata": {},
   "source": [
    "####  Tubería de filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-foster",
   "metadata": {},
   "source": [
    "HDF5 tiene el concepto de una tubería de filtro, que es solo una serie de operaciones realizadas\n",
    "en cada fragmento cuando está escrito. Cada filtro es libre de hacer lo que quiera con los datos en\n",
    "el fragmento: comprimirlo, sumarlo, agregar metadatos, cualquier cosa. Cuando se lee el archivo, cada\n",
    "El filtro se ejecuta en modo \"inverso\" para reconstruir los datos originales.\n",
    "\n",
    "La imagen muestra como sucede la acción\n",
    "\n",
    "\n",
    "![Tuberia_filtro](../Imagenes/tuberia_filtros.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-cradle",
   "metadata": {},
   "source": [
    "#### Filtros de compresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-fleece",
   "metadata": {},
   "source": [
    "Hay varios filtros de compresión disponibles en HDF5. Con mucho, el más utilizado es el filtro GZIP. (También escuchará que se hace referencia a esto como el filtro \"DEFLATED\"; en el mundo HDF5, ambos nombres se utilizan para el mismo filtro).\n",
    "\n",
    "A continuación, se muestra un ejemplo de compresión GZIP utilizada en un conjunto de datos de punto flotante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset =  f.create_dataset('BigDataset', (1000,1000),dtype='f', \n",
    "                         compression='gzip' )\n",
    "dset.compression # 'gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-sterling",
   "metadata": {},
   "source": [
    "### Grupos, enlaces e iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-viewer",
   "metadata": {},
   "source": [
    "Los grupos son el objeto contenedor HDF5, análogo a las carpetas de un sistema de archivos. Ellos pueden\n",
    "contener conjuntos de datos y otros grupos, lo que le permite construir una estructura jerárquica con\n",
    "objetos perfectamente organizados en grupos y subgrupos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-classic",
   "metadata": {},
   "source": [
    "#### El grupo raiz y subgrupos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-backing",
   "metadata": {},
   "source": [
    "El objeto de grupo más general es h5py.Group, del cual h5py.File es una subclase. Otro Los grupos se crean fácilmente con el método *create_group*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inclusive-midwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/SubGroup\" (0 members)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import h5py\n",
    "f = h5py.File('../Datos/Groups.hdf5','w')\n",
    "subgroup = f.create_group('SubGroup')\n",
    "subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pharmaceutical-conditions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SubGroup'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroup.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-original",
   "metadata": {},
   "source": [
    "Por supuesto, los grupos también se pueden anidar. El método *create_group* existe en todos los grupos objetos, no solo File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sensitive-launch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/SubGroup/AnotherGroup'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsubgroup = subgroup.create_group('AnotherGroup')\n",
    "subsubgroup.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-imagination",
   "metadata": {},
   "source": [
    "No es necesario crear manualmente todos los subgrupos anidados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "undefined-miracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/some/big/path\" (0 members)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = f.create_group('/some/big/path')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wound-subsection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['SubGroup', 'some']>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-reservation",
   "metadata": {},
   "source": [
    "### Elementos escenciales de Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-attack",
   "metadata": {},
   "source": [
    "Si no recuerda nada más de este capítulo, recuerde esto: los grupos funcionan principalmente como diccionarios. \n",
    "\n",
    "Hay un par de agujeros en esta abstracción, pero en general funciona sorprendentemente bien. Los grupos son iterables y tienen un subconjunto del diccionario Python normal\n",
    "API.\n",
    "\n",
    "Agregemos unos pocos objetos al archivo de este ejemplo para lo que sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focused-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Dataset1'] = 1.0\n",
    "f['Dataset2'] = 2.0\n",
    "f['Dataset3'] = 3.0\n",
    "subgroup['Dataset4'] = 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-worst",
   "metadata": {},
   "source": [
    "#### Acceso al estilo de diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liquid-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accesa el dataset asociado a la 'clave' dataset1\n",
    "dset1 = f['Dataset1']\n",
    "\n",
    "dset4 = f['SubGroup/Dataset4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "agricultural-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(f))\n",
    "print(len(f['SubGroup']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
